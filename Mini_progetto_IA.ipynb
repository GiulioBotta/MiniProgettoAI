{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e968e5a8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# INTELLIGENZA ARTIFICIALE\n",
    "**Prof. Marco Zorzi, Dr. Alberto Testolin**\n",
    "\n",
    "## Mini-Progetto Individuale - Riconoscimento Cifre Manoscritte\n",
    "\n",
    "**Nome**: [Inserire nome]  \n",
    "**Cognome**: [Inserire cognome]  \n",
    "**Matricola**: [Inserire matricola]  \n",
    "**Data di consegna**: [Inserire data]\n",
    "\n",
    "---\n",
    "\n",
    "### Obiettivo\n",
    "Implementare simulazioni per studiare il riconoscimento di cifre manoscritte\n",
    "utilizzando reti neurali Multi-Layer Perceptron (MLP) e Convolutional Neural Networks (CNN).\n",
    "\n",
    "### Setup Ambiente\n",
    "- Python version: Python 3.11.12\n",
    "- TensorFlow: ‚úÖ Disponibile\n",
    "- PyTorch: ‚úÖ Disponibile\n",
    "- scikit-learn: ‚úÖ Disponibile\n",
    "\n",
    "### Punti da sviluppare:\n",
    "- **[a]** Variazione neuroni/strati e iper-parametri [2 punti]\n",
    "- **[b]** Cifre pi√π difficili da riconoscere (matrice confusione) [1 punto]  \n",
    "- **[c]** Curve psicometriche con rumore graduale [1 punto]\n",
    "- **[d]** Riduzione dataset training (10%) [1 punto]\n",
    "- **[e]** Miglioramento con rumore nel training [1 punto]\n",
    "- **[bonus]** Estensione a FashionMNIST [punto bonus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie necessarie\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# PyTorch (sempre disponibile)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Configurazione per riproducibilit√†\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Configurazione TensorFlow\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Configurazione plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üöÄ Setup completato!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Scikit-learn version: {metrics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac3212",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Caricamento e Preprocessing dei Dati\n",
    "\n",
    "Utilizziamo il dataset MNIST come base per tutti gli esperimenti.\n",
    "Il dataset contiene 70.000 immagini di cifre manoscritte (60.000 training + 10.000 test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dataset MNIST\n",
    "print(\"üìä Caricamento dataset MNIST...\")\n",
    "\n",
    "# Caricamento con torchvision\n",
    "mnist_train = MNIST(root='./data', train=True, download=True)\n",
    "mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Conversione in numpy arrays\n",
    "X_train_full = mnist_train.data.numpy()\n",
    "y_train_full = mnist_train.targets.numpy()\n",
    "X_test = mnist_test.data.numpy() \n",
    "y_test = mnist_test.targets.numpy()\n",
    "\n",
    "print(f\"‚úÖ Dataset caricato:\")\n",
    "print(f\"   Training set: {X_train_full.shape[0]} immagini\")\n",
    "print(f\"   Test set: {X_test.shape[0]} immagini\")\n",
    "print(f\"   Dimensioni immagine: {X_train_full.shape[1]}x{X_train_full.shape[2]}\")\n",
    "print(f\"   Numero classi: {len(np.unique(y_train_full))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dae6b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Preprocessing dei Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing base\n",
    "def preprocess_mnist(X_train, X_test, for_cnn=False):\n",
    "    \"\"\"\n",
    "    Preprocessing del dataset MNIST\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: array numpy delle immagini\n",
    "        for_cnn: se True, mantiene forma 2D per CNN, altrimenti flattens per MLP\n",
    "    \n",
    "    Returns:\n",
    "        X_train_proc, X_test_proc: dati preprocessati\n",
    "    \"\"\"\n",
    "    if for_cnn:\n",
    "        # Per CNN: normalizza e aggiungi dimensione canale\n",
    "        X_train_proc = X_train.astype('float32') / 255.0\n",
    "        X_test_proc = X_test.astype('float32') / 255.0\n",
    "        X_train_proc = X_train_proc.reshape(-1, 28, 28, 1)\n",
    "        X_test_proc = X_test_proc.reshape(-1, 28, 28, 1)\n",
    "    else:\n",
    "        # Per MLP: flatten e normalizza\n",
    "        X_train_proc = X_train.reshape(X_train.shape[0], -1).astype('float32') / 255.0\n",
    "        X_test_proc = X_test.reshape(X_test.shape[0], -1).astype('float32') / 255.0\n",
    "    \n",
    "    return X_train_proc, X_test_proc\n",
    "\n",
    "# Preprocessing iniziale per MLP\n",
    "X_train_mlp, X_test_mlp = preprocess_mnist(X_train_full, X_test, for_cnn=False)\n",
    "\n",
    "# Preprocessing per CNN (useremo dopo)\n",
    "X_train_cnn, X_test_cnn = preprocess_mnist(X_train_full, X_test, for_cnn=True)\n",
    "\n",
    "print(\"‚úÖ Preprocessing completato\")\n",
    "print(f\"   MLP format: {X_train_mlp.shape}\")\n",
    "print(f\"   CNN format: {X_train_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d9df5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Visualizzazione Campioni del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione esempi del dataset\n",
    "def plot_mnist_samples(X, y, n_samples=10, title=\"Campioni MNIST\"):\n",
    "    \"\"\"Visualizza campioni casuali del dataset\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Seleziona campioni casuali\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = axes[i//5, i%5]\n",
    "        \n",
    "        # Reshape per visualizzazione se necessario\n",
    "        img = X[idx].reshape(28, 28) if len(X[idx].shape) == 1 else X[idx].squeeze()\n",
    "        \n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Cifra: {y[idx]}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostra campioni del dataset\n",
    "plot_mnist_samples(X_train_full, y_train_full, title=\"Campioni Dataset MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578a354",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO A: Analisi Architetturale [2 punti]\n",
    "\n",
    "Studiamo l'effetto del numero di neuroni e strati nascosti sulle prestazioni \n",
    "di MLP (scikit-learn) e CNN (PyTorch/TensorFlow).\n",
    "\n",
    "### Nota Implementativa\n",
    "Useremo sia TensorFlow che PyTorch per CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ceebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare analisi sistematica delle architetture\n",
    "print(\"üìã Punto A: Da implementare - Analisi architetturale\")\n",
    "print(\"   üîß MLP: scikit-learn (sempre disponibile)\")\n",
    "print(\"   üîß CNN: PyTorch + TensorFlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037a7c0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Funzioni Utility per CNN con PyTorch\n",
    "\n",
    "Implementiamo le funzioni base per CNN con PyTorch che funzionano sempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"CNN semplice con PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters=32, hidden_size=128):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Calcolo dimensioni per il layer fully connected\n",
    "        self.fc1 = nn.Linear(num_filters*2 * 7 * 7, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_pytorch_cnn(model, train_loader, test_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"Training di CNN con PyTorch\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "print(\"‚úÖ Funzioni PyTorch CNN definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c0a43",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO B: Analisi degli Errori MLP [1 punto]\n",
    "\n",
    "Identifichiamo le cifre pi√π difficili da riconoscere utilizzando la matrice di confusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3175743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare analisi errori MLP\n",
    "print(\"üìã Punto B: Da implementare - Analisi errori\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e75488",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO C: Curve Psicometriche [1 punto]\n",
    "\n",
    "Studiamo come cambia l'accuratezza con l'introduzione graduale di rumore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare curve psicometriche\n",
    "print(\"üìã Punto C: Da implementare - Curve psicometriche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fc680",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO D: Training Set Ridotto [1 punto]\n",
    "\n",
    "Analizziamo l'effetto della riduzione drastica dei pattern di training (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b46b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare training con dataset ridotto\n",
    "print(\"üìã Punto D: Da implementare - Dataset ridotto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fac9d3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO E: Training con Rumore [1 punto]\n",
    "\n",
    "Miglioriamo l'accuratezza sui pattern rumorosi introducendo rumore nel training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d451fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare training con rumore\n",
    "print(\"üìã Punto E: Da implementare - Training con rumore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2b61",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## PUNTO BONUS: Estensione a FashionMNIST\n",
    "\n",
    "Estendiamo le simulazioni al dataset FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementare estensione FashionMNIST\n",
    "print(\"üìã Punto Bonus: Da implementare - FashionMNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20c71a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusioni\n",
    "\n",
    "### Riassunto Risultati\n",
    "[Da completare con i risultati degli esperimenti]\n",
    "\n",
    "### Discussione\n",
    "[Da completare con interpretazioni teoriche]\n",
    "\n",
    "### Limitazioni e Sviluppi Futuri\n",
    "[Da completare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95089711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Template progetto creato!\")\n",
    "print(\"üìù Prossimi passi:\")\n",
    "print(\"   1. Implementare Punto A: Analisi architetturale\")\n",
    "print(\"   2. Implementare Punto B: Analisi errori\")\n",
    "print(\"   3. Implementare Punto C: Curve psicometriche\")\n",
    "print(\"   4. Implementare Punto D: Dataset ridotto\") \n",
    "print(\"   5. Implementare Punto E: Training con rumore\")\n",
    "print(\"   6. [Opzionale] Punto Bonus: FashionMNIST\")\n",
    "print(\"\")\n",
    "print(\"üí° Framework disponibili:\")\n",
    "print(\"   ‚úÖ scikit-learn (MLP)\")\n",
    "print(\"   ‚úÖ PyTorch (CNN)\")\n",
    "print(\"   ‚úÖ TensorFlow (CNN)\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
