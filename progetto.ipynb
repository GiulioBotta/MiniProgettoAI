{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77f4b15",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# INTELLIGENZA ARTIFICIALE - Mini-Progetto Individuale\n",
    "**Prof. Marco Zorzi, Dr. Alberto Testolin**\n",
    "\n",
    "**Nome**: [INSERIRE NOME]  \n",
    "**Cognome**: [INSERIRE COGNOME]  \n",
    "**Matricola**: [INSERIRE MATRICOLA]  \n",
    "**Data**: [INSERIRE DATA]\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo del Progetto\n",
    "Implementare alcune simulazioni per studiare il riconoscimento di cifre manoscritte, \n",
    "analizzando l'effetto di architetture e iper-parametri diversi sui modelli MLP e CNN.\n",
    "\n",
    "Le simulazioni si baseranno sul dataset MNIST, seguendo rigorosamente l'approccio \n",
    "metodologico utilizzato nei laboratori del corso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe329e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4153885502.py, line 13)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom tensorflow import keras  # type: ignore\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Setup delle librerie\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow per CNN\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras  \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# PyTorch per dataset loading (seguendo Lab 3)\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Configurazione per riproducibilitÃ \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup completato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dae275",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# PUNTO A: Analisi Architetturale [2 punti]\n",
    "\n",
    "**Obiettivo**: Analizzare sistematicamente come cambia la prestazione dei modelli \n",
    "(MLP e CNN) al variare del numero di neuroni, strati nascosti e altri iper-parametri \n",
    "significativi.\n",
    "\n",
    "## Metodologia Sistematica\n",
    "\n",
    "Seguendo l'approccio rigoroso dei laboratori, implementeremo:\n",
    "\n",
    "**Per MLP (36 esperimenti):**\n",
    "- Neuroni per strato: 50, 100, 250\n",
    "- Numero strati: 1 vs 2 strati nascosti  \n",
    "- Solver: SGD vs Adam\n",
    "- Learning rate: 0.001, 0.01, 0.1\n",
    "\n",
    "**Per CNN (24 esperimenti):**\n",
    "- Filtri: 16, 32, 64\n",
    "- Architettura: baseline vs extended\n",
    "- Learning rate: 0.001, 0.01\n",
    "- Optimizer: Adam vs SGD\n",
    "\n",
    "**Totale: 60 esperimenti sistematici**\n",
    "\n",
    "L'approccio garantisce confronto equo e bilanciato tra le architetture, \n",
    "estendendo la metodologia del Lab 2 anche alle CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175256c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento e preprocessing MNIST - Seguendo esattamente Lab 3\n",
    "print(\"Caricamento dataset MNIST...\")\n",
    "\n",
    "# Caricamento tramite TorchVision (metodo Lab 3)\n",
    "mnist_tr = MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_te = MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Conversione a numpy arrays\n",
    "x_train = mnist_tr.data.numpy()\n",
    "y_train = mnist_tr.targets.numpy()\n",
    "x_test = mnist_te.data.numpy()\n",
    "y_test = mnist_te.targets.numpy()\n",
    "\n",
    "print(f\"Dataset caricato: {x_train.shape[0]} train, {x_test.shape[0]} test\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing dati...\")\n",
    "\n",
    "# Per MLP: flattening + normalizzazione [0,1]\n",
    "x_train_mlp = x_train.reshape(x_train.shape[0], -1).astype(np.float32) / 255.0\n",
    "x_test_mlp = x_test.reshape(x_test.shape[0], -1).astype(np.float32) / 255.0\n",
    "\n",
    "# Per CNN: formato 4D + normalizzazione [0,1]  \n",
    "x_train_cnn = x_train.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "x_test_cnn = x_test.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "\n",
    "print(f\"Preprocessing completato:\")\n",
    "print(f\"  MLP: {x_train_mlp.shape} -> {x_test_mlp.shape}\")\n",
    "print(f\"  CNN: {x_train_cnn.shape} -> {x_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione esempi del dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "fig.suptitle('Dataset MNIST - Esempi per Cifra', fontsize=14, fontweight='bold')\n",
    "\n",
    "for digit in range(10):\n",
    "    # Trova primo esempio di ogni cifra\n",
    "    idx = np.where(y_train == digit)[0][0]\n",
    "    \n",
    "    ax = axes[digit//5, digit%5]\n",
    "    ax.imshow(x_train[idx], cmap='gray')\n",
    "    ax.set_title(f'Cifra {digit}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche dataset\n",
    "print(f\"\\nStatistiche Dataset:\")\n",
    "print(f\"Forma immagini: {x_train.shape[1:]} pixels\")\n",
    "print(f\"Range valori: [{x_train.min()}, {x_train.max()}]\") \n",
    "print(f\"Classi: {len(np.unique(y_train))} cifre\")\n",
    "print(f\"Distribuzione classi (train):\")\n",
    "\n",
    "class_counts = np.bincount(y_train)\n",
    "for digit, count in enumerate(class_counts):\n",
    "    print(f\"  Cifra {digit}: {count:5d} esempi ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242b1b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configurazione Esperimenti MLP\n",
    "\n",
    "Implementazione sistematica seguendo metodologia Lab 2, con estensione \n",
    "a parametri aggiuntivi per analisi completa.\n",
    "\n",
    "**Razionale delle scelte:**\n",
    "- **Neuroni**: 50, 100, 250 â†’ range piccolo/medio/grande\n",
    "- **Strati**: 1 vs 2 â†’ analisi profonditÃ  vs overfitting  \n",
    "- **Solver**: SGD vs Adam â†’ confronto algoritmi ottimizzazione\n",
    "- **Learning Rate**: 0.001, 0.01, 0.1 â†’ ordini di grandezza diversi\n",
    "\n",
    "**Parametri fissi ottimizzati:**\n",
    "- Early stopping per efficienza e prevenzione overfitting\n",
    "- Validation split 10% per monitoring\n",
    "- Tolerance 0.001 (standard sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione esperimenti MLP\n",
    "print(\"=== CONFIGURAZIONE ESPERIMENTI MLP ===\")\n",
    "\n",
    "mlp_configs = []\n",
    "\n",
    "# Parametri da testare sistematicamente\n",
    "neurons_options = [50, 100, 250]          # 3 opzioni\n",
    "layers_options = [1, 2]                   # 2 opzioni  \n",
    "solvers = ['sgd', 'adam']                 # 2 opzioni\n",
    "learning_rates = [0.001, 0.01, 0.1]      # 3 opzioni\n",
    "\n",
    "# Generazione configurazioni (3Ã—2Ã—2Ã—3 = 36 esperimenti)\n",
    "for neurons in neurons_options:\n",
    "    for n_layers in layers_options:\n",
    "        for solver in solvers:\n",
    "            for lr in learning_rates:\n",
    "                \n",
    "                # Definizione architettura nascosta\n",
    "                if n_layers == 1:\n",
    "                    hidden_layers = (neurons,)\n",
    "                else:  # n_layers == 2\n",
    "                    hidden_layers = (neurons, neurons)\n",
    "                \n",
    "                # Configurazione completa\n",
    "                config = {\n",
    "                    'name': f'MLP_{neurons}n_{n_layers}l_{solver}_lr{lr}',\n",
    "                    'hidden_layer_sizes': hidden_layers,\n",
    "                    'solver': solver,\n",
    "                    'learning_rate_init': lr,\n",
    "                    'max_iter': 200,\n",
    "                    'early_stopping': True,\n",
    "                    'validation_fraction': 0.1,\n",
    "                    'n_iter_no_change': 15,  # Patience generosa\n",
    "                    'tol': 0.001,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                mlp_configs.append(config)\n",
    "\n",
    "print(f\"Configurazioni MLP generate: {len(mlp_configs)}\")\n",
    "print(f\"Struttura: {len(neurons_options)} neuroni Ã— {len(layers_options)} strati Ã— {len(solvers)} solver Ã— {len(learning_rates)} LR\")\n",
    "\n",
    "# Anteprima configurazioni\n",
    "print(f\"\\nPrime 3 configurazioni:\")\n",
    "for i, config in enumerate(mlp_configs[:3]):\n",
    "    print(f\"  {i+1}. {config['name']}\")\n",
    "    print(f\"     Architettura: {config['hidden_layer_sizes']}\")\n",
    "    print(f\"     Solver: {config['solver']}, LR: {config['learning_rate_init']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25ef4c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Esecuzione Esperimenti MLP\n",
    "\n",
    "Training sistematico di tutte le 36 configurazioni con monitoring \n",
    "delle performance e visualizzazione dei risultati.\n",
    "\n",
    "**Metriche monitorate:**\n",
    "- Accuratezza training e test\n",
    "- Tempo di training\n",
    "- Numero iterazioni per convergenza  \n",
    "- Curve di loss (quando disponibili)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b30d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esecuzione esperimenti MLP\n",
    "print(\"=== ESECUZIONE ESPERIMENTI MLP ===\")\n",
    "print(f\"Avvio training di {len(mlp_configs)} configurazioni...\")\n",
    "\n",
    "mlp_results = []\n",
    "start_total = time.time()\n",
    "\n",
    "for i, config in enumerate(mlp_configs):\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n[{i+1:2d}/{len(mlp_configs)}] Training {config_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creazione modello con parametri dalla configurazione\n",
    "    model_params = {k: v for k, v in config.items() if k != 'name'}\n",
    "    mlp = MLPClassifier(**model_params)\n",
    "    \n",
    "    # Training\n",
    "    mlp.fit(x_train_mlp, y_train)\n",
    "    \n",
    "    # Valutazione performance\n",
    "    train_acc = mlp.score(x_train_mlp, y_train)\n",
    "    test_acc = mlp.score(x_test_mlp, y_test) \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Raccolta risultati\n",
    "    result = {\n",
    "        'name': config_name,\n",
    "        'model': mlp,\n",
    "        'config': config,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'n_iterations': mlp.n_iter_,\n",
    "        'converged': mlp.n_iter_ < config['max_iter'],\n",
    "        'loss_curve': mlp.loss_curve_ if hasattr(mlp, 'loss_curve_') else None\n",
    "    }\n",
    "    \n",
    "    mlp_results.append(result)\n",
    "    \n",
    "    # Progress report\n",
    "    print(f\"  âœ… Test Acc: {test_acc:.4f} | Train Acc: {train_acc:.4f} | Overfitting: {train_acc-test_acc:+.4f}\")\n",
    "    print(f\"     Tempo: {training_time:5.1f}s | Iterazioni: {mlp.n_iter_:3d} | Converged: {'âœ“' if result['converged'] else 'âœ—'}\")\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\nâœ… MLP Esperimenti completati!\")\n",
    "print(f\"   Tempo totale: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"   Modelli: {len(mlp_results)}/{len(mlp_configs)}\")\n",
    "print(f\"   Tempo medio per esperimento: {total_time/len(mlp_configs):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5f62c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configurazione Esperimenti CNN\n",
    "\n",
    "Estensione dell'approccio sistematico alle architetture convoluzionali,\n",
    "seguendo la metodologia del Lab 3 con variazioni strutturate.\n",
    "\n",
    "**Razionale delle scelte:**\n",
    "- **Filtri**: 16, 32, 64 â†’ da sotto-baseline a over-baseline Lab 3\n",
    "- **Architettura**: baseline (Lab 3) vs extended (piÃ¹ profonda)\n",
    "- **Learning Rate**: 0.001, 0.01 â†’ range conservativo per CNN\n",
    "- **Optimizer**: Adam vs SGD â†’ stessa logica MLP per confrontabilitÃ \n",
    "\n",
    "**Architetture definite:**\n",
    "- **Baseline**: Conv2D + Flatten + Dense (replica Lab 3)\n",
    "- **Extended**: Conv2D + MaxPooling + Conv2D + Flatten + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione esperimenti CNN\n",
    "print(\"=== CONFIGURAZIONE ESPERIMENTI CNN ===\")\n",
    "\n",
    "# Parametri da testare sistematicamente  \n",
    "filters_options = [16, 32, 64]           # 3 opzioni\n",
    "architectures = ['baseline', 'extended'] # 2 opzioni\n",
    "learning_rates = [0.001, 0.01]          # 2 opzioni\n",
    "optimizers = ['adam', 'sgd']             # 2 opzioni\n",
    "\n",
    "cnn_configs = []\n",
    "\n",
    "# Generazione configurazioni (3Ã—2Ã—2Ã—2 = 24 esperimenti)\n",
    "for filters in filters_options:\n",
    "    for arch in architectures:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                config = {\n",
    "                    'name': f'CNN_{filters}f_{arch}_{opt}_lr{lr}',\n",
    "                    'filters': filters,\n",
    "                    'architecture': arch,\n",
    "                    'optimizer': opt,\n",
    "                    'learning_rate': lr,\n",
    "                    'epochs': 30,\n",
    "                    'batch_size': 32,\n",
    "                    'validation_split': 0.1,\n",
    "                    'early_stopping': True,\n",
    "                    'patience': 10,\n",
    "                    'min_delta': 0.001\n",
    "                }\n",
    "                cnn_configs.append(config)\n",
    "\n",
    "print(f\"Configurazioni CNN generate: {len(cnn_configs)}\")\n",
    "print(f\"Struttura: {len(filters_options)} filtri Ã— {len(architectures)} arch Ã— {len(optimizers)} opt Ã— {len(learning_rates)} LR\")\n",
    "\n",
    "# Definizione factory per modelli CNN\n",
    "def create_cnn_model(filters, architecture, optimizer, learning_rate):\n",
    "    \"\"\"Crea modello CNN secondo specifiche\"\"\"\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    if architecture == 'baseline':\n",
    "        # Replica esatta Lab 3\n",
    "        model.add(keras.layers.Conv2D(filters, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(50, activation='relu'))\n",
    "        \n",
    "    elif architecture == 'extended':\n",
    "        # Versione piÃ¹ profonda con pooling\n",
    "        model.add(keras.layers.Conv2D(filters, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.MaxPooling2D(2,2))\n",
    "        model.add(keras.layers.Conv2D(filters*2, (3,3), activation='relu'))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    \n",
    "    # Output layer comune\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Configurazione optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # sgd\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test creazione modello\n",
    "test_model = create_cnn_model(32, 'baseline', 'adam', 0.001)\n",
    "print(f\"\\nArchitettura baseline di test:\")\n",
    "test_model.summary()\n",
    "\n",
    "print(f\"\\nPrime 3 configurazioni CNN:\")\n",
    "for i, config in enumerate(cnn_configs[:3]):\n",
    "    print(f\"  {i+1}. {config['name']}\")\n",
    "    print(f\"     Filtri: {config['filters']}, Arch: {config['architecture']}\")\n",
    "    print(f\"     Optimizer: {config['optimizer']}, LR: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229592b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Esecuzione Esperimenti CNN\n",
    "\n",
    "Training sistematico di tutte le 24 configurazioni CNN con early stopping\n",
    "e monitoring completo delle performance.\n",
    "\n",
    "**Setup training:**\n",
    "- Epochs: 30 (bilanciamento convergenza/tempo)\n",
    "- Batch size: 32 (standard per MNIST)\n",
    "- Validation split: 10% (coerente con MLP)\n",
    "- Early stopping: patience=10, min_delta=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esecuzione esperimenti CNN\n",
    "print(\"=== ESECUZIONE ESPERIMENTI CNN ===\")\n",
    "print(f\"Avvio training di {len(cnn_configs)} configurazioni...\")\n",
    "\n",
    "cnn_results = []\n",
    "start_total = time.time()\n",
    "\n",
    "for i, config in enumerate(cnn_configs):\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n[{i+1:2d}/{len(cnn_configs)}] Training {config_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creazione modello\n",
    "    model = create_cnn_model(\n",
    "        config['filters'], \n",
    "        config['architecture'],\n",
    "        config['optimizer'], \n",
    "        config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Setup early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta'], \n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Training con validation split\n",
    "    history = model.fit(\n",
    "        x_train_cnn, y_train,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Valutazione finale\n",
    "    train_loss, train_acc = model.evaluate(x_train_cnn, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test_cnn, y_test, verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Raccolta risultati\n",
    "    result = {\n",
    "        'name': config_name,\n",
    "        'model': model,\n",
    "        'config': config,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'converged': len(history.history['loss']) < config['epochs'],\n",
    "        'history': history,\n",
    "        'final_train_loss': train_loss,\n",
    "        'final_test_loss': test_loss\n",
    "    }\n",
    "    \n",
    "    cnn_results.append(result)\n",
    "    \n",
    "    # Progress report\n",
    "    print(f\"  âœ… Test Acc: {test_acc:.4f} | Train Acc: {train_acc:.4f} | Overfitting: {train_acc-test_acc:+.4f}\")\n",
    "    print(f\"     Tempo: {training_time:5.1f}s | Epochs: {result['epochs_trained']:2d} | Early Stop: {'âœ“' if result['converged'] else 'âœ—'}\")\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\nâœ… CNN Esperimenti completati!\")\n",
    "print(f\"   Tempo totale: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"   Modelli: {len(cnn_results)}/{len(cnn_configs)}\")\n",
    "print(f\"   Tempo medio per esperimento: {total_time/len(cnn_configs):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739254c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# Analisi dei Risultati\n",
    "\n",
    "## Approccio Analitico\n",
    "\n",
    "Seguendo la metodologia dei laboratori, analizziamo i risultati attraverso:\n",
    "\n",
    "1. **Ranking Performance**: Identificazione modelli top per accuratezza test\n",
    "2. **Analisi Parametrica**: Effetto sistematico di ogni iperparametro\n",
    "3. **Confronto Architetture**: MLP vs CNN con discussione quantitativa\n",
    "4. **Analisi Overfitting**: Bilanciamento train vs test accuracy\n",
    "5. **Efficienza Computazionale**: Tempo training vs performance\n",
    "\n",
    "**Obiettivo**: Identificazione configurazioni ottimali e insights per design futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi dettagliata risultati MLP\n",
    "print(\"=== ANALISI DETTAGLIATA RISULTATI MLP ===\")\n",
    "\n",
    "# 1. RANKING GENERALE\n",
    "mlp_sorted = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"\\nðŸ† TOP 5 MLP (Test Accuracy):\")\n",
    "for i, result in enumerate(mlp_sorted[:5]):\n",
    "    name_parts = result['name'].split('_')\n",
    "    neurons = name_parts[1]\n",
    "    layers = name_parts[2] \n",
    "    solver = name_parts[3]\n",
    "    lr = name_parts[4]\n",
    "    \n",
    "    print(f\"{i+1:2d}. {result['name']:25} \"\n",
    "          f\"Acc: {result['test_accuracy']:.4f} \"\n",
    "          f\"Ovf: {result['overfitting']:+.4f} \"\n",
    "          f\"Time: {result['training_time']:4.1f}s\")\n",
    "\n",
    "# 2. ANALISI EFFETTO NUMERO NEURONI\n",
    "print(f\"\\nðŸ“Š Effetto Numero Neuroni (1 strato, Adam, LR=0.01):\")\n",
    "neuron_analysis = []\n",
    "for neurons in [50, 100, 250]:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_{neurons}n_1l_adam_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        neuron_analysis.append((neurons, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {neurons:3d} neuroni: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 3. ANALISI EFFETTO PROFONDITÃ€  \n",
    "print(f\"\\nðŸ“ˆ Effetto ProfonditÃ  (100 neuroni, Adam, LR=0.01):\")\n",
    "depth_analysis = []\n",
    "for layers in ['1l', '2l']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_{layers}_adam_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        layers_num = 1 if layers == '1l' else 2\n",
    "        depth_analysis.append((layers_num, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {layers_num} strato/i: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 4. ANALISI EFFETTO SOLVER\n",
    "print(f\"\\nâš™ï¸  Effetto Solver (100 neuroni, 1 strato, LR=0.01):\")\n",
    "solver_analysis = []\n",
    "for solver in ['sgd', 'adam']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_1l_{solver}_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        solver_analysis.append((solver, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {solver.upper():4s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 5. ANALISI EFFETTO LEARNING RATE\n",
    "print(f\"\\nðŸŽ¯ Effetto Learning Rate (100 neuroni, 1 strato, Adam):\")\n",
    "lr_analysis = []\n",
    "for lr in ['0.001', '0.01', '0.1']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_1l_adam_lr{lr}' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        lr_analysis.append((float(lr), result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   LR {lr:5s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 6. STATISTICHE GENERALI\n",
    "test_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "overfits = [r['overfitting'] for r in mlp_results]\n",
    "times = [r['training_time'] for r in mlp_results]\n",
    "\n",
    "print(f\"\\nðŸ“‹ Statistiche Generali MLP:\")\n",
    "print(f\"   Test Accuracy: Î¼={np.mean(test_accs):.4f} Â± {np.std(test_accs):.4f} | Range: [{np.min(test_accs):.4f}, {np.max(test_accs):.4f}]\")\n",
    "print(f\"   Overfitting:   Î¼={np.mean(overfits):.4f} Â± {np.std(overfits):.4f} | Range: [{np.min(overfits):.4f}, {np.max(overfits):.4f}]\")\n",
    "print(f\"   Training Time: Î¼={np.mean(times):.1f}s Â± {np.std(times):.1f}s | Range: [{np.min(times):.1f}s, {np.max(times):.1f}s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi dettagliata risultati CNN  \n",
    "print(\"\\n=== ANALISI DETTAGLIATA RISULTATI CNN ===\")\n",
    "\n",
    "# 1. RANKING GENERALE CNN\n",
    "cnn_sorted = sorted(cnn_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"\\nðŸ† TOP 5 CNN (Test Accuracy):\")\n",
    "for i, result in enumerate(cnn_sorted[:5]):\n",
    "    name_parts = result['name'].split('_')\n",
    "    filters = name_parts[1]\n",
    "    arch = name_parts[2]\n",
    "    opt = name_parts[3]\n",
    "    lr = name_parts[4]\n",
    "    \n",
    "    print(f\"{i+1:2d}. {result['name']:30} \"\n",
    "          f\"Acc: {result['test_accuracy']:.4f} \"\n",
    "          f\"Ovf: {result['overfitting']:+.4f} \"\n",
    "          f\"Time: {result['training_time']:4.1f}s\")\n",
    "\n",
    "# 2. ANALISI EFFETTO FILTRI\n",
    "print(f\"\\nðŸ” Effetto Numero Filtri (baseline, Adam, LR=0.001):\")\n",
    "for filters in ['16f', '32f', '64f']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_{filters}_baseline_adam_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        filters_num = int(filters.replace('f', ''))\n",
    "        print(f\"   {filters_num:2d} filtri: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 3. ANALISI EFFETTO ARCHITETTURA\n",
    "print(f\"\\nðŸ—ï¸  Effetto Architettura (32 filtri, Adam, LR=0.001):\")\n",
    "for arch in ['baseline', 'extended']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_32f_{arch}_adam_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        print(f\"   {arch:8s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 4. ANALISI EFFETTO OPTIMIZER CNN\n",
    "print(f\"\\nâš™ï¸  Effetto Optimizer (32 filtri, baseline, LR=0.001):\")\n",
    "for opt in ['adam', 'sgd']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_32f_baseline_{opt}_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        print(f\"   {opt.upper():4s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 5. STATISTICHE GENERALI CNN\n",
    "cnn_test_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "cnn_overfits = [r['overfitting'] for r in cnn_results]\n",
    "cnn_times = [r['training_time'] for r in cnn_results]\n",
    "\n",
    "print(f\"\\nðŸ“‹ Statistiche Generali CNN:\")\n",
    "print(f\"   Test Accuracy: Î¼={np.mean(cnn_test_accs):.4f} Â± {np.std(cnn_test_accs):.4f} | Range: [{np.min(cnn_test_accs):.4f}, {np.max(cnn_test_accs):.4f}]\")\n",
    "print(f\"   Overfitting:   Î¼={np.mean(cnn_overfits):.4f} Â± {np.std(cnn_overfits):.4f} | Range: [{np.min(cnn_overfits):.4f}, {np.max(cnn_overfits):.4f}]\")\n",
    "print(f\"   Training Time: Î¼={np.mean(cnn_times):.1f}s Â± {np.std(cnn_times):.1f}s | Range: [{np.min(cnn_times):.1f}s, {np.max(cnn_times):.1f}s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto finale MLP vs CNN\n",
    "print(\"\\n=== CONFRONTO FINALE MLP vs CNN ===\")\n",
    "\n",
    "# Miglior MLP\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Miglior MLP:\")\n",
    "print(f\"   Nome: {best_mlp['name']}\")\n",
    "print(f\"   Test Accuracy: {best_mlp['test_accuracy']:.4f}\")\n",
    "print(f\"   Train Accuracy: {best_mlp['train_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting: {best_mlp['overfitting']:+.4f}\")\n",
    "print(f\"   Training Time: {best_mlp['training_time']:.1f}s\")\n",
    "print(f\"   Architettura: {best_mlp['config']['hidden_layer_sizes']}\")\n",
    "print(f\"   Solver: {best_mlp['config']['solver']}, LR: {best_mlp['config']['learning_rate_init']}\")\n",
    "\n",
    "# Miglior CNN\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Miglior CNN:\")\n",
    "print(f\"   Nome: {best_cnn['name']}\")\n",
    "print(f\"   Test Accuracy: {best_cnn['test_accuracy']:.4f}\")\n",
    "print(f\"   Train Accuracy: {best_cnn['train_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting: {best_cnn['overfitting']:+.4f}\")\n",
    "print(f\"   Training Time: {best_cnn['training_time']:.1f}s\")\n",
    "print(f\"   Architettura: {best_cnn['config']['architecture']}, {best_cnn['config']['filters']} filtri\")\n",
    "print(f\"   Optimizer: {best_cnn['config']['optimizer']}, LR: {best_cnn['config']['learning_rate']}\")\n",
    "\n",
    "# Confronto diretto\n",
    "print(f\"\\nâš–ï¸  Confronto Prestazioni:\")\n",
    "print(f\"   Vantaggio CNN accuratezza: {best_cnn['test_accuracy'] - best_mlp['test_accuracy']:+.4f}\")\n",
    "print(f\"   Rapporto training time: {best_cnn['training_time'] / best_mlp['training_time']:.1f}Ã— (CNN/MLP)\")\n",
    "\n",
    "# Statistiche aggregate\n",
    "mlp_mean_acc = np.mean([r['test_accuracy'] for r in mlp_results])\n",
    "cnn_mean_acc = np.mean([r['test_accuracy'] for r in cnn_results])\n",
    "\n",
    "print(f\"\\nðŸ“Š Confronto Medio:\")\n",
    "print(f\"   MLP medio: {mlp_mean_acc:.4f}\")\n",
    "print(f\"   CNN medio: {cnn_mean_acc:.4f}\")\n",
    "print(f\"   Vantaggio medio CNN: {cnn_mean_acc - mlp_mean_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8eafb6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Visualizzazioni Scientifiche\n",
    "\n",
    "Creazione di grafici professionali per l'analisi dei risultati, \n",
    "seguendo lo stile scientifico dei laboratori con focus su \n",
    "interpretabilitÃ  e insights quantitativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione visualizzazioni complete\n",
    "print(\"Generazione visualizzazioni scientifiche...\")\n",
    "\n",
    "# Setup figura principale\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Colori consistenti per le visualizzazioni\n",
    "colors_mlp = '#2E86AB'  # Blu\n",
    "colors_cnn = '#A23B72'  # Rosso/Magenta\n",
    "colors_mixed = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "# 1. RANKING PERFORMANCE (top-left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "# Top 8 MLP\n",
    "top_mlp = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)[:8]\n",
    "names = [r['name'].replace('MLP_', '').replace('_lr', '\\nLR') for r in top_mlp]\n",
    "accs = [r['test_accuracy'] for r in top_mlp]\n",
    "\n",
    "bars = ax1.bar(range(len(names)), accs, color=colors_mlp, alpha=0.7)\n",
    "ax1.set_xlabel('Configurazione MLP')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Top 8 Performance MLP', fontweight='bold')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Valori sulle barre\n",
    "for bar, acc in zip(bars, accs):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "            f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. ANALISI PARAMETRICA MLP (top-center)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "# Effetto numero neuroni\n",
    "neurons_data = {}\n",
    "for result in mlp_results:\n",
    "    config = result['config']\n",
    "    if config['solver'] == 'adam' and config['learning_rate_init'] == 0.01:\n",
    "        neurons = config['hidden_layer_sizes'][0]  # primo strato\n",
    "        layers = len(config['hidden_layer_sizes'])\n",
    "        key = f\"{neurons}n_{layers}l\"\n",
    "        if key not in neurons_data:\n",
    "            neurons_data[key] = []\n",
    "        neurons_data[key].append(result['test_accuracy'])\n",
    "\n",
    "# Plot effetto neuroni\n",
    "if neurons_data:\n",
    "    keys = sorted(neurons_data.keys())\n",
    "    means = [np.mean(neurons_data[k]) for k in keys]\n",
    "    stds = [np.std(neurons_data[k]) for k in keys]\n",
    "    \n",
    "    x_pos = range(len(keys))\n",
    "    bars = ax2.bar(x_pos, means, yerr=stds, capsize=5, \n",
    "                  color=colors_mlp, alpha=0.7, error_kw={'linewidth': 2})\n",
    "    \n",
    "    ax2.set_xlabel('Configurazione (Neuroni + Strati)')\n",
    "    ax2.set_ylabel('Test Accuracy (mean Â± std)')\n",
    "    ax2.set_title('Effetto Neuroni e Strati (MLP)', fontweight='bold')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(keys, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. CURVE DI LOSS (top-right)  \n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "# Mostra curve di loss per top 3 MLP\n",
    "top_3_mlp = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)[:3]\n",
    "\n",
    "for i, result in enumerate(top_3_mlp):\n",
    "    if result['loss_curve'] is not None:\n",
    "        label = result['name'].replace('MLP_', '').replace('_lr', ' LR')\n",
    "        ax3.plot(result['loss_curve'], label=label, linewidth=2, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Iterazioni')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Curve di Loss - Top 3 MLP', fontweight='bold')\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. CONFRONTO MLP vs CNN (middle-left)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "mlp_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "cnn_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "\n",
    "# Boxplot comparison\n",
    "box_data = [mlp_accs, cnn_accs]\n",
    "bp = ax4.boxplot(box_data, labels=['MLP', 'CNN'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor(colors_mlp)\n",
    "bp['boxes'][1].set_facecolor(colors_cnn)\n",
    "\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "ax4.set_title('Distribuzione Performance:\\nMLP vs CNN', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistiche testuali\n",
    "mlp_mean = np.mean(mlp_accs)\n",
    "cnn_mean = np.mean(cnn_accs)\n",
    "ax4.text(0.02, 0.98, f'MLP: Î¼={mlp_mean:.3f}\\nCNN: Î¼={cnn_mean:.3f}\\nÎ”={cnn_mean-mlp_mean:+.3f}', \n",
    "        transform=ax4.transAxes, va='top', fontsize=9,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. ANALISI OVERFITTING (middle-center)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "all_results = mlp_results + cnn_results\n",
    "train_accs = [r['train_accuracy'] for r in all_results]\n",
    "test_accs = [r['test_accuracy'] for r in all_results]\n",
    "\n",
    "# Colori per tipo\n",
    "colors = [colors_mlp if 'MLP' in r['name'] else colors_cnn for r in all_results]\n",
    "\n",
    "scatter = ax5.scatter(train_accs, test_accs, c=colors, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Linea perfetta generalizzazione\n",
    "min_acc = min(min(train_accs), min(test_accs))\n",
    "max_acc = max(max(train_accs), max(test_accs))\n",
    "ax5.plot([min_acc, max_acc], [min_acc, max_acc], 'k--', alpha=0.5, linewidth=2, label='Perfetta Generalizzazione')\n",
    "\n",
    "ax5.set_xlabel('Train Accuracy')\n",
    "ax5.set_ylabel('Test Accuracy') \n",
    "ax5.set_title('Analisi Overfitting', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Legenda colori\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_mlp, label='MLP'),\n",
    "                   Patch(facecolor=colors_cnn, label='CNN')]\n",
    "ax5.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "# 6. EFFICIENZA COMPUTAZIONALE (middle-right)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "test_accs_all = [r['test_accuracy'] for r in all_results]\n",
    "times_all = [r['training_time'] for r in all_results]\n",
    "colors_all = [colors_mlp if 'MLP' in r['name'] else colors_cnn for r in all_results]\n",
    "\n",
    "scatter = ax6.scatter(times_all, test_accs_all, c=colors_all, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax6.set_xlabel('Training Time (secondi)')\n",
    "ax6.set_ylabel('Test Accuracy')\n",
    "ax6.set_title('Efficienza:\\nAccuracy vs Training Time', fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Identificazione miglior rapporto\n",
    "# Calcola efficiency score (accuracy/time ratio normalizzato)\n",
    "max_acc = max(test_accs_all)\n",
    "min_time = min(times_all)\n",
    "efficiency_scores = [(acc/max_acc) / (time/min_time) for acc, time in zip(test_accs_all, times_all)]\n",
    "best_idx = np.argmax(efficiency_scores)\n",
    "\n",
    "ax6.scatter(times_all[best_idx], test_accs_all[best_idx], \n",
    "           s=200, facecolors='none', edgecolors='gold', linewidth=3, label='Migliore Efficienza')\n",
    "ax6.legend()\n",
    "\n",
    "# 7. HEATMAP PERFORMANCE MLP (bottom-left)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "# Crea matrice performance per heatmap\n",
    "solvers = ['sgd', 'adam']\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Matrice per 100 neuroni, 1 strato\n",
    "heatmap_data = np.zeros((len(solvers), len(lrs)))\n",
    "\n",
    "for i, solver in enumerate(solvers):\n",
    "    for j, lr in enumerate(lrs):\n",
    "        matching = [r for r in mlp_results \n",
    "                   if f'_100n_1l_{solver}_lr{lr}' in r['name']]\n",
    "        if matching:\n",
    "            heatmap_data[i, j] = matching[0]['test_accuracy']\n",
    "        else:\n",
    "            heatmap_data[i, j] = np.nan\n",
    "\n",
    "im = ax7.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "ax7.set_xticks(range(len(lrs)))\n",
    "ax7.set_xticklabels([f'{lr}' for lr in lrs])\n",
    "ax7.set_yticks(range(len(solvers)))\n",
    "ax7.set_yticklabels([s.upper() for s in solvers])\n",
    "ax7.set_xlabel('Learning Rate')\n",
    "ax7.set_ylabel('Solver')\n",
    "ax7.set_title('Heatmap Performance MLP\\n(100 neuroni, 1 strato)', fontweight='bold')\n",
    "\n",
    "# Aggiunta valori nelle celle\n",
    "for i in range(len(solvers)):\n",
    "    for j in range(len(lrs)):\n",
    "        if not np.isnan(heatmap_data[i, j]):\n",
    "            text = ax7.text(j, i, f'{heatmap_data[i, j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax7, shrink=0.8)\n",
    "\n",
    "# 8. SUMMARY STATISTICS (bottom-center & bottom-right)\n",
    "ax8 = fig.add_subplot(gs[2, 1:])\n",
    "ax8.axis('off')\n",
    "\n",
    "# Testo riassuntivo\n",
    "summary_text = \"RISULTATI PUNTO A - ANALISI ARCHITETTURALE\\n\\n\"\n",
    "\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "summary_text += f\"ðŸ† MIGLIOR MLP:\\n\"\n",
    "summary_text += f\"   â€¢ Configurazione: {best_mlp['name']}\\n\"\n",
    "summary_text += f\"   â€¢ Test Accuracy: {best_mlp['test_accuracy']:.4f}\\n\"\n",
    "summary_text += f\"   â€¢ Overfitting: {best_mlp['overfitting']:+.4f}\\n\"\n",
    "summary_text += f\"   â€¢ Training Time: {best_mlp['training_time']:.1f}s\\n\\n\"\n",
    "\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "summary_text += f\"ðŸ† MIGLIOR CNN:\\n\"\n",
    "summary_text += f\"   â€¢ Configurazione: {best_cnn['name']}\\n\"\n",
    "summary_text += f\"   â€¢ Test Accuracy: {best_cnn['test_accuracy']:.4f}\\n\"\n",
    "summary_text += f\"   â€¢ Overfitting: {best_cnn['overfitting']:+.4f}\\n\"\n",
    "summary_text += f\"   â€¢ Training Time: {best_cnn['training_time']:.1f}s\\n\\n\"\n",
    "\n",
    "summary_text += f\"âš–ï¸ CONFRONTO:\\n\"\n",
    "summary_text += f\"   â€¢ Vantaggio CNN: {best_cnn['test_accuracy'] - best_mlp['test_accuracy']:+.4f}\\n\"\n",
    "summary_text += f\"   â€¢ Rapporto tempo: {best_cnn['training_time'] / best_mlp['training_time']:.1f}Ã— (CNN/MLP)\\n\\n\"\n",
    "\n",
    "# Insights principali\n",
    "summary_text += f\"ðŸ” INSIGHTS PRINCIPALI:\\n\"\n",
    "mlp_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "cnn_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "summary_text += f\"   â€¢ MLP: range accuracy {np.min(mlp_accs):.3f} - {np.max(mlp_accs):.3f}\\n\"\n",
    "summary_text += f\"   â€¢ CNN: range accuracy {np.min(cnn_accs):.3f} - {np.max(cnn_accs):.3f}\\n\"\n",
    "summary_text += f\"   â€¢ Esperimenti totali: {len(mlp_results)} MLP + {len(cnn_results)} CNN\\n\"\n",
    "summary_text += f\"   â€¢ Tempo totale: ~{(np.sum([r['training_time'] for r in all_results])/60):.0f} minuti\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.suptitle('PUNTO A: ANALISI ARCHITETTURALE COMPLETA - RISULTATI SISTEMATICI', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizzazioni complete generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed67a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# Conclusioni del Punto A\n",
    "\n",
    "## Risultati Principali della Sperimentazione Sistematica\n",
    "\n",
    "### ðŸŽ¯ Configurazioni Ottimali Identificate\n",
    "\n",
    "**Migliori Performance:**\n",
    "- I modelli piÃ¹ performanti emergono dalla sperimentazione sistematica di 60 configurazioni\n",
    "- La metodologia ha permesso di identificare combinazioni ottimali di iperparametri\n",
    "- I risultati forniscono una base solida per i punti successivi del progetto\n",
    "\n",
    "### ðŸ“Š Insights dall'Analisi Parametrica\n",
    "\n",
    "**1. Effetto del Numero di Neuroni (MLP)**\n",
    "- Pattern sistematico nell'incremento delle performance con piÃ¹ neuroni\n",
    "- Identificazione del punto di saturazione oltre il quale i miglioramenti sono marginali\n",
    "- Bilanciamento tra capacitÃ  del modello e rischio di overfitting\n",
    "\n",
    "**2. Effetto della ProfonditÃ **\n",
    "- Confronto quantitativo tra architetture a 1 vs 2 strati nascosti\n",
    "- Analisi del trade-off complessitÃ  vs generalizzazione\n",
    "- Evidenza empirica per le scelte architetturali\n",
    "\n",
    "**3. Impatto degli Algoritmi di Ottimizzazione**\n",
    "- Confronto sistematico SGD vs Adam sia per MLP che CNN\n",
    "- Analisi dell'interazione tra optimizer e learning rate\n",
    "- Identificazione delle configurazioni piÃ¹ stabili\n",
    "\n",
    "**4. SensibilitÃ  al Learning Rate**\n",
    "- Range testing sistematico su tre ordini di grandezza\n",
    "- Identificazione della zona ottimale per convergenza\n",
    "- Correlazione con architettura e tipo di modello\n",
    "\n",
    "### ðŸ—ï¸ Confronto Architetturale MLP vs CNN\n",
    "\n",
    "**Performance Relative:**\n",
    "- Valutazione quantitativa del vantaggio CNN per dati visivi\n",
    "- Analisi del rapporto efficienza computazionale vs performance\n",
    "- Trade-off tra interpretabilitÃ  (MLP) e specializzazione (CNN)\n",
    "\n",
    "**Robustezza e Generalizzazione:**\n",
    "- Confronto dei livelli di overfitting tra le architetture\n",
    "- StabilitÃ  dei risultati attraverso configurazioni multiple\n",
    "- Implications per applicazioni reali\n",
    "\n",
    "### âš¡ Efficienza Computazionale\n",
    "\n",
    "**Training Time Analysis:**\n",
    "- Quantificazione dei costi computazionali per ogni configurazione\n",
    "- Identificazione del miglior rapporto performance/tempo\n",
    "- Considerazioni pratiche per deployment\n",
    "\n",
    "### ðŸ” Metodologia e RiproducibilitÃ \n",
    "\n",
    "**Rigore Sperimentale:**\n",
    "- 60 esperimenti sistematici con parametri controllati\n",
    "- Early stopping per efficienza e prevenzione overfitting\n",
    "- Documentazione completa per riproducibilitÃ \n",
    "\n",
    "**Validazione Statistica:**\n",
    "- Analisi delle distribuzioni di performance\n",
    "- Identificazione di pattern significativi vs variabilitÃ  casuale\n",
    "- Confidence negli insights derivati\n",
    "\n",
    "### ðŸŽ“ Contributi all'Obiettivo Didattico\n",
    "\n",
    "**Collegamento con i Laboratori:**\n",
    "- Estensione naturale della metodologia del Lab 2 (MLP)\n",
    "- Integrazione con i concetti del Lab 3 (CNN)\n",
    "- Approccio sistematico vs esplorativo\n",
    "\n",
    "**Preparazione per Punti Successivi:**\n",
    "- Identificazione dei modelli ottimali per l'analisi degli errori (Punto B)\n",
    "- Baseline solide per gli esperimenti di robustezza (Punti C-E)\n",
    "- Framework metodologico per estensioni future\n",
    "\n",
    "---\n",
    "\n",
    "## Modelli Selezionati per Continuazione\n",
    "\n",
    "**Per Punto B (Analisi Errori):** \n",
    "Il miglior MLP identificato verrÃ  utilizzato per l'analisi dettagliata degli errori di classificazione.\n",
    "\n",
    "**Per Punti C-E (Robustezza e Training):** \n",
    "La configurazione con il miglior bilanciamento performance/efficienza guiderÃ  gli esperimenti di robustezza al rumore e training con dataset ridotti.\n",
    "\n",
    "Questi risultati costituiscono una solida foundation empirica per il resto del progetto, \n",
    "dimostrando l'efficacia dell'approccio sistematico nell'identificazione di configurazioni \n",
    "ottimali per il riconoscimento di cifre manoscritte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio modelli ottimali per punti successivi\n",
    "print(\"=== PREPARAZIONE PER PUNTI SUCCESSIVI ===\")\n",
    "\n",
    "# Identificazione modelli ottimali\n",
    "selected_models = {}\n",
    "\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "selected_models['best_mlp'] = {\n",
    "    'model': best_mlp['model'],\n",
    "    'config': best_mlp['config'],\n",
    "    'performance': {\n",
    "        'test_accuracy': best_mlp['test_accuracy'],\n",
    "        'train_accuracy': best_mlp['train_accuracy'],\n",
    "        'overfitting': best_mlp['overfitting']\n",
    "    },\n",
    "    'name': best_mlp['name']\n",
    "}\n",
    "print(f\"âœ… Miglior MLP selezionato per Punto B: {best_mlp['name']}\")\n",
    "print(f\"   Test Accuracy: {best_mlp['test_accuracy']:.4f}\")\n",
    "\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "selected_models['best_cnn'] = {\n",
    "    'model': best_cnn['model'],\n",
    "    'config': best_cnn['config'],\n",
    "    'performance': {\n",
    "        'test_accuracy': best_cnn['test_accuracy'],\n",
    "        'train_accuracy': best_cnn['train_accuracy'],\n",
    "        'overfitting': best_cnn['overfitting']\n",
    "    },\n",
    "    'name': best_cnn['name']\n",
    "}\n",
    "print(f\"âœ… Miglior CNN selezionato per Punti C-E: {best_cnn['name']}\")\n",
    "print(f\"   Test Accuracy: {best_cnn['test_accuracy']:.4f}\")\n",
    "\n",
    "# Salvataggio dati preprocessati\n",
    "selected_models['data'] = {\n",
    "    'x_train_mlp': x_train_mlp,\n",
    "    'x_test_mlp': x_test_mlp,\n",
    "    'x_train_cnn': x_train_cnn,\n",
    "    'x_test_cnn': x_test_cnn,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "# Riepilogo finale\n",
    "print(f\"\\nðŸ“‹ RIEPILOGO PUNTO A:\")\n",
    "print(f\"   â€¢ Esperimenti MLP: {len(mlp_configs)} configurazioni\")\n",
    "print(f\"   â€¢ Esperimenti CNN: {len(cnn_configs)} configurazioni\")\n",
    "print(f\"   â€¢ Successi MLP: {len(mlp_results)}/{len(mlp_configs)}\")\n",
    "print(f\"   â€¢ Successi CNN: {len(cnn_results)}/{len(cnn_configs)}\")\n",
    "print(f\"   â€¢ Modelli selezionati per continuazione: {len(selected_models)-1}\")  # -1 per 'data'\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PUNTO A COMPLETATO CON SUCCESSO!\")\n",
    "print(f\"   Base solida per implementazione Punti B-E del progetto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badae218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Mini Progetto IA",
   "language": "python",
   "name": "mini-progetto-ia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
