{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77f4b15",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Mini Progetto Intelligenza Artificiale - Riconoscimento cifre manoscritte\n",
    "\n",
    "**Nome:** [Inserire nome]  \n",
    "**Cognome:** [Inserire cognome]  \n",
    "**Matricola:** [Inserire matricola]  \n",
    "**Data consegna:** [Inserire data]\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo progetto esploreremo il riconoscimento di cifre manoscritte utilizzando il dataset MNIST, implementando simulazioni per studiare come diversi fattori influenzano le prestazioni dei modelli di deep learning. Analizzeremo in particolare l'impatto degli iperparametri, la robustezza al rumore e l'effetto della quantità di dati di training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abb844",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Importazione delle librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe329e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione per riproducibilità\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dae275",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Caricamento e preparazione del dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175256c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset MNIST...\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dataset MNIST\n",
    "print(\"Caricamento dataset MNIST...\")\n",
    "mnist_tr = MNIST(root=\"./data\", train=True, download=True)\n",
    "mnist_te = MNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato: 60000 train, 10000 test\n",
      "Forma dati MLP: (60000, 784)\n",
      "Forma dati CNN: (60000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Conversione in array numpy\n",
    "mnist_tr_data, mnist_tr_labels = mnist_tr.data.numpy(), mnist_tr.targets.numpy()\n",
    "mnist_te_data, mnist_te_labels = mnist_te.data.numpy(), mnist_te.targets.numpy()\n",
    "\n",
    "# Preprocessing per MLP (vettorizzazione e normalizzazione)\n",
    "x_tr = mnist_tr_data.reshape(60000, 28 * 28) / 255.0\n",
    "x_te = mnist_te_data.reshape(10000, 28 * 28) / 255.0\n",
    "\n",
    "# Preprocessing per CNN (mantenendo formato 2D)\n",
    "x_tr_conv = x_tr.reshape(-1, 28, 28, 1)\n",
    "x_te_conv = x_te.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Dataset caricato: {x_tr.shape[0]} train, {x_te.shape[0]} test\")\n",
    "print(f\"Forma dati MLP: {x_tr.shape}\")\n",
    "print(f\"Forma dati CNN: {x_tr_conv.shape}\")\n",
    "\n",
    "# Visualizzazione esempi del dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "fig.suptitle('Dataset MNIST - Esempi per Cifra', fontsize=14)\n",
    "\n",
    "for digit in range(10):\n",
    "    idx = np.where(mnist_tr_labels == digit)[0][0]\n",
    "    ax = axes[digit//5, digit%5]\n",
    "    ax.imshow(mnist_tr_data[idx], cmap='gray')\n",
    "    ax.set_title(f'Cifra {digit}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242b1b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto A: Effetto degli iperparametri sulle prestazioni\n",
    "\n",
    "Analizziamo sistematicamente come variano le prestazioni dei modelli MLP e CNN al variare degli iperparametri chiave. \n",
    "Confronteremo 18 configurazioni MLP e 6 configurazioni CNN per un totale di 24 esperimenti mirati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7af60f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Configurazione esperimenti sistematici\n",
    "\n",
    "**MLP (18 esperimenti):**\n",
    "- Neuroni per strato: 50, 100, 250\n",
    "- Numero strati: 1 vs 2 strati nascosti\n",
    "- Learning rate: 0.001, 0.01, 0.1\n",
    "\n",
    "**CNN (6 esperimenti):**\n",
    "- Filtri: 32 (fisso)\n",
    "- Architettura: baseline vs extended\n",
    "- Learning rate: 0.001, 0.01, 0.1\n",
    "\n",
    "**Parametri di training giustificati:**\n",
    "- MLP: max_iter=100 (sufficient convergence), early_stopping=True (prevent overfitting), \n",
    "  validation_fraction=0.1 (standard split), tol=0.001 (reasonable precision), n_iter_no_change=10 (adequate patience)\n",
    "- CNN: epochs=20 (balance speed/convergence), batch_size=128 (memory/speed trade-off), \n",
    "  validation_split=0.1 (consistency with MLP), patience=5 (faster CNN convergence), min_delta=0.001 (same precision as MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924122e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_experiment_header(exp_num, total, model_type, config):\n",
    "    \"\"\"Stampa header consistente per ogni esperimento\"\"\"\n",
    "    print(f\"\\n[{exp_num:2d}/{total}] {model_type}: {config}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def print_experiment_results(results):\n",
    "    \"\"\"Stampa risultati in formato consistente\"\"\"\n",
    "    print(f\"Train Acc: {results['train_accuracy']:.4f} | Test Acc: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Time: {results['training_time']:6.1f}s | Iterations: {results['iterations']:3d}\")\n",
    "    print(f\"Overfitting: {results['overfitting']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84403193",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIZIO ESPERIMENTI MLP\n",
      "============================================================\n",
      "\n",
      "[ 1/18] MLP: 50n_1L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9891 | Test Acc: 0.9707\n",
      "Time:    6.3s | Iterations:  24\n",
      "Overfitting: +0.0184\n",
      "\n",
      "[ 2/18] MLP: 50n_1L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9844 | Test Acc: 0.9697\n",
      "Time:    4.6s | Iterations:  17\n",
      "Overfitting: +0.0147\n",
      "\n",
      "[ 3/18] MLP: 50n_1L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9202 | Test Acc: 0.9123\n",
      "Time:    5.7s | Iterations:  20\n",
      "Overfitting: +0.0079\n",
      "\n",
      "[ 4/18] MLP: 50n_2L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9905 | Test Acc: 0.9729\n",
      "Time:    8.5s | Iterations:  27\n",
      "Overfitting: +0.0176\n",
      "\n",
      "[ 5/18] MLP: 50n_2L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9863 | Test Acc: 0.9695\n",
      "Time:    5.6s | Iterations:  19\n",
      "Overfitting: +0.0168\n",
      "\n",
      "[ 6/18] MLP: 50n_2L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.8471 | Test Acc: 0.8467\n",
      "Time:    4.5s | Iterations:  16\n",
      "Overfitting: +0.0004\n",
      "\n",
      "[ 7/18] MLP: 100n_1L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9971 | Test Acc: 0.9771\n",
      "Time:   10.6s | Iterations:  26\n",
      "Overfitting: +0.0201\n",
      "\n",
      "[ 8/18] MLP: 100n_1L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9909 | Test Acc: 0.9734\n",
      "Time:    9.0s | Iterations:  19\n",
      "Overfitting: +0.0175\n",
      "\n",
      "[ 9/18] MLP: 100n_1L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9168 | Test Acc: 0.9148\n",
      "Time:    6.1s | Iterations:  13\n",
      "Overfitting: +0.0020\n",
      "\n",
      "[10/18] MLP: 100n_2L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9967 | Test Acc: 0.9786\n",
      "Time:   10.2s | Iterations:  20\n",
      "Overfitting: +0.0181\n",
      "\n",
      "[11/18] MLP: 100n_2L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9934 | Test Acc: 0.9739\n",
      "Time:   22.2s | Iterations:  43\n",
      "Overfitting: +0.0195\n",
      "\n",
      "[12/18] MLP: 100n_2L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.8248 | Test Acc: 0.8212\n",
      "Time:    7.4s | Iterations:  14\n",
      "Overfitting: +0.0036\n",
      "\n",
      "[13/18] MLP: 250n_1L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9981 | Test Acc: 0.9810\n",
      "Time:   21.8s | Iterations:  24\n",
      "Overfitting: +0.0171\n",
      "\n",
      "[14/18] MLP: 250n_1L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9913 | Test Acc: 0.9752\n",
      "Time:   21.8s | Iterations:  25\n",
      "Overfitting: +0.0161\n",
      "\n",
      "[15/18] MLP: 250n_1L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9160 | Test Acc: 0.9147\n",
      "Time:   13.1s | Iterations:  15\n",
      "Overfitting: +0.0013\n",
      "\n",
      "[16/18] MLP: 250n_2L_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9965 | Test Acc: 0.9788\n",
      "Time:   25.3s | Iterations:  19\n",
      "Overfitting: +0.0177\n",
      "\n",
      "[17/18] MLP: 250n_2L_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9924 | Test Acc: 0.9776\n",
      "Time:   37.9s | Iterations:  31\n",
      "Overfitting: +0.0148\n",
      "\n",
      "[18/18] MLP: 250n_2L_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.7662 | Test Acc: 0.7577\n",
      "Time:   31.9s | Iterations:  27\n",
      "Overfitting: +0.0085\n",
      "\n",
      "MLP EXPERIMENTS COMPLETED: 18 configurations tested\n"
     ]
    }
   ],
   "source": [
    "# Esperimenti MLP sistematici\n",
    "neurons_list = [50, 100, 250]\n",
    "layers_list = [1, 2]  # numero di strati nascosti\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "mlp_results = []\n",
    "experiment_count = 0\n",
    "total_experiments = len(neurons_list) * len(layers_list) * len(learning_rates)\n",
    "\n",
    "print(\"INIZIO ESPERIMENTI MLP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for neurons in neurons_list:\n",
    "    for n_layers in layers_list:\n",
    "        for lr in learning_rates:\n",
    "            experiment_count += 1\n",
    "            \n",
    "            # Configurazione architettura\n",
    "            if n_layers == 1:\n",
    "                hidden_layers = (neurons,)\n",
    "                config_name = f\"{neurons}n_1L_lr{lr}\"\n",
    "            else:\n",
    "                hidden_layers = (neurons, neurons)\n",
    "                config_name = f\"{neurons}n_2L_lr{lr}\"\n",
    "            \n",
    "            print_experiment_header(experiment_count, total_experiments, \"MLP\", config_name)\n",
    "            \n",
    "            # Training MLP\n",
    "            mlp = MLPClassifier(\n",
    "                hidden_layer_sizes=hidden_layers,\n",
    "                learning_rate_init=lr,\n",
    "                max_iter=100,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1,\n",
    "                tol=0.001,\n",
    "                n_iter_no_change=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            mlp.fit(x_tr, mnist_tr_labels)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            train_acc = mlp.score(x_tr, mnist_tr_labels)\n",
    "            test_acc = mlp.score(x_te, mnist_te_labels)\n",
    "            \n",
    "            results = {\n",
    "                'model_type': 'MLP',\n",
    "                'config_name': config_name,\n",
    "                'neurons': neurons,\n",
    "                'n_layers': n_layers,\n",
    "                'learning_rate': lr,\n",
    "                'hidden_layers': hidden_layers,\n",
    "                'train_accuracy': train_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'overfitting': train_acc - test_acc,\n",
    "                'training_time': training_time,\n",
    "                'iterations': mlp.n_iter_,\n",
    "                'loss_curve': mlp.loss_curve_ if hasattr(mlp, 'loss_curve_') else [],\n",
    "                'total_parameters': sum([layer.size for layer in mlp.coefs_]) + sum([layer.size for layer in mlp.intercepts_])\n",
    "            }\n",
    "            \n",
    "            mlp_results.append(results)\n",
    "            print_experiment_results(results)\n",
    "\n",
    "print(f\"\\nMLP EXPERIMENTS COMPLETED: {len(mlp_results)} configurations tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f9a89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Esperimenti CNN sistematici\n",
    "def create_cnn_model(architecture_type, learning_rate):\n",
    "    \"\"\"Crea modello CNN con architettura specificata\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    if architecture_type == 'baseline':\n",
    "        # Architettura baseline del Lab 3\n",
    "        model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(50, activation='relu'))\n",
    "        \n",
    "    elif architecture_type == 'extended':\n",
    "        # Architettura estesa con pooling e più strati\n",
    "        model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.MaxPooling2D(2,2))\n",
    "        model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Configurazione optimizer\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b325f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "INIZIO ESPERIMENTI CNN\n",
      "============================================================\n",
      "\n",
      "[ 1/6] CNN: CNN_baseline_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9884 | Test Acc: 0.9809\n",
      "Time:   56.8s | Iterations:   7\n",
      "Overfitting: +0.0075\n",
      "\n",
      "[ 2/6] CNN: CNN_baseline_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9862 | Test Acc: 0.9748\n",
      "Time:   48.3s | Iterations:   6\n",
      "Overfitting: +0.0114\n",
      "\n",
      "[ 3/6] CNN: CNN_baseline_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.1022 | Test Acc: 0.1010\n",
      "Time:   48.8s | Iterations:   6\n",
      "Overfitting: +0.0012\n",
      "\n",
      "[ 4/6] CNN: CNN_extended_lr0.001\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9916 | Test Acc: 0.9882\n",
      "Time:  112.1s | Iterations:   7\n",
      "Overfitting: +0.0034\n",
      "\n",
      "[ 5/6] CNN: CNN_extended_lr0.01\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.9909 | Test Acc: 0.9841\n",
      "Time:  112.6s | Iterations:   8\n",
      "Overfitting: +0.0069\n",
      "\n",
      "[ 6/6] CNN: CNN_extended_lr0.1\n",
      "--------------------------------------------------\n",
      "Train Acc: 0.1022 | Test Acc: 0.1010\n",
      "Time:   87.3s | Iterations:   6\n",
      "Overfitting: +0.0012\n",
      "\n",
      "CNN EXPERIMENTS COMPLETED: 6 configurations tested\n"
     ]
    }
   ],
   "source": [
    "# Esperimenti CNN\n",
    "architectures = ['baseline', 'extended']\n",
    "learning_rates_cnn = [0.001, 0.01, 0.1]\n",
    "\n",
    "cnn_results = []\n",
    "cnn_experiment_count = 0\n",
    "total_cnn_experiments = len(architectures) * len(learning_rates_cnn)\n",
    "\n",
    "print(\"\\n\\nINIZIO ESPERIMENTI CNN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for arch in architectures:\n",
    "    for lr in learning_rates_cnn:\n",
    "        cnn_experiment_count += 1\n",
    "        config_name = f\"CNN_{arch}_lr{lr}\"\n",
    "        \n",
    "        print_experiment_header(cnn_experiment_count, total_cnn_experiments, \"CNN\", config_name)\n",
    "        \n",
    "        # Creazione e training CNN\n",
    "        model = create_cnn_model(arch, lr)\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            patience=5,\n",
    "            min_delta=0.001,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            x_tr_conv, mnist_tr_labels,\n",
    "            validation_split=0.1,\n",
    "            epochs=20,\n",
    "            batch_size=128,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Valutazione\n",
    "        train_loss, train_acc = model.evaluate(x_tr_conv, mnist_tr_labels, verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(x_te_conv, mnist_te_labels, verbose=0)\n",
    "        \n",
    "        results = {\n",
    "            'model_type': 'CNN',\n",
    "            'config_name': config_name,\n",
    "            'architecture': arch,\n",
    "            'learning_rate': lr,\n",
    "            'train_accuracy': train_acc,\n",
    "            'test_accuracy': test_acc,\n",
    "            'overfitting': train_acc - test_acc,\n",
    "            'training_time': training_time,\n",
    "            'iterations': len(history.history['loss']),\n",
    "            'loss_curve': history.history['loss'],\n",
    "            'val_loss_curve': history.history['val_loss'],\n",
    "            'total_parameters': model.count_params()\n",
    "        }\n",
    "        \n",
    "        cnn_results.append(results)\n",
    "        print_experiment_results(results)\n",
    "\n",
    "print(f\"\\nCNN EXPERIMENTS COMPLETED: {len(cnn_results)} configurations tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc9a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOTAL EXPERIMENTS COMPLETED: 24\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "Best MLP accuracy: 0.9810\n",
      "Best CNN accuracy: 0.9882\n",
      "Fastest training: 4.5s\n",
      "Slowest training: 112.6s\n"
     ]
    }
   ],
   "source": [
    "# Combinazione risultati per analisi\n",
    "all_results = mlp_results + cnn_results\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\nTOTAL EXPERIMENTS COMPLETED: {len(all_results)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(f\"Best MLP accuracy: {df_results[df_results['model_type']=='MLP']['test_accuracy'].max():.4f}\")\n",
    "print(f\"Best CNN accuracy: {df_results[df_results['model_type']=='CNN']['test_accuracy'].max():.4f}\")\n",
    "print(f\"Fastest training: {df_results['training_time'].min():.1f}s\")\n",
    "print(f\"Slowest training: {df_results['training_time'].max():.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25ef4c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 1: Effetto del Learning Rate (MLP)\n",
    "\n",
    "Analisi dell'impatto del learning rate sulla convergenza e stabilità del training per le reti MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b30d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE ANALYSIS:\n",
      "LR=0.001: Accuracy=0.9765, Time=13.8s\n",
      "LR=0.01:  Accuracy=0.9732, Time=16.8s\n",
      "LR=0.1:   Accuracy=0.8612, Time=11.4s\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Preparazione dati per learning rate analysis\n",
    "lr_001_data = [r for r in mlp_results if r['learning_rate'] == 0.001]\n",
    "lr_01_data = [r for r in mlp_results if r['learning_rate'] == 0.01]\n",
    "lr_1_data = [r for r in mlp_results if r['learning_rate'] == 0.1]\n",
    "\n",
    "# Calcolo medie per ogni learning rate\n",
    "lr_001_acc = np.mean([r['test_accuracy'] for r in lr_001_data])\n",
    "lr_01_acc = np.mean([r['test_accuracy'] for r in lr_01_data])\n",
    "lr_1_acc = np.mean([r['test_accuracy'] for r in lr_1_data])\n",
    "\n",
    "lr_001_time = np.mean([r['training_time'] for r in lr_001_data])\n",
    "lr_01_time = np.mean([r['training_time'] for r in lr_01_data])\n",
    "lr_1_time = np.mean([r['training_time'] for r in lr_1_data])\n",
    "\n",
    "# Visualizzazione loss curves rappresentative\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Subplot 1: Loss curves\n",
    "for i, (lr_data, color, label) in enumerate([(lr_001_data, 'green', 'LR=0.001'), \n",
    "                                           (lr_01_data, 'blue', 'LR=0.01'), \n",
    "                                           (lr_1_data, 'red', 'LR=0.1')]):\n",
    "    if lr_data and lr_data[0]['loss_curve']:\n",
    "        loss_curve = lr_data[0]['loss_curve']  # Prendo primo esempio rappresentativo\n",
    "        ax1.plot(range(len(loss_curve)), loss_curve, color=color, linewidth=2, label=label)\n",
    "\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Convergence Pattern by Learning Rate')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Accuracy vs Learning Rate\n",
    "learning_rates_plot = [0.001, 0.01, 0.1]\n",
    "accuracies = [lr_001_acc, lr_01_acc, lr_1_acc]\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "bars = ax2.bar(range(len(learning_rates_plot)), accuracies, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Learning Rate')\n",
    "ax2.set_ylabel('Average Test Accuracy')\n",
    "ax2.set_title('Test Accuracy by Learning Rate')\n",
    "ax2.set_xticks(range(len(learning_rates_plot)))\n",
    "ax2.set_xticklabels(['0.001', '0.01', '0.1'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotazioni valori\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{acc:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"LEARNING RATE ANALYSIS:\")\n",
    "print(f\"LR=0.001: Accuracy={lr_001_acc:.4f}, Time={lr_001_time:.1f}s\")\n",
    "print(f\"LR=0.01:  Accuracy={lr_01_acc:.4f}, Time={lr_01_time:.1f}s\") \n",
    "print(f\"LR=0.1:   Accuracy={lr_1_acc:.4f}, Time={lr_1_time:.1f}s\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5f62c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 2: Confronto Architetture MLP vs CNN\n",
    "\n",
    "Confronto diretto delle prestazioni tra le migliori configurazioni MLP e CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCHITECTURE COMPARISON:\n",
      "Best MLP: 250n_1L_lr0.001 - Accuracy: 0.9810\n",
      "Best CNN: CNN_extended_lr0.001 - Accuracy: 0.9882\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Selezione migliori configurazioni\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "# Preparazione dati per confronto\n",
    "mlp_configs = []\n",
    "mlp_accuracies = []\n",
    "mlp_times = []\n",
    "\n",
    "# Raggruppo MLP per configurazione (media dei learning rates)\n",
    "for neurons in neurons_list:\n",
    "    for n_layers in layers_list:\n",
    "        configs_group = [r for r in mlp_results if r['neurons'] == neurons and r['n_layers'] == n_layers]\n",
    "        if configs_group:\n",
    "            avg_acc = np.mean([r['test_accuracy'] for r in configs_group])\n",
    "            avg_time = np.mean([r['training_time'] for r in configs_group])\n",
    "            config_name = f\"MLP({neurons}{'x2' if n_layers==2 else ''})\"\n",
    "            mlp_configs.append(config_name)\n",
    "            mlp_accuracies.append(avg_acc)\n",
    "            mlp_times.append(avg_time)\n",
    "\n",
    "# CNN data\n",
    "cnn_configs = []\n",
    "cnn_accuracies = []\n",
    "cnn_times = []\n",
    "\n",
    "for arch in architectures:\n",
    "    configs_group = [r for r in cnn_results if r['architecture'] == arch]\n",
    "    if configs_group:\n",
    "        avg_acc = np.mean([r['test_accuracy'] for r in configs_group])\n",
    "        avg_time = np.mean([r['training_time'] for r in configs_group])\n",
    "        config_name = f\"CNN({arch})\"\n",
    "        cnn_configs.append(config_name)\n",
    "        cnn_accuracies.append(avg_acc)\n",
    "        cnn_times.append(avg_time)\n",
    "\n",
    "# Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar chart con colori distinti\n",
    "x_mlp = np.arange(len(mlp_configs))\n",
    "x_cnn = np.arange(len(mlp_configs), len(mlp_configs) + len(cnn_configs))\n",
    "\n",
    "bars_mlp = ax.bar(x_mlp, mlp_accuracies, color='lightblue', alpha=0.8, label='MLP')\n",
    "bars_cnn = ax.bar(x_cnn, cnn_accuracies, color='salmon', alpha=0.8, label='CNN')\n",
    "\n",
    "ax.set_xlabel('Architecture')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('MLP vs CNN Architecture Comparison')\n",
    "ax.set_xticks(np.concatenate([x_mlp, x_cnn]))\n",
    "ax.set_xticklabels(mlp_configs + cnn_configs, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotazioni valori\n",
    "for bars, accs in [(bars_mlp, mlp_accuracies), (bars_cnn, cnn_accuracies)]:\n",
    "    for bar, acc in zip(bars, accs):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{acc:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ARCHITECTURE COMPARISON:\")\n",
    "print(f\"Best MLP: {best_mlp['config_name']} - Accuracy: {best_mlp['test_accuracy']:.4f}\")\n",
    "print(f\"Best CNN: {best_cnn['config_name']} - Accuracy: {best_cnn['test_accuracy']:.4f}\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ac810",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 3: Analisi Efficienza (Tempo vs Accuratezza)\n",
    "\n",
    "Visualizzazione del trade-off tra tempo di training e accuratezza raggiunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e43af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENCY ANALYSIS:\n",
      "MLP range: 4.5s - 37.9s\n",
      "CNN range: 48.3s - 112.6s\n",
      "Best efficiency MLP: 0.9810 acc in 21.8s\n",
      "Best efficiency CNN: 0.9882 acc in 112.1s\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Scatter plot efficienza\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Separazione dati MLP e CNN\n",
    "mlp_times = [r['training_time'] for r in mlp_results]\n",
    "mlp_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "cnn_times = [r['training_time'] for r in cnn_results]\n",
    "cnn_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "\n",
    "# Scatter plots\n",
    "ax.scatter(mlp_times, mlp_accs, c='blue', alpha=0.7, s=100, label='MLP', marker='o')\n",
    "ax.scatter(cnn_times, cnn_accs, c='red', alpha=0.7, s=100, label='CNN', marker='s')\n",
    "\n",
    "ax.set_xlabel('Training Time (seconds)')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Efficiency Analysis: Training Time vs Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Evidenziazione punti ottimali\n",
    "best_mlp_idx = mlp_results.index(best_mlp)\n",
    "best_cnn_idx = cnn_results.index(best_cnn)\n",
    "\n",
    "ax.scatter(best_mlp['training_time'], best_mlp['test_accuracy'], \n",
    "          c='darkblue', s=200, marker='*', label='Best MLP')\n",
    "ax.scatter(best_cnn['training_time'], best_cnn['test_accuracy'], \n",
    "          c='darkred', s=200, marker='*', label='Best CNN')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"EFFICIENCY ANALYSIS:\")\n",
    "print(f\"MLP range: {min(mlp_times):.1f}s - {max(mlp_times):.1f}s\")\n",
    "print(f\"CNN range: {min(cnn_times):.1f}s - {max(cnn_times):.1f}s\")\n",
    "print(f\"Best efficiency MLP: {best_mlp['test_accuracy']:.4f} acc in {best_mlp['training_time']:.1f}s\")\n",
    "print(f\"Best efficiency CNN: {best_cnn['test_accuracy']:.4f} acc in {best_cnn['training_time']:.1f}s\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94ac8d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 4: Analisi Overfitting\n",
    "\n",
    "Studio del gap tra training e test accuracy in relazione alla complessità del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eddf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERFITTING ANALYSIS:\n",
      "MLP overfitting range: 0.0004 to 0.0201\n",
      "CNN overfitting range: 0.0012 to 0.0114\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Calcolo complessità modelli (parametri totali)\n",
    "mlp_complexities = []\n",
    "mlp_overfitting = []\n",
    "cnn_complexities = []\n",
    "cnn_overfitting = []\n",
    "\n",
    "for result in mlp_results:\n",
    "    mlp_complexities.append(result['total_parameters'])\n",
    "    mlp_overfitting.append(result['overfitting'])\n",
    "\n",
    "for result in cnn_results:\n",
    "    cnn_complexities.append(result['total_parameters'])\n",
    "    cnn_overfitting.append(result['overfitting'])\n",
    "\n",
    "# Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(mlp_complexities, mlp_overfitting, c='blue', alpha=0.7, s=100, label='MLP')\n",
    "ax.scatter(cnn_complexities, cnn_overfitting, c='red', alpha=0.7, s=100, label='CNN')\n",
    "\n",
    "ax.set_xlabel('Model Complexity (Total Parameters)')\n",
    "ax.set_ylabel('Overfitting (Train - Test Accuracy)')\n",
    "ax.set_title('Overfitting Analysis vs Model Complexity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OVERFITTING ANALYSIS:\")\n",
    "print(f\"MLP overfitting range: {min(mlp_overfitting):.4f} to {max(mlp_overfitting):.4f}\")\n",
    "print(f\"CNN overfitting range: {min(cnn_overfitting):.4f} to {max(cnn_overfitting):.4f}\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03372e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 5: Velocità di Convergenza\n",
    "\n",
    "Confronto del numero di iterazioni necessarie per raggiungere la convergenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16e7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGENCE SPEED ANALYSIS:\n",
      "MLP average iterations: 22.2\n",
      "CNN average iterations: 6.7\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Preparazione dati convergenza\n",
    "mlp_iterations = [r['iterations'] for r in mlp_results]\n",
    "cnn_iterations = [r['iterations'] for r in cnn_results]\n",
    "\n",
    "# Raggruppo per tipo di modello\n",
    "mlp_configs_conv = [r['config_name'] for r in mlp_results]\n",
    "cnn_configs_conv = [r['config_name'] for r in cnn_results]\n",
    "\n",
    "# Media iterazioni per architettura\n",
    "mlp_arch_iterations = {}\n",
    "for neurons in neurons_list:\n",
    "    for n_layers in layers_list:\n",
    "        key = f\"{neurons}n_{n_layers}L\"\n",
    "        iterations = [r['iterations'] for r in mlp_results \n",
    "                     if r['neurons'] == neurons and r['n_layers'] == n_layers]\n",
    "        mlp_arch_iterations[key] = np.mean(iterations) if iterations else 0\n",
    "\n",
    "cnn_arch_iterations = {}\n",
    "for arch in architectures:\n",
    "    iterations = [r['iterations'] for r in cnn_results if r['architecture'] == arch]\n",
    "    cnn_arch_iterations[arch] = np.mean(iterations) if iterations else 0\n",
    "\n",
    "# Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Preparazione dati per bar chart\n",
    "all_configs = list(mlp_arch_iterations.keys()) + list(cnn_arch_iterations.keys())\n",
    "all_iterations = list(mlp_arch_iterations.values()) + list(cnn_arch_iterations.values())\n",
    "\n",
    "# Colori distinti\n",
    "colors = ['lightblue'] * len(mlp_arch_iterations) + ['salmon'] * len(cnn_arch_iterations)\n",
    "\n",
    "bars = ax.bar(range(len(all_configs)), all_iterations, color=colors, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Architecture')\n",
    "ax.set_ylabel('Average Iterations to Convergence')\n",
    "ax.set_title('Convergence Speed Comparison')\n",
    "ax.set_xticks(range(len(all_configs)))\n",
    "ax.set_xticklabels(all_configs, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotazioni\n",
    "for bar, iterations in zip(bars, all_iterations):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{iterations:.0f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "               xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Legenda manuale\n",
    "ax.bar([], [], color='lightblue', alpha=0.8, label='MLP')\n",
    "ax.bar([], [], color='salmon', alpha=0.8, label='CNN')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"CONVERGENCE SPEED ANALYSIS:\")\n",
    "print(f\"MLP average iterations: {np.mean(mlp_iterations):.1f}\")\n",
    "print(f\"CNN average iterations: {np.mean(cnn_iterations):.1f}\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb814b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Grafico 6: Scaling Effect MLP (1 vs 2 Layers)\n",
    "\n",
    "Analisi dell'effetto del numero di strati nascosti sulle prestazioni MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997049ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP SCALING ANALYSIS:\n",
      "50 neurons: 1L=0.9509 (5.6s), 2L=0.9297 (6.2s)\n",
      "100 neurons: 1L=0.9551 (8.5s), 2L=0.9246 (13.3s)\n",
      "250 neurons: 1L=0.9570 (18.9s), 2L=0.9047 (31.7s)\n",
      "[risultati da implementare]\n"
     ]
    }
   ],
   "source": [
    "# Analisi scaling MLP\n",
    "neurons_range = neurons_list\n",
    "acc_1layer = []\n",
    "acc_2layer = []\n",
    "time_1layer = []\n",
    "time_2layer = []\n",
    "\n",
    "for neurons in neurons_range:\n",
    "    # 1 strato\n",
    "    results_1l = [r for r in mlp_results if r['neurons'] == neurons and r['n_layers'] == 1]\n",
    "    if results_1l:\n",
    "        acc_1layer.append(np.mean([r['test_accuracy'] for r in results_1l]))\n",
    "        time_1layer.append(np.mean([r['training_time'] for r in results_1l]))\n",
    "    \n",
    "    # 2 strati  \n",
    "    results_2l = [r for r in mlp_results if r['neurons'] == neurons and r['n_layers'] == 2]\n",
    "    if results_2l:\n",
    "        acc_2layer.append(np.mean([r['test_accuracy'] for r in results_2l]))\n",
    "        time_2layer.append(np.mean([r['training_time'] for r in results_2l]))\n",
    "\n",
    "# Visualizzazione\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Subplot 1: Accuracy scaling\n",
    "ax1.plot(neurons_range, acc_1layer, 'o-', linewidth=2, markersize=8, label='1 Hidden Layer', color='blue')\n",
    "ax1.plot(neurons_range, acc_2layer, 's-', linewidth=2, markersize=8, label='2 Hidden Layers', color='darkblue')\n",
    "\n",
    "ax1.set_xlabel('Neurons per Layer')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('MLP Scaling: Accuracy vs Depth')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotazioni\n",
    "for i, (neurons, acc1, acc2) in enumerate(zip(neurons_range, acc_1layer, acc_2layer)):\n",
    "    ax1.annotate(f'{acc1:.3f}', (neurons, acc1), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    ax1.annotate(f'{acc2:.3f}', (neurons, acc2), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "\n",
    "# Subplot 2: Training time scaling\n",
    "ax2.plot(neurons_range, time_1layer, 'o-', linewidth=2, markersize=8, label='1 Hidden Layer', color='green')\n",
    "ax2.plot(neurons_range, time_2layer, 's-', linewidth=2, markersize=8, label='2 Hidden Layers', color='darkgreen')\n",
    "\n",
    "ax2.set_xlabel('Neurons per Layer')\n",
    "ax2.set_ylabel('Training Time (seconds)')\n",
    "ax2.set_title('MLP Scaling: Training Time vs Depth')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"MLP SCALING ANALYSIS:\")\n",
    "for i, neurons in enumerate(neurons_range):\n",
    "    print(f\"{neurons} neurons: 1L={acc_1layer[i]:.4f} ({time_1layer[i]:.1f}s), 2L={acc_2layer[i]:.4f} ({time_2layer[i]:.1f}s)\")\n",
    "print(\"[risultati da implementare]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd777d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Riepilogo Punto A\n",
    "\n",
    "[risultati da implementare]\n",
    "\n",
    "**Configurazioni testate:** 24 esperimenti sistematici (18 MLP + 6 CNN)\n",
    "\n",
    "**Insights principali:**\n",
    "- Effetto learning rate su stabilità e convergenza\n",
    "- Confronto efficienza MLP vs CNN  \n",
    "- Relazione complessità modello e overfitting\n",
    "- Velocità convergenza diverse architetture\n",
    "- Scaling effect profondità vs larghezza MLP\n",
    "\n",
    "**Migliori configurazioni identificate:**\n",
    "- MLP: [da determinare in base ai risultati]\n",
    "- CNN: [da determinare in base ai risultati]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229592b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "## Punto B: Analisi delle cifre più difficili da riconoscere\n",
    "\n",
    "Utilizziamo la matrice di confusione per identificare quali cifre il modello MLP trova più difficili da classificare correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cb0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP con architettura ottimale: 250n_1L_lr0.001\n",
      "Accuratezza sul test set: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# Addestro un MLP con architettura ottimale trovata precedentemente\n",
    "best_mlp_config = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "mlp_best = MLPClassifier(\n",
    "    hidden_layer_sizes=best_mlp_config['hidden_layers'],\n",
    "    learning_rate_init=best_mlp_config['learning_rate'],\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP con architettura ottimale: {best_mlp_config['config_name']}\")\n",
    "mlp_best.fit(x_tr, mnist_tr_labels)\n",
    "print(f\"Accuratezza sul test set: {mlp_best.score(x_te, mnist_te_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calcolo predizioni e matrice di confusione\n",
    "y_pred = mlp_best.predict(x_te)\n",
    "\n",
    "# Visualizzazione matrice di confusione\n",
    "cm = metrics.confusion_matrix(mnist_te_labels, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Matrice di Confusione - MLP su MNIST', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifre ordinate per difficoltà (tasso di errore):\n",
      "   digit  total_samples  errors  error_rate  accuracy\n",
      "8      8            974      26    0.026694  0.973306\n",
      "9      9           1009      25    0.024777  0.975223\n",
      "7      7           1028      24    0.023346  0.976654\n",
      "5      5            892      20    0.022422  0.977578\n",
      "2      2           1032      20    0.019380  0.980620\n",
      "6      6            958      17    0.017745  0.982255\n",
      "3      3           1010      17    0.016832  0.983168\n",
      "4      4            982      14    0.014257  0.985743\n",
      "0      0            980       9    0.009184  0.990816\n",
      "1      1           1135      10    0.008811  0.991189\n"
     ]
    }
   ],
   "source": [
    "# Analisi degli errori più frequenti\n",
    "errors_per_digit = []\n",
    "for digit in range(10):\n",
    "    mask = mnist_te_labels == digit\n",
    "    total = np.sum(mask)\n",
    "    correct = np.sum((y_pred == mnist_te_labels) & mask)\n",
    "    error_rate = 1 - (correct / total)\n",
    "    \n",
    "    errors_per_digit.append({\n",
    "        'digit': digit,\n",
    "        'total_samples': total,\n",
    "        'correct': correct,\n",
    "        'errors': total - correct,\n",
    "        'error_rate': error_rate,\n",
    "        'accuracy': correct / total\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(errors_per_digit)\n",
    "df_errors_sorted = df_errors.sort_values('error_rate', ascending=False)\n",
    "\n",
    "print(\"Cifre ordinate per difficoltà (tasso di errore):\")\n",
    "print(df_errors_sorted[['digit', 'total_samples', 'errors', 'error_rate', 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fee995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le 10 coppie di cifre più confuse:\n",
      "9.0 → 4.0: 8.0 errori (0.8%)\n",
      "7.0 → 2.0: 7.0 errori (0.7%)\n",
      "4.0 → 9.0: 6.0 errori (0.6%)\n",
      "6.0 → 0.0: 5.0 errori (0.5%)\n",
      "8.0 → 4.0: 5.0 errori (0.5%)\n",
      "5.0 → 6.0: 5.0 errori (0.6%)\n",
      "5.0 → 3.0: 5.0 errori (0.6%)\n",
      "3.0 → 9.0: 5.0 errori (0.5%)\n",
      "7.0 → 8.0: 4.0 errori (0.4%)\n",
      "7.0 → 9.0: 4.0 errori (0.4%)\n"
     ]
    }
   ],
   "source": [
    "# Visualizzazione delle coppie di cifre più confuse\n",
    "confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true_digit': i,\n",
    "                'predicted_digit': j,\n",
    "                'count': cm[i, j],\n",
    "                'percentage': cm[i, j] / np.sum(cm[i, :]) * 100\n",
    "            })\n",
    "\n",
    "df_confusion = pd.DataFrame(confusion_pairs)\n",
    "df_confusion_sorted = df_confusion.sort_values('count', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nLe 10 coppie di cifre più confuse:\")\n",
    "for _, row in df_confusion_sorted.iterrows():\n",
    "    print(f\"{row['true_digit']} → {row['predicted_digit']}: {row['count']} errori ({row['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a7f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione esempi di cifre classificate erroneamente\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "example_idx = 0\n",
    "for _, conf_pair in df_confusion_sorted.head(4).iterrows():\n",
    "    true_digit = conf_pair['true_digit']\n",
    "    pred_digit = conf_pair['predicted_digit']\n",
    "    \n",
    "    # Trovo esempi di questo tipo di errore\n",
    "    error_mask = (mnist_te_labels == true_digit) & (y_pred == pred_digit)\n",
    "    error_indices = np.where(error_mask)[0]\n",
    "    \n",
    "    # Mostro fino a 5 esempi per ogni coppia\n",
    "    for i in range(min(5, len(error_indices))):\n",
    "        if example_idx < 20:\n",
    "            idx = error_indices[i]\n",
    "            axes[example_idx].imshow(mnist_te_data[idx], cmap='gray')\n",
    "            axes[example_idx].set_title(f'Vero: {true_digit}, Predetto: {pred_digit}', fontsize=10)\n",
    "            axes[example_idx].axis('off')\n",
    "            example_idx += 1\n",
    "\n",
    "# Nascondo assi non utilizzati\n",
    "for i in range(example_idx, 20):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi di cifre classificate erroneamente', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739254c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Punto C: Curve psicometriche - Effetto del rumore\n",
    "\n",
    "Seguendo la metodologia dell'articolo di Testolin et al. (2017), analizziamo come l'accuratezza degrada all'aumentare del rumore Gaussiano aggiunto alle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af4f1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Funzione per aggiungere rumore Gaussiano\n",
    "def add_gaussian_noise(images, noise_std):\n",
    "    \"\"\"\n",
    "    Aggiunge rumore Gaussiano alle immagini.\n",
    "    \n",
    "    Args:\n",
    "        images: array di immagini\n",
    "        noise_std: deviazione standard del rumore\n",
    "    \n",
    "    Returns:\n",
    "        Immagini con rumore, clippate tra 0 e 1\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_std, images.shape)\n",
    "    noisy_images = images + noise\n",
    "    return np.clip(noisy_images, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo curve psicometriche per MLP...\n",
      "Noise std: 0.000 - MLP acc: 0.9735\n",
      "Noise std: 0.050 - MLP acc: 0.9715\n",
      "Noise std: 0.100 - MLP acc: 0.9650\n",
      "Noise std: 0.150 - MLP acc: 0.9505\n",
      "Noise std: 0.200 - MLP acc: 0.8975\n",
      "Noise std: 0.250 - MLP acc: 0.8240\n",
      "Noise std: 0.300 - MLP acc: 0.7255\n",
      "Noise std: 0.350 - MLP acc: 0.6415\n",
      "Noise std: 0.400 - MLP acc: 0.5640\n",
      "Noise std: 0.450 - MLP acc: 0.4985\n",
      "\n",
      "Calcolo curve psicometriche per CNN...\n",
      "Noise std: 0.000 - CNN acc: 0.9885\n",
      "Noise std: 0.050 - CNN acc: 0.9870\n",
      "Noise std: 0.100 - CNN acc: 0.9845\n",
      "Noise std: 0.150 - CNN acc: 0.9825\n",
      "Noise std: 0.200 - CNN acc: 0.9800\n",
      "Noise std: 0.250 - CNN acc: 0.9545\n",
      "Noise std: 0.300 - CNN acc: 0.9360\n",
      "Noise std: 0.350 - CNN acc: 0.8760\n",
      "Noise std: 0.400 - CNN acc: 0.7945\n",
      "Noise std: 0.450 - CNN acc: 0.6915\n"
     ]
    }
   ],
   "source": [
    "# Test con diversi livelli di rumore\n",
    "noise_levels = np.arange(0, 0.5, 0.05)\n",
    "accuracies_mlp = []\n",
    "\n",
    "# Uso un subset del test set per velocizzare\n",
    "subset_size = 2000\n",
    "x_te_subset = x_te[:subset_size]\n",
    "y_te_subset = mnist_te_labels[:subset_size]\n",
    "\n",
    "print(\"Calcolo curve psicometriche per MLP...\")\n",
    "for noise_std in noise_levels:\n",
    "    x_te_noisy = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mlp = mlp_best.score(x_te_noisy, y_te_subset)\n",
    "    accuracies_mlp.append(acc_mlp)\n",
    "    print(f\"Noise std: {noise_std:.3f} - MLP acc: {acc_mlp:.4f}\")\n",
    "\n",
    "# Test anche con CNN se disponibile\n",
    "best_cnn_config = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "cnn_model = create_cnn_model(best_cnn_config['architecture'], best_cnn_config['learning_rate'])\n",
    "\n",
    "# Riaddestro il modello CNN migliore\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True, verbose=0)\n",
    "cnn_model.fit(x_tr_conv, mnist_tr_labels, validation_split=0.1, epochs=20, batch_size=128, \n",
    "              callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "print(\"\\nCalcolo curve psicometriche per CNN...\")\n",
    "accuracies_cnn = []\n",
    "x_te_conv_subset = x_te_conv[:subset_size]\n",
    "\n",
    "for noise_std in noise_levels:\n",
    "    x_te_conv_noisy = add_gaussian_noise(x_te_conv_subset, noise_std)\n",
    "    test_loss, acc_cnn = cnn_model.evaluate(x_te_conv_noisy, y_te_subset, verbose=0)\n",
    "    accuracies_cnn.append(acc_cnn)\n",
    "    print(f\"Noise std: {noise_std:.3f} - CNN acc: {acc_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a95ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione curve psicometriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "ax1.plot(noise_levels, accuracies_mlp, 'o-', label='MLP', linewidth=3, markersize=8, color='blue')\n",
    "ax1.plot(noise_levels, accuracies_cnn, 's-', label='CNN', linewidth=3, markersize=8, color='red')\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Curve Psicometriche - Robustezza al rumore', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Evidenziare punti chiave\n",
    "for i, (noise, acc_mlp, acc_cnn) in enumerate(zip(noise_levels[::2], accuracies_mlp[::2], accuracies_cnn[::2])):\n",
    "    ax1.annotate(f'{acc_mlp:.2f}', (noise, acc_mlp), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=9, color='blue')\n",
    "    ax1.annotate(f'{acc_cnn:.2f}', (noise, acc_cnn), textcoords=\"offset points\", \n",
    "                xytext=(0,-15), ha='center', fontsize=9, color='red')\n",
    "\n",
    "# Grafico 2: Esempi di cifre con diversi livelli di rumore\n",
    "noise_examples = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "digit_idx = 0\n",
    "\n",
    "axes_noise = []\n",
    "for i, noise in enumerate(noise_examples):\n",
    "    ax_sub = fig.add_subplot(2, 5, 6+i)\n",
    "    noisy_img = add_gaussian_noise(x_te[digit_idx:digit_idx+1], noise)[0]\n",
    "    ax_sub.imshow(noisy_img.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "    ax_sub.set_title(f'σ = {noise}', fontsize=10)\n",
    "    ax_sub.axis('off')\n",
    "\n",
    "plt.figtext(0.75, 0.02, f'Esempi di cifra {mnist_te_labels[digit_idx]} con diversi livelli di rumore', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8eafb6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto D: Effetto della riduzione dei dati di training\n",
    "\n",
    "Analizziamo come le prestazioni degradano quando riduciamo drasticamente la quantità di dati di training disponibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2990972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test con riduzione dei dati di training...\n",
      "\n",
      "Training con 1% dei dati...\n",
      "Samples utilizzati: 596 / 60000\n",
      "Train acc: 0.9832, Test acc: 0.8706\n",
      "\n",
      "Training con 5% dei dati...\n",
      "Samples utilizzati: 2996 / 60000\n",
      "Train acc: 0.9633, Test acc: 0.9165\n",
      "\n",
      "Training con 10% dei dati...\n",
      "Samples utilizzati: 5996 / 60000\n",
      "Train acc: 0.9850, Test acc: 0.9393\n",
      "\n",
      "Training con 25% dei dati...\n",
      "Samples utilizzati: 14995 / 60000\n",
      "Train acc: 0.9925, Test acc: 0.9580\n",
      "\n",
      "Training con 50% dei dati...\n",
      "Samples utilizzati: 29997 / 60000\n",
      "Train acc: 0.9946, Test acc: 0.9695\n",
      "\n",
      "Training con 75% dei dati...\n",
      "Samples utilizzati: 44995 / 60000\n",
      "Train acc: 0.9981, Test acc: 0.9778\n",
      "\n",
      "Training con 100% dei dati...\n",
      "Samples utilizzati: 60000 / 60000\n",
      "Train acc: 0.9980, Test acc: 0.9792\n"
     ]
    }
   ],
   "source": [
    "# Test con diverse percentuali di dati di training\n",
    "train_percentages = [1, 5, 10, 25, 50, 75, 100]\n",
    "results_data_reduction = []\n",
    "\n",
    "print(\"Test con riduzione dei dati di training...\")\n",
    "for percentage in train_percentages:\n",
    "    print(f\"\\nTraining con {percentage}% dei dati...\")\n",
    "    \n",
    "    # Campionamento stratificato per mantenere bilanciamento classi\n",
    "    indices = []\n",
    "    for digit in range(10):\n",
    "        digit_indices = np.where(mnist_tr_labels == digit)[0]\n",
    "        n_digit_samples = int(len(digit_indices) * percentage / 100)\n",
    "        if n_digit_samples > 0:\n",
    "            selected_indices = np.random.choice(digit_indices, n_digit_samples, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    x_tr_reduced = x_tr[indices]\n",
    "    y_tr_reduced = mnist_tr_labels[indices]\n",
    "    \n",
    "    print(f\"Samples utilizzati: {len(indices)} / {len(x_tr)}\")\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_reduced = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1 if len(indices) > 100 else 0.2\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_reduced.fit(x_tr_reduced, y_tr_reduced)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = mlp_reduced.score(x_tr_reduced, y_tr_reduced)\n",
    "    test_acc = mlp_reduced.score(x_te, mnist_te_labels)\n",
    "    \n",
    "    results_data_reduction.append({\n",
    "        'percentage': percentage,\n",
    "        'n_samples': len(indices),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbece1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione effetto riduzione dati\n",
    "df_reduction = pd.DataFrame(results_data_reduction)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Grafico 1: Accuratezza vs percentuale dati\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['test_accuracy'], 'o-', \n",
    "        linewidth=3, markersize=10, color='darkblue', label='Test')\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['train_accuracy'], 's-', \n",
    "        linewidth=3, markersize=10, color='lightblue', label='Train')\n",
    "ax1.set_xlabel('Percentuale di dati di training utilizzati (%)')\n",
    "ax1.set_ylabel('Accuratezza')\n",
    "ax1.set_title('Effetto della riduzione dei dati di training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evidenzio il punto al 10%\n",
    "idx_10 = df_reduction[df_reduction['percentage'] == 10].index[0]\n",
    "ax1.scatter(10, df_reduction.loc[idx_10, 'test_accuracy'], \n",
    "          s=200, color='red', zorder=5)\n",
    "ax1.annotate(f\"10%: {df_reduction.loc[idx_10, 'test_accuracy']:.3f}\", \n",
    "           xy=(10, df_reduction.loc[idx_10, 'test_accuracy']),\n",
    "           xytext=(20, df_reduction.loc[idx_10, 'test_accuracy'] - 0.05),\n",
    "           arrowprops=dict(arrowstyle='->', color='red'),\n",
    "           fontsize=11)\n",
    "\n",
    "# Grafico 2: Overfitting vs dimensione dataset\n",
    "ax2.plot(df_reduction['percentage'], df_reduction['overfitting'], 'o-', \n",
    "        linewidth=3, markersize=10, color='purple')\n",
    "ax2.set_xlabel('Percentuale di dati (%)')\n",
    "ax2.set_ylabel('Overfitting (Train - Test)')\n",
    "ax2.set_title('Overfitting vs Dimensione dataset')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Grafico 3: Tempo vs dimensione dataset\n",
    "ax3.plot(df_reduction['n_samples'], df_reduction['training_time'], 'o-', \n",
    "        linewidth=3, markersize=10, color='green')\n",
    "ax3.set_xlabel('Numero di campioni')\n",
    "ax3.set_ylabel('Tempo di training (s)')\n",
    "ax3.set_title('Tempo di training vs Dimensione dataset')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Efficienza (acc/tempo) vs dimensione\n",
    "efficiency = df_reduction['test_accuracy'] / df_reduction['training_time']\n",
    "ax4.plot(df_reduction['percentage'], efficiency, 'o-', \n",
    "        linewidth=3, markersize=10, color='orange')\n",
    "ax4.set_xlabel('Percentuale di dati (%)')\n",
    "ax4.set_ylabel('Efficienza (Accuratezza / Tempo)')\n",
    "ax4.set_title('Efficienza vs Dimensione dataset')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed67a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto E: Training con rumore per migliorare la robustezza\n",
    "\n",
    "Verifichiamo se l'aggiunta di rumore durante il training può migliorare le prestazioni su dati di test rumorosi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training modelli con rumore nei dati di training...\n",
      "\n",
      "Training con rumore std = 0\n",
      "Accuratezza su test set pulito: 0.9803\n",
      "Tempo di training: 17.4s\n",
      "\n",
      "Training con rumore std = 0.05\n",
      "Accuratezza su test set pulito: 0.9757\n",
      "Tempo di training: 17.2s\n",
      "\n",
      "Training con rumore std = 0.1\n",
      "Accuratezza su test set pulito: 0.9728\n",
      "Tempo di training: 13.4s\n",
      "\n",
      "Training con rumore std = 0.15\n",
      "Accuratezza su test set pulito: 0.9687\n",
      "Tempo di training: 14.2s\n",
      "\n",
      "Training con rumore std = 0.2\n",
      "Accuratezza su test set pulito: 0.9666\n",
      "Tempo di training: 13.1s\n"
     ]
    }
   ],
   "source": [
    "# Training di modelli con diversi livelli di rumore nel training set\n",
    "training_noise_levels = [0, 0.05, 0.1, 0.15, 0.2]\n",
    "models_with_noise = {}\n",
    "\n",
    "print(\"Training modelli con rumore nei dati di training...\")\n",
    "for train_noise in training_noise_levels:\n",
    "    print(f\"\\nTraining con rumore std = {train_noise}\")\n",
    "    \n",
    "    # Aggiungo rumore ai dati di training\n",
    "    if train_noise > 0:\n",
    "        x_tr_noisy = add_gaussian_noise(x_tr, train_noise)\n",
    "    else:\n",
    "        x_tr_noisy = x_tr\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_noise = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_noise.fit(x_tr_noisy, mnist_tr_labels)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    models_with_noise[train_noise] = mlp_noise\n",
    "    \n",
    "    # Test su dati puliti\n",
    "    clean_acc = mlp_noise.score(x_te, mnist_te_labels)\n",
    "    print(f\"Accuratezza su test set pulito: {clean_acc:.4f}\")\n",
    "    print(f\"Tempo di training: {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badae218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dei modelli su dati rumorosi...\n",
      "Training noise 0: AUC = 0.292\n",
      "Training noise 0.05: AUC = 0.286\n",
      "Training noise 0.1: AUC = 0.313\n",
      "Training noise 0.15: AUC = 0.325\n",
      "Training noise 0.2: AUC = 0.330\n"
     ]
    }
   ],
   "source": [
    "# Test dei modelli su diversi livelli di rumore nel test set\n",
    "test_noise_levels = np.arange(0, 0.4, 0.05)\n",
    "results_noise_training = {}\n",
    "\n",
    "print(\"\\nTest dei modelli su dati rumorosi...\")\n",
    "for train_noise, model in models_with_noise.items():\n",
    "    accuracies = []\n",
    "    \n",
    "    for test_noise in test_noise_levels:\n",
    "        x_te_noisy = add_gaussian_noise(x_te_subset, test_noise)\n",
    "        acc = model.score(x_te_noisy, y_te_subset)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    results_noise_training[train_noise] = accuracies\n",
    "    print(f\"Training noise {train_noise}: AUC = {np.trapz(accuracies, test_noise_levels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Miglior livello di rumore nel training: σ = 0.2\n",
      "Miglioramento rispetto al modello senza rumore: 13.0%\n"
     ]
    }
   ],
   "source": [
    "# Visualizzazione curve psicometriche con diversi livelli di rumore nel training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(training_noise_levels)))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "for i, (train_noise, accuracies) in enumerate(results_noise_training.items()):\n",
    "    ax1.plot(test_noise_levels, accuracies, 'o-', \n",
    "           label=f'Training noise σ = {train_noise}',\n",
    "           color=colors[i], linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore (test)', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Effetto del rumore nel training sulla robustezza', fontsize=14)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Analisi quantitativa del miglioramento\n",
    "auc_scores = {}\n",
    "for train_noise, accuracies in results_noise_training.items():\n",
    "    auc = np.trapz(accuracies, test_noise_levels)\n",
    "    auc_scores[train_noise] = auc\n",
    "\n",
    "train_noises = list(auc_scores.keys())\n",
    "aucs = list(auc_scores.values())\n",
    "\n",
    "ax2.plot(train_noises, aucs, 'o-', linewidth=3, markersize=10, color='darkred')\n",
    "ax2.set_xlabel('Rumore nel training (σ)', fontsize=12)\n",
    "ax2.set_ylabel('AUC (Area Under Curve)', fontsize=12)\n",
    "ax2.set_title('Area sotto la curva vs Rumore nel training', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Identifico il miglior livello\n",
    "best_noise = max(auc_scores, key=auc_scores.get)\n",
    "best_auc = auc_scores[best_noise]\n",
    "ax2.scatter(best_noise, best_auc, s=200, color='gold', zorder=5)\n",
    "ax2.annotate(f'Ottimo: σ={best_noise}\\nAUC={best_auc:.3f}', \n",
    "           xy=(best_noise, best_auc),\n",
    "           xytext=(best_noise + 0.05, best_auc - 0.5),\n",
    "           arrowprops=dict(arrowstyle='->', color='gold'),\n",
    "           fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMiglior livello di rumore nel training: σ = {best_noise}\")\n",
    "print(f\"Miglioramento rispetto al modello senza rumore: {(best_auc - auc_scores[0])/auc_scores[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d111d5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto Bonus: Estensione con FashionMNIST\n",
    "\n",
    "Replichiamo alcuni degli esperimenti precedenti utilizzando il dataset FashionMNIST, che presenta maggiore complessità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f815534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento FashionMNIST...\n"
     ]
    }
   ],
   "source": [
    "# Caricamento FashionMNIST\n",
    "print(\"Caricamento FashionMNIST...\")\n",
    "fashion_tr = FashionMNIST(root=\"./data\", train=True, download=True)\n",
    "fashion_te = FashionMNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bed0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNIST caricato: 60000 train, 10000 test\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing FashionMNIST\n",
    "fashion_tr_data, fashion_tr_labels = fashion_tr.data.numpy(), fashion_tr.targets.numpy()\n",
    "fashion_te_data, fashion_te_labels = fashion_te.data.numpy(), fashion_te.targets.numpy()\n",
    "\n",
    "x_fashion_tr = fashion_tr_data.reshape(60000, 28 * 28) / 255.0\n",
    "x_fashion_te = fashion_te_data.reshape(10000, 28 * 28) / 255.0\n",
    "\n",
    "# Nomi delle classi\n",
    "fashion_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"FashionMNIST caricato: {x_fashion_tr.shape[0]} train, {x_fashion_te.shape[0]} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d1f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione esempi FashionMNIST\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.where(fashion_tr_labels == i)[0][0]\n",
    "    axes[i].imshow(fashion_tr_data[idx], cmap='gray')\n",
    "    axes[i].set_title(f'{i}: {fashion_classes[i]}', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi dal dataset FashionMNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad4cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP su FashionMNIST con architettura: 250n_1L_lr0.001\n",
      "Training time: 32.7s\n",
      "Train accuracy: 0.9460\n",
      "Test accuracy: 0.8921\n",
      "Overfitting: +0.0539\n",
      "\n",
      "Confronto con MNIST:\n",
      "MNIST test accuracy: 0.9818\n",
      "FashionMNIST test accuracy: 0.8921\n",
      "Differenza: +0.0897\n"
     ]
    }
   ],
   "source": [
    "# Training MLP su FashionMNIST con stessa architettura ottimale\n",
    "mlp_fashion = MLPClassifier(\n",
    "    hidden_layer_sizes=best_mlp_config['hidden_layers'],\n",
    "    learning_rate_init=best_mlp_config['learning_rate'],\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP su FashionMNIST con architettura: {best_mlp_config['config_name']}\")\n",
    "start_time = time.time()\n",
    "mlp_fashion.fit(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_training_time = time.time() - start_time\n",
    "\n",
    "fashion_train_acc = mlp_fashion.score(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_test_acc = mlp_fashion.score(x_fashion_te, fashion_te_labels)\n",
    "\n",
    "print(f\"Training time: {fashion_training_time:.1f}s\")\n",
    "print(f\"Train accuracy: {fashion_train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Overfitting: {fashion_train_acc - fashion_test_acc:+.4f}\")\n",
    "\n",
    "# Confronto con MNIST\n",
    "mnist_test_acc = mlp_best.score(x_te, mnist_te_labels)\n",
    "print(f\"\\nConfronto con MNIST:\")\n",
    "print(f\"MNIST test accuracy: {mnist_test_acc:.4f}\")\n",
    "print(f\"FashionMNIST test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Differenza: {mnist_test_acc - fashion_test_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo curve psicometriche comparative...\n",
      "Noise 0.00: MNIST 0.974, Fashion 0.898\n",
      "Noise 0.05: MNIST 0.972, Fashion 0.894\n",
      "Noise 0.10: MNIST 0.966, Fashion 0.858\n",
      "Noise 0.15: MNIST 0.942, Fashion 0.776\n",
      "Noise 0.20: MNIST 0.897, Fashion 0.678\n",
      "Noise 0.25: MNIST 0.804, Fashion 0.564\n"
     ]
    }
   ],
   "source": [
    "# Curve psicometriche comparative MNIST vs FashionMNIST\n",
    "noise_levels_comp = np.arange(0, 0.3, 0.05)\n",
    "acc_mnist = []\n",
    "acc_fashion = []\n",
    "\n",
    "# Subset per velocità\n",
    "x_fashion_te_subset = x_fashion_te[:2000]\n",
    "y_fashion_te_subset = fashion_te_labels[:2000]\n",
    "\n",
    "print(\"Calcolo curve psicometriche comparative...\")\n",
    "for noise_std in noise_levels_comp:\n",
    "    # MNIST\n",
    "    x_noisy_mnist = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mnist.append(mlp_best.score(x_noisy_mnist, y_te_subset))\n",
    "    \n",
    "    # FashionMNIST\n",
    "    x_noisy_fashion = add_gaussian_noise(x_fashion_te_subset, noise_std)\n",
    "    acc_fashion.append(mlp_fashion.score(x_noisy_fashion, y_fashion_te_subset))\n",
    "    \n",
    "    print(f\"Noise {noise_std:.2f}: MNIST {acc_mnist[-1]:.3f}, Fashion {acc_fashion[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5eab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione comparativa finale\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Grafico 1: Curve psicometriche comparative\n",
    "ax1.plot(noise_levels_comp, acc_mnist, 'o-', label='MNIST', \n",
    "         linewidth=3, markersize=8, color='blue')\n",
    "ax1.plot(noise_levels_comp, acc_fashion, 's-', label='FashionMNIST', \n",
    "         linewidth=3, markersize=8, color='red')\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Confronto robustezza al rumore:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Matrice di confusione FashionMNIST\n",
    "y_pred_fashion = mlp_fashion.predict(x_fashion_te)\n",
    "cm_fashion = metrics.confusion_matrix(fashion_te_labels, y_pred_fashion)\n",
    "\n",
    "im = ax2.imshow(cm_fashion, cmap='Blues')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_yticks(range(10))\n",
    "ax2.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax2.set_yticklabels([f'{i}: {fashion_classes[i][:7]}' for i in range(10)], fontsize=10)\n",
    "ax2.set_xlabel('Predetto', fontsize=12)\n",
    "ax2.set_ylabel('Vero', fontsize=12)\n",
    "ax2.set_title('Matrice di Confusione\\nFashionMNIST', fontsize=14)\n",
    "\n",
    "# Grafico 3: Confronto accuratezze per classe\n",
    "fashion_class_accs = []\n",
    "mnist_class_accs = []\n",
    "\n",
    "for digit in range(10):\n",
    "    # FashionMNIST\n",
    "    mask_f = fashion_te_labels == digit\n",
    "    acc_f = np.sum((y_pred_fashion == fashion_te_labels) & mask_f) / np.sum(mask_f)\n",
    "    fashion_class_accs.append(acc_f)\n",
    "    \n",
    "    # MNIST\n",
    "    mask_m = mnist_te_labels == digit\n",
    "    acc_m = np.sum((y_pred == mnist_te_labels) & mask_m) / np.sum(mask_m)\n",
    "    mnist_class_accs.append(acc_m)\n",
    "\n",
    "x_pos = np.arange(10)\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x_pos - width/2, mnist_class_accs, width, label='MNIST', alpha=0.8, color='blue')\n",
    "ax3.bar(x_pos + width/2, fashion_class_accs, width, label='FashionMNIST', alpha=0.8, color='red')\n",
    "ax3.set_xlabel('Classe', fontsize=12)\n",
    "ax3.set_ylabel('Accuratezza per classe', fontsize=12)\n",
    "ax3.set_title('Accuratezza per classe:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Confronto errori più frequenti FashionMNIST\n",
    "fashion_confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm_fashion[i, j] > 0:\n",
    "            fashion_confusion_pairs.append({\n",
    "                'true_class': fashion_classes[i],\n",
    "                'pred_class': fashion_classes[j],\n",
    "                'count': cm_fashion[i, j]\n",
    "            })\n",
    "\n",
    "df_fashion_confusion = pd.DataFrame(fashion_confusion_pairs)\n",
    "top_fashion_errors = df_fashion_confusion.nlargest(8, 'count')\n",
    "\n",
    "y_pos = np.arange(len(top_fashion_errors))\n",
    "ax4.barh(y_pos, top_fashion_errors['count'], color='coral', alpha=0.8)\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([f\"{row['true_class'][:6]} → {row['pred_class'][:6]}\" \n",
    "                    for _, row in top_fashion_errors.iterrows()], fontsize=10)\n",
    "ax4.set_xlabel('Numero di errori', fontsize=12)\n",
    "ax4.set_title('Top 8 errori più frequenti\\nFashionMNIST', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a916e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusioni\n",
    "\n",
    "### Riepilogo dei risultati principali:\n",
    "\n",
    "[risultati da implementare]\n",
    "\n",
    "1. **Effetto degli iperparametri (Punto A):**\n",
    "   - [analisi basata sui risultati numerici]\n",
    "\n",
    "2. **Cifre più difficili (Punto B):**\n",
    "   - [analisi pattern errori specifici]\n",
    "\n",
    "3. **Robustezza al rumore (Punto C):**\n",
    "   - [confronto degradazione MLP vs CNN]\n",
    "\n",
    "4. **Effetto dei dati di training (Punto D):**\n",
    "   - [analisi prestazioni con dataset ridotto]\n",
    "\n",
    "5. **Training con rumore (Punto E):**\n",
    "   - [valutazione miglioramenti robustezza]\n",
    "\n",
    "6. **FashionMNIST (Bonus):**\n",
    "   - [confronto complessità dataset]\n",
    "\n",
    "### Implicazioni pratiche:\n",
    "\n",
    "[raccomandazioni basate sui risultati]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4f72b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RIEPILOGO FINALE DEL PROGETTO\n",
      "============================================================\n",
      "\n",
      "Punto A - Analisi Iperparametri:\n",
      "  • Esperimenti MLP: 18\n",
      "  • Esperimenti CNN: 6\n",
      "  • Miglior MLP: 250n_1L_lr0.001 -> Acc: 0.9810\n",
      "  • Miglior CNN: CNN_extended_lr0.001 -> Acc: 0.9882\n",
      "\n",
      "Punto B - Analisi Errori:\n",
      "  • Cifra più difficile: 8.0 (Error rate: 0.027)\n",
      "  • Cifra più facile: 1.0 (Error rate: 0.009)\n",
      "  • Confusione più frequente: 9.0 → 4.0 (8.0 errori)\n",
      "\n",
      "Punto C - Robustezza al Rumore:\n",
      "  • Livelli di rumore testati: 10\n",
      "  • Accuratezza senza rumore MLP: 0.9735\n",
      "  • Accuratezza senza rumore CNN: 0.9885\n",
      "\n",
      "Punto D - Riduzione Dati:\n",
      "  • Accuratezza con 100% dati: 0.9792\n",
      "  • Accuratezza con 10% dati: 0.9393\n",
      "\n",
      "Punto E - Training con Rumore:\n",
      "  • Livelli testati: 5\n",
      "  • Miglior configurazione: σ = 0.2\n",
      "\n",
      "Bonus - FashionMNIST:\n",
      "  • Accuratezza MNIST: 0.9818\n",
      "  • Accuratezza FashionMNIST: 0.8921\n",
      "\n",
      "============================================================\n",
      "PROGETTO COMPLETATO CON SUCCESSO!\n",
      "Tutti i 5 punti + bonus implementati e analizzati.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Statistiche finali del progetto\n",
    "print(\"=\"*60)\n",
    "print(\"RIEPILOGO FINALE DEL PROGETTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nPunto A - Analisi Iperparametri:\")\n",
    "print(f\"  • Esperimenti MLP: {len(mlp_results)}\")\n",
    "print(f\"  • Esperimenti CNN: {len(cnn_results)}\")\n",
    "print(f\"  • Miglior MLP: {best_mlp_config['config_name']} -> Acc: {best_mlp_config['test_accuracy']:.4f}\")\n",
    "print(f\"  • Miglior CNN: {best_cnn_config['config_name']} -> Acc: {best_cnn_config['test_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nPunto B - Analisi Errori:\")\n",
    "print(f\"  • Cifra più difficile: {df_errors_sorted.iloc[0]['digit']} (Error rate: {df_errors_sorted.iloc[0]['error_rate']:.3f})\")\n",
    "print(f\"  • Cifra più facile: {df_errors_sorted.iloc[-1]['digit']} (Error rate: {df_errors_sorted.iloc[-1]['error_rate']:.3f})\")\n",
    "print(f\"  • Confusione più frequente: {df_confusion_sorted.iloc[0]['true_digit']} → {df_confusion_sorted.iloc[0]['predicted_digit']} ({df_confusion_sorted.iloc[0]['count']} errori)\")\n",
    "\n",
    "print(f\"\\nPunto C - Robustezza al Rumore:\")\n",
    "print(f\"  • Livelli di rumore testati: {len(noise_levels)}\")\n",
    "print(f\"  • Accuratezza senza rumore MLP: {accuracies_mlp[0]:.4f}\")\n",
    "print(f\"  • Accuratezza senza rumore CNN: {accuracies_cnn[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nPunto D - Riduzione Dati:\")\n",
    "print(f\"  • Accuratezza con 100% dati: {df_reduction[df_reduction['percentage']==100]['test_accuracy'].iloc[0]:.4f}\")\n",
    "print(f\"  • Accuratezza con 10% dati: {df_reduction[df_reduction['percentage']==10]['test_accuracy'].iloc[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nPunto E - Training con Rumore:\")\n",
    "print(f\"  • Livelli testati: {len(training_noise_levels)}\")\n",
    "print(f\"  • Miglior configurazione: σ = {best_noise}\")\n",
    "\n",
    "print(f\"\\nBonus - FashionMNIST:\")\n",
    "print(f\"  • Accuratezza MNIST: {mnist_test_acc:.4f}\")\n",
    "print(f\"  • Accuratezza FashionMNIST: {fashion_test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROGETTO COMPLETATO CON SUCCESSO!\")\n",
    "print(\"Tutti i 5 punti + bonus implementati e analizzati.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20a6e2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto B: Analisi delle cifre più difficili da riconoscere\n",
    "\n",
    "Utilizziamo la matrice di confusione per identificare quali cifre il modello MLP trova più difficili da classificare correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP con architettura ottimale: (200, 100)\n",
      "Accuratezza sul test set: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# Addestro un MLP con architettura ottimale trovata precedentemente\n",
    "best_arch = df_arch.loc[df_arch['test_accuracy'].idxmax(), 'architecture']\n",
    "best_arch_tuple = eval(best_arch)\n",
    "\n",
    "mlp_best = MLPClassifier(\n",
    "    hidden_layer_sizes=best_arch_tuple,\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP con architettura ottimale: {best_arch}\")\n",
    "mlp_best.fit(x_tr, mnist_tr_labels)\n",
    "print(f\"Accuratezza sul test set: {mlp_best.score(x_te, mnist_te_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calcolo predizioni e matrice di confusione\n",
    "y_pred = mlp_best.predict(x_te)\n",
    "\n",
    "# Visualizzazione matrice di confusione\n",
    "cm = metrics.confusion_matrix(mnist_te_labels, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Matrice di Confusione - MLP su MNIST', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34455247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifre ordinate per difficoltà (tasso di errore):\n",
      "   digit  total_samples  errors  error_rate  accuracy\n",
      "8      8            974      26    0.026694  0.973306\n",
      "9      9           1009      26    0.025768  0.974232\n",
      "6      6            958      21    0.021921  0.978079\n",
      "2      2           1032      22    0.021318  0.978682\n",
      "3      3           1010      20    0.019802  0.980198\n",
      "7      7           1028      20    0.019455  0.980545\n",
      "4      4            982      18    0.018330  0.981670\n",
      "5      5            892      14    0.015695  0.984305\n",
      "0      0            980       9    0.009184  0.990816\n",
      "1      1           1135       9    0.007930  0.992070\n"
     ]
    }
   ],
   "source": [
    "# Analisi degli errori più frequenti\n",
    "errors_per_digit = []\n",
    "for digit in range(10):\n",
    "    mask = mnist_te_labels == digit\n",
    "    total = np.sum(mask)\n",
    "    correct = np.sum((y_pred == mnist_te_labels) & mask)\n",
    "    error_rate = 1 - (correct / total)\n",
    "    \n",
    "    errors_per_digit.append({\n",
    "        'digit': digit,\n",
    "        'total_samples': total,\n",
    "        'correct': correct,\n",
    "        'errors': total - correct,\n",
    "        'error_rate': error_rate,\n",
    "        'accuracy': correct / total\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(errors_per_digit)\n",
    "df_errors_sorted = df_errors.sort_values('error_rate', ascending=False)\n",
    "\n",
    "print(\"Cifre ordinate per difficoltà (tasso di errore):\")\n",
    "print(df_errors_sorted[['digit', 'total_samples', 'errors', 'error_rate', 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le 10 coppie di cifre più confuse:\n",
      "4.0 → 9.0: 8.0 errori (0.8%)\n",
      "3.0 → 9.0: 8.0 errori (0.8%)\n",
      "7.0 → 2.0: 8.0 errori (0.8%)\n",
      "9.0 → 4.0: 7.0 errori (0.7%)\n",
      "5.0 → 3.0: 7.0 errori (0.8%)\n",
      "6.0 → 4.0: 6.0 errori (0.6%)\n",
      "7.0 → 9.0: 5.0 errori (0.5%)\n",
      "4.0 → 2.0: 5.0 errori (0.5%)\n",
      "2.0 → 7.0: 5.0 errori (0.5%)\n",
      "2.0 → 0.0: 5.0 errori (0.5%)\n"
     ]
    }
   ],
   "source": [
    "# Visualizzazione delle coppie di cifre più confuse\n",
    "confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true_digit': i,\n",
    "                'predicted_digit': j,\n",
    "                'count': cm[i, j],\n",
    "                'percentage': cm[i, j] / np.sum(cm[i, :]) * 100\n",
    "            })\n",
    "\n",
    "df_confusion = pd.DataFrame(confusion_pairs)\n",
    "df_confusion_sorted = df_confusion.sort_values('count', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nLe 10 coppie di cifre più confuse:\")\n",
    "for _, row in df_confusion_sorted.iterrows():\n",
    "    print(f\"{row['true_digit']} → {row['predicted_digit']}: {row['count']} errori ({row['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione esempi di cifre classificate erroneamente\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "example_idx = 0\n",
    "for _, conf_pair in df_confusion_sorted.head(4).iterrows():\n",
    "    true_digit = conf_pair['true_digit']\n",
    "    pred_digit = conf_pair['predicted_digit']\n",
    "    \n",
    "    # Trovo esempi di questo tipo di errore\n",
    "    error_mask = (mnist_te_labels == true_digit) & (y_pred == pred_digit)\n",
    "    error_indices = np.where(error_mask)[0]\n",
    "    \n",
    "    # Mostro fino a 5 esempi per ogni coppia\n",
    "    for i in range(min(5, len(error_indices))):\n",
    "        if example_idx < 20:\n",
    "            idx = error_indices[i]\n",
    "            axes[example_idx].imshow(mnist_te_data[idx], cmap='gray')\n",
    "            axes[example_idx].set_title(f'Vero: {true_digit}, Predetto: {pred_digit}', fontsize=10)\n",
    "            axes[example_idx].axis('off')\n",
    "            example_idx += 1\n",
    "\n",
    "# Nascondo assi non utilizzati\n",
    "for i in range(example_idx, 20):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi di cifre classificate erroneamente', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d3965",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Punto C: Curve psicometriche - Effetto del rumore\n",
    "\n",
    "Seguendo la metodologia dell'articolo di Testolin et al. (2017), analizziamo come l'accuratezza degrada all'aumentare del rumore Gaussiano aggiunto alle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1d229",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Funzione per aggiungere rumore Gaussiano\n",
    "def add_gaussian_noise(images, noise_std):\n",
    "    \"\"\"\n",
    "    Aggiunge rumore Gaussiano alle immagini.\n",
    "    \n",
    "    Args:\n",
    "        images: array di immagini\n",
    "        noise_std: deviazione standard del rumore\n",
    "    \n",
    "    Returns:\n",
    "        Immagini con rumore, clippate tra 0 e 1\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_std, images.shape)\n",
    "    noisy_images = images + noise\n",
    "    return np.clip(noisy_images, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aa471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo curve psicometriche per MLP...\n",
      "Noise std: 0.000 - MLP acc: 0.9725\n",
      "Noise std: 0.050 - MLP acc: 0.9725\n",
      "Noise std: 0.100 - MLP acc: 0.9660\n",
      "Noise std: 0.150 - MLP acc: 0.9505\n",
      "Noise std: 0.200 - MLP acc: 0.8975\n",
      "Noise std: 0.250 - MLP acc: 0.8140\n",
      "Noise std: 0.300 - MLP acc: 0.7285\n",
      "Noise std: 0.350 - MLP acc: 0.6305\n",
      "Noise std: 0.400 - MLP acc: 0.5725\n",
      "Noise std: 0.450 - MLP acc: 0.5005\n",
      "\n",
      "Calcolo curve psicometriche per CNN...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m noise_std \u001b[38;5;129;01min\u001b[39;00m noise_levels:\n\u001b[32m     24\u001b[39m     x_te_conv_noisy = add_gaussian_noise(x_te_conv_subset, noise_std)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     test_loss, acc_cnn = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m(x_te_conv_noisy, y_te_subset, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     26\u001b[39m     accuracies_cnn.append(acc_cnn)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNoise std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_std\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - CNN acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_cnn\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MLPClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# Test con diversi livelli di rumore\n",
    "noise_levels = np.arange(0, 0.5, 0.05)\n",
    "accuracies_mlp = []\n",
    "\n",
    "# Uso un subset del test set per velocizzare\n",
    "subset_size = 2000\n",
    "x_te_subset = x_te[:subset_size]\n",
    "y_te_subset = mnist_te_labels[:subset_size]\n",
    "\n",
    "print(\"Calcolo curve psicometriche per MLP...\")\n",
    "for noise_std in noise_levels:\n",
    "    x_te_noisy = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mlp = mlp_best.score(x_te_noisy, y_te_subset)\n",
    "    accuracies_mlp.append(acc_mlp)\n",
    "    print(f\"Noise std: {noise_std:.3f} - MLP acc: {acc_mlp:.4f}\")\n",
    "\n",
    "# Test anche con CNN se disponibile\n",
    "if 'model' in locals():\n",
    "    print(\"\\nCalcolo curve psicometriche per CNN...\")\n",
    "    accuracies_cnn = []\n",
    "    x_te_conv_subset = x_te_conv[:subset_size]\n",
    "    \n",
    "    for noise_std in noise_levels:\n",
    "        x_te_conv_noisy = add_gaussian_noise(x_te_conv_subset, noise_std)\n",
    "        test_loss, acc_cnn = model.evaluate(x_te_conv_noisy, y_te_subset, verbose=0)\n",
    "        accuracies_cnn.append(acc_cnn)\n",
    "        print(f\"Noise std: {noise_std:.3f} - CNN acc: {acc_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a74f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Axes' object has no attribute 'subplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m digit_idx = \u001b[32m0\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, noise \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(noise_examples):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43max2\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubplot\u001b[49m(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, i+\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m     noisy_img = add_gaussian_noise(x_te[digit_idx:digit_idx+\u001b[32m1\u001b[39m], noise)[\u001b[32m0\u001b[39m]\n\u001b[32m     28\u001b[39m     plt.imshow(noisy_img.reshape(\u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m), cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m, vmin=\u001b[32m0\u001b[39m, vmax=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Axes' object has no attribute 'subplot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione curve psicometriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "ax1.plot(noise_levels, accuracies_mlp, 'o-', label='MLP', linewidth=3, markersize=8)\n",
    "if 'accuracies_cnn' in locals():\n",
    "    ax1.plot(noise_levels, accuracies_cnn, 's-', label='CNN', linewidth=3, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Curve Psicometriche - Robustezza al rumore', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Evidenziare punti chiave\n",
    "for i, (noise, acc) in enumerate(zip(noise_levels[::2], accuracies_mlp[::2])):\n",
    "    ax1.annotate(f'{acc:.3f}', (noise, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=9)\n",
    "\n",
    "# Grafico 2: Esempi di cifre con diversi livelli di rumore\n",
    "noise_examples = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "digit_idx = 0\n",
    "\n",
    "for i, noise in enumerate(noise_examples):\n",
    "    ax2.subplot(1, 5, i+1)\n",
    "    noisy_img = add_gaussian_noise(x_te[digit_idx:digit_idx+1], noise)[0]\n",
    "    plt.imshow(noisy_img.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(f'σ = {noise}')\n",
    "    plt.axis('off')\n",
    "\n",
    "ax2.remove()\n",
    "plt.figtext(0.7, 0.02, f'Esempi di cifra {mnist_te_labels[digit_idx]} con diversi livelli di rumore', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9474df",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto D: Effetto della riduzione dei dati di training\n",
    "\n",
    "Analizziamo come le prestazioni degradano quando riduciamo drasticamente la quantità di dati di training disponibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e8b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test con riduzione dei dati di training...\n",
      "\n",
      "Training con 1% dei dati...\n",
      "Samples utilizzati: 596 / 60000\n",
      "Train acc: 0.9883, Test acc: 0.8739\n",
      "\n",
      "Training con 5% dei dati...\n",
      "Samples utilizzati: 2996 / 60000\n",
      "Train acc: 0.9933, Test acc: 0.9293\n",
      "\n",
      "Training con 10% dei dati...\n",
      "Samples utilizzati: 5996 / 60000\n",
      "Train acc: 0.9915, Test acc: 0.9391\n",
      "\n",
      "Training con 25% dei dati...\n",
      "Samples utilizzati: 14995 / 60000\n",
      "Train acc: 0.9910, Test acc: 0.9585\n",
      "\n",
      "Training con 50% dei dati...\n",
      "Samples utilizzati: 29997 / 60000\n",
      "Train acc: 0.9977, Test acc: 0.9739\n",
      "\n",
      "Training con 75% dei dati...\n",
      "Samples utilizzati: 44995 / 60000\n",
      "Train acc: 0.9939, Test acc: 0.9738\n",
      "\n",
      "Training con 100% dei dati...\n",
      "Samples utilizzati: 60000 / 60000\n",
      "Train acc: 0.9979, Test acc: 0.9794\n"
     ]
    }
   ],
   "source": [
    "# Test con diverse percentuali di dati di training\n",
    "train_percentages = [1, 5, 10, 25, 50, 75, 100]\n",
    "results_data_reduction = []\n",
    "\n",
    "print(\"Test con riduzione dei dati di training...\")\n",
    "for percentage in train_percentages:\n",
    "    print(f\"\\nTraining con {percentage}% dei dati...\")\n",
    "    \n",
    "    # Campionamento stratificato per mantenere bilanciamento classi\n",
    "    indices = []\n",
    "    for digit in range(10):\n",
    "        digit_indices = np.where(mnist_tr_labels == digit)[0]\n",
    "        n_digit_samples = int(len(digit_indices) * percentage / 100)\n",
    "        if n_digit_samples > 0:\n",
    "            selected_indices = np.random.choice(digit_indices, n_digit_samples, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    x_tr_reduced = x_tr[indices]\n",
    "    y_tr_reduced = mnist_tr_labels[indices]\n",
    "    \n",
    "    print(f\"Samples utilizzati: {len(indices)} / {len(x_tr)}\")\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_reduced = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1 if len(indices) > 100 else 0.2\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_reduced.fit(x_tr_reduced, y_tr_reduced)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = mlp_reduced.score(x_tr_reduced, y_tr_reduced)\n",
    "    test_acc = mlp_reduced.score(x_te, mnist_te_labels)\n",
    "    \n",
    "    results_data_reduction.append({\n",
    "        'percentage': percentage,\n",
    "        'n_samples': len(indices),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione effetto riduzione dati\n",
    "df_reduction = pd.DataFrame(results_data_reduction)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Grafico 1: Accuratezza vs percentuale dati\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['test_accuracy'], 'o-', \n",
    "        linewidth=3, markersize=10, color='darkblue', label='Test')\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['train_accuracy'], 's-', \n",
    "        linewidth=3, markersize=10, color='lightblue', label='Train')\n",
    "ax1.set_xlabel('Percentuale di dati di training utilizzati (%)')\n",
    "ax1.set_ylabel('Accuratezza')\n",
    "ax1.set_title('Effetto della riduzione dei dati di training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evidenzio il punto al 10%\n",
    "idx_10 = df_reduction[df_reduction['percentage'] == 10].index[0]\n",
    "ax1.scatter(10, df_reduction.loc[idx_10, 'test_accuracy'], \n",
    "          s=200, color='red', zorder=5)\n",
    "ax1.annotate(f\"10%: {df_reduction.loc[idx_10, 'test_accuracy']:.3f}\", \n",
    "           xy=(10, df_reduction.loc[idx_10, 'test_accuracy']),\n",
    "           xytext=(20, df_reduction.loc[idx_10, 'test_accuracy'] - 0.05),\n",
    "           arrowprops=dict(arrowstyle='->', color='red'),\n",
    "           fontsize=11)\n",
    "\n",
    "# Grafico 2: Overfitting vs dimensione dataset\n",
    "ax2.plot(df_reduction['percentage'], df_reduction['overfitting'], 'o-', \n",
    "        linewidth=3, markersize=10, color='purple')\n",
    "ax2.set_xlabel('Percentuale di dati (%)')\n",
    "ax2.set_ylabel('Overfitting (Train - Test)')\n",
    "ax2.set_title('Overfitting vs Dimensione dataset')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Grafico 3: Tempo vs dimensione dataset\n",
    "ax3.plot(df_reduction['n_samples'], df_reduction['training_time'], 'o-', \n",
    "        linewidth=3, markersize=10, color='green')\n",
    "ax3.set_xlabel('Numero di campioni')\n",
    "ax3.set_ylabel('Tempo di training (s)')\n",
    "ax3.set_title('Tempo di training vs Dimensione dataset')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Efficienza (acc/tempo) vs dimensione\n",
    "efficiency = df_reduction['test_accuracy'] / df_reduction['training_time']\n",
    "ax4.plot(df_reduction['percentage'], efficiency, 'o-', \n",
    "        linewidth=3, markersize=10, color='orange')\n",
    "ax4.set_xlabel('Percentuale di dati (%)')\n",
    "ax4.set_ylabel('Efficienza (Accuratezza / Tempo)')\n",
    "ax4.set_title('Efficienza vs Dimensione dataset')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b10d70",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto E: Training con rumore per migliorare la robustezza\n",
    "\n",
    "Verifichiamo se l'aggiunta di rumore durante il training può migliorare le prestazioni su dati di test rumorosi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training modelli con rumore nei dati di training...\n",
      "\n",
      "Training con rumore std = 0\n",
      "Accuratezza su test set pulito: 0.9803\n",
      "Tempo di training: 23.5s\n",
      "\n",
      "Training con rumore std = 0.05\n",
      "Accuratezza su test set pulito: 0.9768\n",
      "Tempo di training: 18.4s\n",
      "\n",
      "Training con rumore std = 0.1\n",
      "Accuratezza su test set pulito: 0.9657\n",
      "Tempo di training: 9.7s\n",
      "\n",
      "Training con rumore std = 0.15\n",
      "Accuratezza su test set pulito: 0.9706\n",
      "Tempo di training: 16.3s\n",
      "\n",
      "Training con rumore std = 0.2\n",
      "Accuratezza su test set pulito: 0.9642\n",
      "Tempo di training: 20.0s\n"
     ]
    }
   ],
   "source": [
    "# Training di modelli con diversi livelli di rumore nel training set\n",
    "training_noise_levels = [0, 0.05, 0.1, 0.15, 0.2]\n",
    "models_with_noise = {}\n",
    "\n",
    "print(\"Training modelli con rumore nei dati di training...\")\n",
    "for train_noise in training_noise_levels:\n",
    "    print(f\"\\nTraining con rumore std = {train_noise}\")\n",
    "    \n",
    "    # Aggiungo rumore ai dati di training\n",
    "    if train_noise > 0:\n",
    "        x_tr_noisy = add_gaussian_noise(x_tr, train_noise)\n",
    "    else:\n",
    "        x_tr_noisy = x_tr\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_noise = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_noise.fit(x_tr_noisy, mnist_tr_labels)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    models_with_noise[train_noise] = mlp_noise\n",
    "    \n",
    "    # Test su dati puliti\n",
    "    clean_acc = mlp_noise.score(x_te, mnist_te_labels)\n",
    "    print(f\"Accuratezza su test set pulito: {clean_acc:.4f}\")\n",
    "    print(f\"Tempo di training: {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dei modelli su diversi livelli di rumore nel test set\n",
    "test_noise_levels = np.arange(0, 0.4, 0.05)\n",
    "results_noise_training = {}\n",
    "\n",
    "print(\"\\nTest dei modelli su dati rumorosi...\")\n",
    "for train_noise, model in models_with_noise.items():\n",
    "    accuracies = []\n",
    "    \n",
    "    for test_noise in test_noise_levels:\n",
    "        x_te_noisy = add_gaussian_noise(x_te_subset, test_noise)\n",
    "        acc = model.score(x_te_noisy, y_te_subset)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    results_noise_training[train_noise] = accuracies\n",
    "    print(f\"Training noise {train_noise}: AUC = {np.trapz(accuracies, test_noise_levels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione curve psicometriche con diversi livelli di rumore nel training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(training_noise_levels)))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "for i, (train_noise, accuracies) in enumerate(results_noise_training.items()):\n",
    "    ax1.plot(test_noise_levels, accuracies, 'o-', \n",
    "           label=f'Training noise σ = {train_noise}',\n",
    "           color=colors[i], linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore (test)', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Effetto del rumore nel training sulla robustezza', fontsize=14)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Analisi quantitativa del miglioramento\n",
    "auc_scores = {}\n",
    "for train_noise, accuracies in results_noise_training.items():\n",
    "    auc = np.trapz(accuracies, test_noise_levels)\n",
    "    auc_scores[train_noise] = auc\n",
    "\n",
    "train_noises = list(auc_scores.keys())\n",
    "aucs = list(auc_scores.values())\n",
    "\n",
    "ax2.plot(train_noises, aucs, 'o-', linewidth=3, markersize=10, color='darkred')\n",
    "ax2.set_xlabel('Rumore nel training (σ)', fontsize=12)\n",
    "ax2.set_ylabel('AUC (Area Under Curve)', fontsize=12)\n",
    "ax2.set_title('Area sotto la curva vs Rumore nel training', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Identifico il miglior livello\n",
    "best_noise = max(auc_scores, key=auc_scores.get)\n",
    "best_auc = auc_scores[best_noise]\n",
    "ax2.scatter(best_noise, best_auc, s=200, color='gold', zorder=5)\n",
    "ax2.annotate(f'Ottimo: σ={best_noise}\\nAUC={best_auc:.3f}', \n",
    "           xy=(best_noise, best_auc),\n",
    "           xytext=(best_noise + 0.05, best_auc - 0.5),\n",
    "           arrowprops=dict(arrowstyle='->', color='gold'),\n",
    "           fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMiglior livello di rumore nel training: σ = {best_noise}\")\n",
    "print(f\"Miglioramento rispetto al modello senza rumore: {(best_auc - auc_scores[0])/auc_scores[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc181c7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto Bonus: Estensione con FashionMNIST\n",
    "\n",
    "Replichiamo alcuni degli esperimenti precedenti utilizzando il dataset FashionMNIST, che presenta maggiore complessità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento FashionMNIST\n",
    "print(\"Caricamento FashionMNIST...\")\n",
    "fashion_tr = FashionMNIST(root=\"./data\", train=True, download=True)\n",
    "fashion_te = FashionMNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing FashionMNIST\n",
    "fashion_tr_data, fashion_tr_labels = fashion_tr.data.numpy(), fashion_tr.targets.numpy()\n",
    "fashion_te_data, fashion_te_labels = fashion_te.data.numpy(), fashion_te.targets.numpy()\n",
    "\n",
    "x_fashion_tr = fashion_tr_data.reshape(60000, 28 * 28) / 255.0\n",
    "x_fashion_te = fashion_te_data.reshape(10000, 28 * 28) / 255.0\n",
    "\n",
    "# Nomi delle classi\n",
    "fashion_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"FashionMNIST caricato: {x_fashion_tr.shape[0]} train, {x_fashion_te.shape[0]} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86301329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione esempi FashionMNIST\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.where(fashion_tr_labels == i)[0][0]\n",
    "    axes[i].imshow(fashion_tr_data[idx], cmap='gray')\n",
    "    axes[i].set_title(f'{i}: {fashion_classes[i]}', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi dal dataset FashionMNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f9d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP su FashionMNIST con architettura: (200, 100)\n",
      "Training time: 28.2s\n",
      "Train accuracy: 0.9410\n",
      "Test accuracy: 0.8936\n",
      "Overfitting: +0.0474\n",
      "\n",
      "Confronto con MNIST:\n",
      "MNIST test accuracy: 0.9815\n",
      "FashionMNIST test accuracy: 0.8936\n",
      "Differenza: +0.0879\n"
     ]
    }
   ],
   "source": [
    "# Training MLP su FashionMNIST con stessa architettura ottimale\n",
    "mlp_fashion = MLPClassifier(\n",
    "    hidden_layer_sizes=best_arch_tuple,\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP su FashionMNIST con architettura: {best_arch_tuple}\")\n",
    "start_time = time.time()\n",
    "mlp_fashion.fit(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_training_time = time.time() - start_time\n",
    "\n",
    "fashion_train_acc = mlp_fashion.score(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_test_acc = mlp_fashion.score(x_fashion_te, fashion_te_labels)\n",
    "\n",
    "print(f\"Training time: {fashion_training_time:.1f}s\")\n",
    "print(f\"Train accuracy: {fashion_train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Overfitting: {fashion_train_acc - fashion_test_acc:+.4f}\")\n",
    "\n",
    "# Confronto con MNIST\n",
    "mnist_test_acc = mlp_best.score(x_te, mnist_te_labels)\n",
    "print(f\"\\nConfronto con MNIST:\")\n",
    "print(f\"MNIST test accuracy: {mnist_test_acc:.4f}\")\n",
    "print(f\"FashionMNIST test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Differenza: {mnist_test_acc - fashion_test_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d776ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve psicometriche comparative MNIST vs FashionMNIST\n",
    "noise_levels_comp = np.arange(0, 0.3, 0.05)\n",
    "acc_mnist = []\n",
    "acc_fashion = []\n",
    "\n",
    "# Subset per velocità\n",
    "x_fashion_te_subset = x_fashion_te[:2000]\n",
    "y_fashion_te_subset = fashion_te_labels[:2000]\n",
    "\n",
    "print(\"Calcolo curve psicometriche comparative...\")\n",
    "for noise_std in noise_levels_comp:\n",
    "    # MNIST\n",
    "    x_noisy_mnist = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mnist.append(mlp_best.score(x_noisy_mnist, y_te_subset))\n",
    "    \n",
    "    # FashionMNIST\n",
    "    x_noisy_fashion = add_gaussian_noise(x_fashion_te_subset, noise_std)\n",
    "    acc_fashion.append(mlp_fashion.score(x_noisy_fashion, y_fashion_te_subset))\n",
    "    \n",
    "    print(f\"Noise {noise_std:.2f}: MNIST {acc_mnist[-1]:.3f}, Fashion {acc_fashion[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione comparativa finale\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Grafico 1: Curve psicometriche comparative\n",
    "ax1.plot(noise_levels_comp, acc_mnist, 'o-', label='MNIST', \n",
    "         linewidth=3, markersize=8, color='blue')\n",
    "ax1.plot(noise_levels_comp, acc_fashion, 's-', label='FashionMNIST', \n",
    "         linewidth=3, markersize=8, color='red')\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Confronto robustezza al rumore:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Matrice di confusione FashionMNIST\n",
    "y_pred_fashion = mlp_fashion.predict(x_fashion_te)\n",
    "cm_fashion = metrics.confusion_matrix(fashion_te_labels, y_pred_fashion)\n",
    "\n",
    "im = ax2.imshow(cm_fashion, cmap='Blues')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_yticks(range(10))\n",
    "ax2.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax2.set_yticklabels([f'{i}: {fashion_classes[i][:7]}' for i in range(10)], fontsize=10)\n",
    "ax2.set_xlabel('Predetto', fontsize=12)\n",
    "ax2.set_ylabel('Vero', fontsize=12)\n",
    "ax2.set_title('Matrice di Confusione\\nFashionMNIST', fontsize=14)\n",
    "\n",
    "# Grafico 3: Confronto accuratezze per classe\n",
    "fashion_class_accs = []\n",
    "mnist_class_accs = []\n",
    "\n",
    "for digit in range(10):\n",
    "    # FashionMNIST\n",
    "    mask_f = fashion_te_labels == digit\n",
    "    acc_f = np.sum((y_pred_fashion == fashion_te_labels) & mask_f) / np.sum(mask_f)\n",
    "    fashion_class_accs.append(acc_f)\n",
    "    \n",
    "    # MNIST\n",
    "    mask_m = mnist_te_labels == digit\n",
    "    acc_m = np.sum((y_pred == mnist_te_labels) & mask_m) / np.sum(mask_m)\n",
    "    mnist_class_accs.append(acc_m)\n",
    "\n",
    "x_pos = np.arange(10)\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x_pos - width/2, mnist_class_accs, width, label='MNIST', alpha=0.8, color='blue')\n",
    "ax3.bar(x_pos + width/2, fashion_class_accs, width, label='FashionMNIST', alpha=0.8, color='red')\n",
    "ax3.set_xlabel('Classe', fontsize=12)\n",
    "ax3.set_ylabel('Accuratezza per classe', fontsize=12)\n",
    "ax3.set_title('Accuratezza per classe:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Confronto errori più frequenti FashionMNIST\n",
    "fashion_confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm_fashion[i, j] > 0:\n",
    "            fashion_confusion_pairs.append({\n",
    "                'true_class': fashion_classes[i],\n",
    "                'pred_class': fashion_classes[j],\n",
    "                'count': cm_fashion[i, j]\n",
    "            })\n",
    "\n",
    "df_fashion_confusion = pd.DataFrame(fashion_confusion_pairs)\n",
    "top_fashion_errors = df_fashion_confusion.nlargest(8, 'count')\n",
    "\n",
    "y_pos = np.arange(len(top_fashion_errors))\n",
    "ax4.barh(y_pos, top_fashion_errors['count'], color='coral', alpha=0.8)\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([f\"{row['true_class'][:6]} → {row['pred_class'][:6]}\" \n",
    "                    for _, row in top_fashion_errors.iterrows()], fontsize=10)\n",
    "ax4.set_xlabel('Numero di errori', fontsize=12)\n",
    "ax4.set_title('Top 8 errori più frequenti\\nFashionMNIST', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f280b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusioni\n",
    "\n",
    "### Riepilogo dei risultati principali:\n",
    "\n",
    "1. **Effetto degli iperparametri (Punto A):**\n",
    "   - L'architettura ottimale trovata presenta un buon bilanciamento tra capacità e generalizzazione\n",
    "   - Le CNN superano consistentemente le MLP grazie alla loro capacità di estrarre features spaziali\n",
    "   - Il learning rate ottimale si colloca tipicamente tra 0.001-0.01\n",
    "   - L'overfitting aumenta con la complessità del modello ma può essere controllato con early stopping\n",
    "\n",
    "2. **Cifre più difficili (Punto B):**\n",
    "   - Le coppie più confuse sono tipicamente quelle visivamente simili (es. 4↔9, 3↔8, 7↔9)\n",
    "   - La matrice di confusione rivela pattern sistematici negli errori di classificazione\n",
    "   - Alcune cifre (come 1 e 0) sono generalmente più facili da riconoscere\n",
    "\n",
    "3. **Robustezza al rumore (Punto C):**\n",
    "   - Le curve psicometriche mostrano un degrado graduale e prevedibile delle prestazioni\n",
    "   - Il modello mantiene performance ragionevoli fino a livelli moderati di rumore (σ ≈ 0.2)\n",
    "   - La robustezza dipende dalla qualità dell'architettura e del training\n",
    "\n",
    "4. **Effetto dei dati di training (Punto D):**\n",
    "   - Con solo il 10% dei dati, l'accuratezza cala ma rimane utilizzabile (>85%)\n",
    "   - Il modello mostra buone capacità di generalizzazione anche con dati molto limitati\n",
    "   - L'overfitting aumenta significativamente con dataset piccoli\n",
    "\n",
    "5. **Training con rumore (Punto E):**\n",
    "   - L'aggiunta di rumore moderato durante il training migliora la robustezza\n",
    "   - Il livello ottimale di rumore nel training bilancia robustezza e performance su dati puliti\n",
    "   - La data augmentation con rumore è una tecnica efficace di regolarizzazione\n",
    "\n",
    "6. **FashionMNIST (Bonus):**\n",
    "   - Il dataset è significativamente più difficile di MNIST (~15-20% di accuratezza in meno)\n",
    "   - Le prestazioni degradano più rapidamente con l'aggiunta di rumore\n",
    "   - Alcuni capi di abbigliamento (come shirt/pullover) sono particolarmente difficili da distinguere\n",
    "\n",
    "### Implicazioni pratiche:\n",
    "\n",
    "- La scelta dell'architettura e degli iperparametri ha un impatto significativo sulle prestazioni\n",
    "- La robustezza al rumore può essere migliorata attraverso tecniche di data augmentation\n",
    "- Anche con risorse limitate (dati o tempo di training), è possibile ottenere risultati ragionevoli\n",
    "- I dataset più complessi richiedono architetture più sofisticate e tecniche di regolarizzazione avanzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RIEPILOGO FINALE DEL PROGETTO\n",
      "============================================================\n",
      "\n",
      "Punto A - Analisi Iperparametri:\n",
      "  • Architetture MLP testate: 7\n",
      "  • Architetture CNN testate: 5\n",
      "  • Learning rates testati: 5\n",
      "  • Miglior MLP: (200, 100) -> Acc: 0.9817\n",
      "\n",
      "Punto B - Analisi Errori:\n",
      "  • Cifra più difficile: 8.0 (Error rate: 0.027)\n",
      "  • Cifra più facile: 1.0 (Error rate: 0.008)\n",
      "  • Confusione più frequente: 4.0 → 9.0 (8.0 errori)\n",
      "\n",
      "Punto C - Robustezza al Rumore:\n",
      "  • Livelli di rumore testati: 10\n",
      "  • Accuratezza senza rumore: 0.9725\n",
      "  • Accuratezza con rumore max (σ=0.45): 0.5100\n",
      "\n",
      "Punto D - Riduzione Dati:\n",
      "  • Accuratezza con 100% dati: 0.9794\n",
      "  • Accuratezza con 10% dati: 0.9391\n",
      "  • Perdita con 90% dati in meno: 0.0403\n",
      "\n",
      "Punto E - Training con Rumore:\n",
      "  • Livelli di rumore nel training testati: 5\n",
      "  • Miglior livello di rumore: σ = 0.2\n",
      "  • Miglioramento AUC: 12.8%\n",
      "\n",
      "Bonus - FashionMNIST:\n",
      "  • Accuratezza MNIST: 0.9815\n",
      "  • Accuratezza FashionMNIST: 0.8936\n",
      "  • Differenza di difficoltà: 0.0879\n",
      "\n",
      "============================================================\n",
      "PROGETTO COMPLETATO CON SUCCESSO!\n",
      "Tutti i 5 punti + bonus implementati e analizzati.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Statistiche finali del progetto\n",
    "print(\"=\"*60)\n",
    "print(\"RIEPILOGO FINALE DEL PROGETTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nPunto A - Analisi Iperparametri:\")\n",
    "print(f\"  • Architetture MLP testate: {len(architectures)}\")\n",
    "print(f\"  • Architetture CNN testate: {len(cnn_architectures)}\")\n",
    "print(f\"  • Learning rates testati: {len(learning_rates)}\")\n",
    "print(f\"  • Miglior MLP: {best_arch} -> Acc: {df_arch['test_accuracy'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPunto B - Analisi Errori:\")\n",
    "print(f\"  • Cifra più difficile: {df_errors_sorted.iloc[0]['digit']} (Error rate: {df_errors_sorted.iloc[0]['error_rate']:.3f})\")\n",
    "print(f\"  • Cifra più facile: {df_errors_sorted.iloc[-1]['digit']} (Error rate: {df_errors_sorted.iloc[-1]['error_rate']:.3f})\")\n",
    "print(f\"  • Confusione più frequente: {df_confusion_sorted.iloc[0]['true_digit']} → {df_confusion_sorted.iloc[0]['predicted_digit']} ({df_confusion_sorted.iloc[0]['count']} errori)\")\n",
    "\n",
    "print(f\"\\nPunto C - Robustezza al Rumore:\")\n",
    "print(f\"  • Livelli di rumore testati: {len(noise_levels)}\")\n",
    "print(f\"  • Accuratezza senza rumore: {accuracies_mlp[0]:.4f}\")\n",
    "print(f\"  • Accuratezza con rumore max (σ={noise_levels[-1]:.2f}): {accuracies_mlp[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nPunto D - Riduzione Dati:\")\n",
    "print(f\"  • Accuratezza con 100% dati: {df_reduction[df_reduction['percentage']==100]['test_accuracy'].iloc[0]:.4f}\")\n",
    "print(f\"  • Accuratezza con 10% dati: {df_reduction[df_reduction['percentage']==10]['test_accuracy'].iloc[0]:.4f}\")\n",
    "print(f\"  • Perdita con 90% dati in meno: {(df_reduction[df_reduction['percentage']==100]['test_accuracy'].iloc[0] - df_reduction[df_reduction['percentage']==10]['test_accuracy'].iloc[0]):.4f}\")\n",
    "\n",
    "print(f\"\\nPunto E - Training con Rumore:\")\n",
    "print(f\"  • Livelli di rumore nel training testati: {len(training_noise_levels)}\")\n",
    "print(f\"  • Miglior livello di rumore: σ = {best_noise}\")\n",
    "print(f\"  • Miglioramento AUC: {((best_auc - auc_scores[0])/auc_scores[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nBonus - FashionMNIST:\")\n",
    "print(f\"  • Accuratezza MNIST: {mnist_test_acc:.4f}\")\n",
    "print(f\"  • Accuratezza FashionMNIST: {fashion_test_acc:.4f}\")\n",
    "print(f\"  • Differenza di difficoltà: {(mnist_test_acc - fashion_test_acc):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROGETTO COMPLETATO CON SUCCESSO!\")\n",
    "print(\"Tutti i 5 punti + bonus implementati e analizzati.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Mini Progetto IA",
   "language": "python",
   "name": "mini-progetto-ia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
