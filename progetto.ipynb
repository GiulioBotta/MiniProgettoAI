{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77f4b15",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Mini Progetto Intelligenza Artificiale - Riconoscimento cifre manoscritte\n",
    "\n",
    "**Nome:** [Inserire nome]  \n",
    "**Cognome:** [Inserire cognome]  \n",
    "**Matricola:** [Inserire matricola]  \n",
    "**Data consegna:** [Inserire data]\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo progetto esploreremo il riconoscimento di cifre manoscritte utilizzando il dataset MNIST, implementando simulazioni per studiare come diversi fattori influenzano le prestazioni dei modelli di deep learning. Analizzeremo in particolare l'impatto degli iperparametri, la robustezza al rumore e l'effetto della quantità di dati di training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abb844",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Importazione delle librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe329e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione per riproducibilità\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dae275",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Caricamento e preparazione del dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175256c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset MNIST...\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dataset MNIST\n",
    "print(\"Caricamento dataset MNIST...\")\n",
    "mnist_tr = MNIST(root=\"./data\", train=True, download=True)\n",
    "mnist_te = MNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato: 60000 train, 10000 test\n",
      "Forma dati MLP: (60000, 784)\n",
      "Forma dati CNN: (60000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Conversione in array numpy\n",
    "mnist_tr_data, mnist_tr_labels = mnist_tr.data.numpy(), mnist_tr.targets.numpy()\n",
    "mnist_te_data, mnist_te_labels = mnist_te.data.numpy(), mnist_te.targets.numpy()\n",
    "\n",
    "# Preprocessing per MLP (vettorizzazione e normalizzazione)\n",
    "x_tr = mnist_tr_data.reshape(60000, 28 * 28) / 255.0\n",
    "x_te = mnist_te_data.reshape(10000, 28 * 28) / 255.0\n",
    "\n",
    "# Preprocessing per CNN (mantenendo formato 2D)\n",
    "x_tr_conv = x_tr.reshape(-1, 28, 28, 1)\n",
    "x_te_conv = x_te.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Dataset caricato: {x_tr.shape[0]} train, {x_te.shape[0]} test\")\n",
    "print(f\"Forma dati MLP: {x_tr.shape}\")\n",
    "print(f\"Forma dati CNN: {x_tr_conv.shape}\")\n",
    "\n",
    "# Visualizzazione esempi del dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "fig.suptitle('Dataset MNIST - Esempi per Cifra', fontsize=14)\n",
    "\n",
    "for digit in range(10):\n",
    "    idx = np.where(mnist_tr_labels == digit)[0][0]\n",
    "    ax = axes[digit//5, digit%5]\n",
    "    ax.imshow(mnist_tr_data[idx], cmap='gray')\n",
    "    ax.set_title(f'Cifra {digit}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242b1b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto A: Effetto degli iperparametri sulle prestazioni\n",
    "\n",
    "Analizziamo come variano le prestazioni dei modelli MLP e CNN al variare del numero di neuroni, strati nascosti e altri iperparametri chiave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7af60f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Analisi MLP - Variazione numero di neuroni e strati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924122e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test delle diverse architetture MLP...\n",
      "\n",
      "Architettura: (50,)\n",
      "Train accuracy: 0.9891\n",
      "Test accuracy: 0.9707\n",
      "Overfitting: +0.0184\n",
      "Training time: 7.4s\n",
      "\n",
      "Architettura: (100,)\n",
      "Train accuracy: 0.9971\n",
      "Test accuracy: 0.9771\n",
      "Overfitting: +0.0201\n",
      "Training time: 11.3s\n",
      "\n",
      "Architettura: (200,)\n",
      "Train accuracy: 0.9982\n",
      "Test accuracy: 0.9801\n",
      "Overfitting: +0.0181\n",
      "Training time: 29.4s\n",
      "\n",
      "Architettura: (50, 50)\n",
      "Train accuracy: 0.9905\n",
      "Test accuracy: 0.9729\n",
      "Overfitting: +0.0176\n",
      "Training time: 8.3s\n",
      "\n",
      "Architettura: (100, 100)\n",
      "Train accuracy: 0.9967\n",
      "Test accuracy: 0.9786\n",
      "Overfitting: +0.0181\n",
      "Training time: 10.7s\n",
      "\n",
      "Architettura: (200, 100)\n",
      "Train accuracy: 0.9980\n",
      "Test accuracy: 0.9817\n",
      "Overfitting: +0.0163\n",
      "Training time: 37.6s\n",
      "\n",
      "Architettura: (100, 50, 25)\n",
      "Train accuracy: 0.9895\n",
      "Test accuracy: 0.9741\n",
      "Overfitting: +0.0154\n",
      "Training time: 16.8s\n"
     ]
    }
   ],
   "source": [
    "# Definisco diverse architetture da testare\n",
    "architectures = [\n",
    "    (50,),           # 1 strato con 50 neuroni\n",
    "    (100,),          # 1 strato con 100 neuroni  \n",
    "    (200,),          # 1 strato con 200 neuroni\n",
    "    (50, 50),        # 2 strati con 50 neuroni ciascuno\n",
    "    (100, 100),      # 2 strati con 100 neuroni ciascuno\n",
    "    (200, 100),      # 2 strati: 200 e 100 neuroni\n",
    "    (100, 50, 25),   # 3 strati decrescenti\n",
    "]\n",
    "\n",
    "results_architecture = []\n",
    "\n",
    "print(\"Test delle diverse architetture MLP...\")\n",
    "for arch in architectures:\n",
    "    print(f\"\\nArchitettura: {arch}\")\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        tol=0.001\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp.fit(x_tr, mnist_tr_labels)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = mlp.score(x_tr, mnist_tr_labels)\n",
    "    test_acc = mlp.score(x_te, mnist_te_labels)\n",
    "    \n",
    "    results_architecture.append({\n",
    "        'architecture': str(arch),\n",
    "        'n_layers': len(arch),\n",
    "        'total_neurons': sum(arch),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'n_iter': mlp.n_iter_\n",
    "    })\n",
    "    \n",
    "    print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Overfitting: {train_acc - test_acc:+.4f}\")\n",
    "    print(f\"Training time: {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84403193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione risultati architetture\n",
    "df_arch = pd.DataFrame(results_architecture)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Grafico 1: Accuratezza vs numero totale di neuroni\n",
    "scatter = ax1.scatter(df_arch['total_neurons'], df_arch['test_accuracy'], \n",
    "                     c=df_arch['n_layers'], s=100, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('Numero totale di neuroni')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Accuratezza vs Numero di neuroni')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax1, label='Numero di strati')\n",
    "\n",
    "# Grafico 2: Confronto train vs test accuracy\n",
    "x_pos = np.arange(len(df_arch))\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, df_arch['train_accuracy'], width, label='Train', alpha=0.8)\n",
    "ax2.bar(x_pos + width/2, df_arch['test_accuracy'], width, label='Test', alpha=0.8)\n",
    "ax2.set_xlabel('Architettura')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Train vs Test Accuracy per architettura')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([arch.split(',')[0].strip('(') for arch in df_arch['architecture']], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 3: Overfitting vs complessità\n",
    "ax3.scatter(df_arch['total_neurons'], df_arch['overfitting'], \n",
    "           c=df_arch['n_layers'], s=100, cmap='plasma', alpha=0.7)\n",
    "ax3.set_xlabel('Numero totale di neuroni')\n",
    "ax3.set_ylabel('Overfitting (Train - Test)')\n",
    "ax3.set_title('Overfitting vs Complessità del modello')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Grafico 4: Tempo vs Accuratezza\n",
    "ax4.scatter(df_arch['training_time'], df_arch['test_accuracy'], \n",
    "           c=df_arch['total_neurons'], s=100, cmap='coolwarm', alpha=0.7)\n",
    "ax4.set_xlabel('Tempo di training (s)')\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "ax4.set_title('Efficienza: Tempo vs Accuratezza')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25ef4c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Analisi CNN - Variazione architettura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b30d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test delle diverse architetture CNN...\n",
      "\n",
      "Architettura CNN_Simple: Conv filters=[32], Dense=[50]\n",
      "Train accuracy: 0.9926\n",
      "Test accuracy: 0.9817\n",
      "Training time: 66.2s\n",
      "\n",
      "Architettura CNN_Dense100: Conv filters=[32], Dense=[100]\n",
      "Train accuracy: 0.9946\n",
      "Test accuracy: 0.9845\n",
      "Training time: 69.0s\n",
      "\n",
      "Architettura CNN_64Filters: Conv filters=[64], Dense=[100]\n",
      "Train accuracy: 0.9972\n",
      "Test accuracy: 0.9857\n",
      "Training time: 140.2s\n",
      "\n",
      "Architettura CNN_2Conv: Conv filters=[32, 64], Dense=[100]\n",
      "Train accuracy: 0.9961\n",
      "Test accuracy: 0.9898\n",
      "Training time: 139.7s\n",
      "\n",
      "Architettura CNN_3Conv: Conv filters=[32, 64, 128], Dense=[100]\n",
      "Train accuracy: 0.9921\n",
      "Test accuracy: 0.9838\n",
      "Training time: 172.7s\n"
     ]
    }
   ],
   "source": [
    "# Test di diverse architetture CNN\n",
    "cnn_architectures = [\n",
    "    {'filters': [32], 'dense': [50], 'name': 'CNN_Simple'},\n",
    "    {'filters': [32], 'dense': [100], 'name': 'CNN_Dense100'},\n",
    "    {'filters': [64], 'dense': [100], 'name': 'CNN_64Filters'},\n",
    "    {'filters': [32, 64], 'dense': [100], 'name': 'CNN_2Conv'},\n",
    "    {'filters': [32, 64, 128], 'dense': [100], 'name': 'CNN_3Conv'},\n",
    "]\n",
    "\n",
    "results_cnn = []\n",
    "\n",
    "print(\"Test delle diverse architetture CNN...\")\n",
    "for i, arch in enumerate(cnn_architectures):\n",
    "    print(f\"\\nArchitettura {arch['name']}: Conv filters={arch['filters']}, Dense={arch['dense']}\")\n",
    "    \n",
    "    # Costruzione del modello\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Strati convoluzionali\n",
    "    for j, filters in enumerate(arch['filters']):\n",
    "        if j == 0:\n",
    "            model.add(keras.layers.Conv2D(filters=filters, kernel_size=(3,3), \n",
    "                                         activation='relu', input_shape=(28,28,1)))\n",
    "        else:\n",
    "            model.add(keras.layers.Conv2D(filters=filters, kernel_size=(3,3), \n",
    "                                         activation='relu'))\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    # Strati densi\n",
    "    for units in arch['dense']:\n",
    "        model.add(keras.layers.Dense(units=units, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Training con early stopping\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_tr_conv, mnist_tr_labels, \n",
    "                       validation_split=0.1,\n",
    "                       epochs=10, \n",
    "                       batch_size=128,\n",
    "                       verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(x_te_conv, mnist_te_labels, verbose=0)\n",
    "    train_loss, train_acc = model.evaluate(x_tr_conv, mnist_tr_labels, verbose=0)\n",
    "    \n",
    "    results_cnn.append({\n",
    "        'name': arch['name'],\n",
    "        'architecture': f\"Conv={arch['filters']}, Dense={arch['dense']}\",\n",
    "        'n_conv_layers': len(arch['filters']),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'val_accuracy': history.history['val_accuracy'][-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Training time: {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5f62c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Analisi altri iperparametri significativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dell'effetto del learning rate...\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Test accuracy: 0.9753\n",
      "Iterations: 50\n",
      "\n",
      "Learning rate: 0.001\n",
      "Test accuracy: 0.9803\n",
      "Iterations: 39\n",
      "\n",
      "Learning rate: 0.01\n",
      "Test accuracy: 0.9727\n",
      "Iterations: 50\n",
      "\n",
      "Learning rate: 0.1\n",
      "Test accuracy: 0.8212\n",
      "Iterations: 14\n",
      "\n",
      "Learning rate: 0.5\n",
      "Test accuracy: 0.1135\n",
      "Iterations: 13\n"
     ]
    }
   ],
   "source": [
    "# Test effetto del learning rate su MLP\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "results_lr = []\n",
    "\n",
    "print(\"Test dell'effetto del learning rate...\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nLearning rate: {lr}\")\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp.fit(x_tr, mnist_tr_labels)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    test_acc = mlp.score(x_te, mnist_te_labels)\n",
    "    \n",
    "    results_lr.append({\n",
    "        'learning_rate': lr,\n",
    "        'test_accuracy': test_acc,\n",
    "        'training_time': training_time,\n",
    "        'n_iter': mlp.n_iter_,\n",
    "        'final_loss': mlp.loss_curve_[-1] if hasattr(mlp, 'loss_curve_') and mlp.loss_curve_ else np.nan\n",
    "    })\n",
    "    \n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Iterations: {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e43af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione effetto learning rate\n",
    "df_lr = pd.DataFrame(results_lr)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Grafico accuratezza vs learning rate\n",
    "ax1.semilogx(df_lr['learning_rate'], df_lr['test_accuracy'], 'o-', markersize=10, linewidth=2)\n",
    "ax1.set_xlabel('Learning Rate')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Effetto del Learning Rate sulle prestazioni')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotazioni sui punti\n",
    "for _, row in df_lr.iterrows():\n",
    "    ax1.annotate(f'{row[\"test_accuracy\"]:.3f}', \n",
    "                (row['learning_rate'], row['test_accuracy']),\n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Grafico tempo vs learning rate\n",
    "ax2.semilogx(df_lr['learning_rate'], df_lr['training_time'], 's-', \n",
    "            markersize=10, linewidth=2, color='orange')\n",
    "ax2.set_xlabel('Learning Rate')\n",
    "ax2.set_ylabel('Training Time (s)')\n",
    "ax2.set_title('Learning Rate vs Tempo di Training')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229592b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto B: Analisi delle cifre più difficili da riconoscere\n",
    "\n",
    "Utilizziamo la matrice di confusione per identificare quali cifre il modello MLP trova più difficili da classificare correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP con architettura ottimale: (200, 100)\n",
      "Accuratezza sul test set: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# Addestro un MLP con architettura ottimale trovata precedentemente\n",
    "best_arch = df_arch.loc[df_arch['test_accuracy'].idxmax(), 'architecture']\n",
    "best_arch_tuple = eval(best_arch)\n",
    "\n",
    "mlp_best = MLPClassifier(\n",
    "    hidden_layer_sizes=best_arch_tuple,\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP con architettura ottimale: {best_arch}\")\n",
    "mlp_best.fit(x_tr, mnist_tr_labels)\n",
    "print(f\"Accuratezza sul test set: {mlp_best.score(x_te, mnist_te_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calcolo predizioni e matrice di confusione\n",
    "y_pred = mlp_best.predict(x_te)\n",
    "\n",
    "# Visualizzazione matrice di confusione\n",
    "cm = metrics.confusion_matrix(mnist_te_labels, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Matrice di Confusione - MLP su MNIST', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifre ordinate per difficoltà (tasso di errore):\n",
      "   digit  total_samples  errors  error_rate  accuracy\n",
      "8      8            974      26    0.026694  0.973306\n",
      "9      9           1009      26    0.025768  0.974232\n",
      "6      6            958      21    0.021921  0.978079\n",
      "2      2           1032      22    0.021318  0.978682\n",
      "3      3           1010      20    0.019802  0.980198\n",
      "7      7           1028      20    0.019455  0.980545\n",
      "4      4            982      18    0.018330  0.981670\n",
      "5      5            892      14    0.015695  0.984305\n",
      "0      0            980       9    0.009184  0.990816\n",
      "1      1           1135       9    0.007930  0.992070\n"
     ]
    }
   ],
   "source": [
    "# Analisi degli errori più frequenti\n",
    "errors_per_digit = []\n",
    "for digit in range(10):\n",
    "    mask = mnist_te_labels == digit\n",
    "    total = np.sum(mask)\n",
    "    correct = np.sum((y_pred == mnist_te_labels) & mask)\n",
    "    error_rate = 1 - (correct / total)\n",
    "    \n",
    "    errors_per_digit.append({\n",
    "        'digit': digit,\n",
    "        'total_samples': total,\n",
    "        'correct': correct,\n",
    "        'errors': total - correct,\n",
    "        'error_rate': error_rate,\n",
    "        'accuracy': correct / total\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(errors_per_digit)\n",
    "df_errors_sorted = df_errors.sort_values('error_rate', ascending=False)\n",
    "\n",
    "print(\"Cifre ordinate per difficoltà (tasso di errore):\")\n",
    "print(df_errors_sorted[['digit', 'total_samples', 'errors', 'error_rate', 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fee995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le 10 coppie di cifre più confuse:\n",
      "4.0 → 9.0: 8.0 errori (0.8%)\n",
      "3.0 → 9.0: 8.0 errori (0.8%)\n",
      "7.0 → 2.0: 8.0 errori (0.8%)\n",
      "9.0 → 4.0: 7.0 errori (0.7%)\n",
      "5.0 → 3.0: 7.0 errori (0.8%)\n",
      "6.0 → 4.0: 6.0 errori (0.6%)\n",
      "7.0 → 9.0: 5.0 errori (0.5%)\n",
      "4.0 → 2.0: 5.0 errori (0.5%)\n",
      "2.0 → 7.0: 5.0 errori (0.5%)\n",
      "2.0 → 0.0: 5.0 errori (0.5%)\n"
     ]
    }
   ],
   "source": [
    "# Visualizzazione delle coppie di cifre più confuse\n",
    "confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true_digit': i,\n",
    "                'predicted_digit': j,\n",
    "                'count': cm[i, j],\n",
    "                'percentage': cm[i, j] / np.sum(cm[i, :]) * 100\n",
    "            })\n",
    "\n",
    "df_confusion = pd.DataFrame(confusion_pairs)\n",
    "df_confusion_sorted = df_confusion.sort_values('count', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nLe 10 coppie di cifre più confuse:\")\n",
    "for _, row in df_confusion_sorted.iterrows():\n",
    "    print(f\"{row['true_digit']} → {row['predicted_digit']}: {row['count']} errori ({row['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a7f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione esempi di cifre classificate erroneamente\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "example_idx = 0\n",
    "for _, conf_pair in df_confusion_sorted.head(4).iterrows():\n",
    "    true_digit = conf_pair['true_digit']\n",
    "    pred_digit = conf_pair['predicted_digit']\n",
    "    \n",
    "    # Trovo esempi di questo tipo di errore\n",
    "    error_mask = (mnist_te_labels == true_digit) & (y_pred == pred_digit)\n",
    "    error_indices = np.where(error_mask)[0]\n",
    "    \n",
    "    # Mostro fino a 5 esempi per ogni coppia\n",
    "    for i in range(min(5, len(error_indices))):\n",
    "        if example_idx < 20:\n",
    "            idx = error_indices[i]\n",
    "            axes[example_idx].imshow(mnist_te_data[idx], cmap='gray')\n",
    "            axes[example_idx].set_title(f'Vero: {true_digit}, Predetto: {pred_digit}', fontsize=10)\n",
    "            axes[example_idx].axis('off')\n",
    "            example_idx += 1\n",
    "\n",
    "# Nascondo assi non utilizzati\n",
    "for i in range(example_idx, 20):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi di cifre classificate erroneamente', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739254c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Punto C: Curve psicometriche - Effetto del rumore\n",
    "\n",
    "Seguendo la metodologia dell'articolo di Testolin et al. (2017), analizziamo come l'accuratezza degrada all'aumentare del rumore Gaussiano aggiunto alle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af4f1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Funzione per aggiungere rumore Gaussiano\n",
    "def add_gaussian_noise(images, noise_std):\n",
    "    \"\"\"\n",
    "    Aggiunge rumore Gaussiano alle immagini.\n",
    "    \n",
    "    Args:\n",
    "        images: array di immagini\n",
    "        noise_std: deviazione standard del rumore\n",
    "    \n",
    "    Returns:\n",
    "        Immagini con rumore, clippate tra 0 e 1\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_std, images.shape)\n",
    "    noisy_images = images + noise\n",
    "    return np.clip(noisy_images, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aa471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo curve psicometriche per MLP...\n",
      "Noise std: 0.000 - MLP acc: 0.9725\n",
      "Noise std: 0.050 - MLP acc: 0.9730\n",
      "Noise std: 0.100 - MLP acc: 0.9680\n",
      "Noise std: 0.150 - MLP acc: 0.9480\n",
      "Noise std: 0.200 - MLP acc: 0.8990\n",
      "Noise std: 0.250 - MLP acc: 0.8105\n",
      "Noise std: 0.300 - MLP acc: 0.7160\n",
      "Noise std: 0.350 - MLP acc: 0.6375\n",
      "Noise std: 0.400 - MLP acc: 0.5610\n",
      "Noise std: 0.450 - MLP acc: 0.5100\n",
      "\n",
      "Calcolo curve psicometriche per CNN...\n",
      "Noise std: 0.000 - CNN acc: 0.9795\n",
      "Noise std: 0.050 - CNN acc: 0.9760\n",
      "Noise std: 0.100 - CNN acc: 0.9775\n",
      "Noise std: 0.150 - CNN acc: 0.9720\n",
      "Noise std: 0.200 - CNN acc: 0.9680\n",
      "Noise std: 0.250 - CNN acc: 0.9550\n",
      "Noise std: 0.300 - CNN acc: 0.9330\n",
      "Noise std: 0.350 - CNN acc: 0.8890\n",
      "Noise std: 0.400 - CNN acc: 0.8050\n",
      "Noise std: 0.450 - CNN acc: 0.7355\n"
     ]
    }
   ],
   "source": [
    "# Test con diversi livelli di rumore\n",
    "noise_levels = np.arange(0, 0.5, 0.05)\n",
    "accuracies_mlp = []\n",
    "\n",
    "# Uso un subset del test set per velocizzare\n",
    "subset_size = 2000\n",
    "x_te_subset = x_te[:subset_size]\n",
    "y_te_subset = mnist_te_labels[:subset_size]\n",
    "\n",
    "print(\"Calcolo curve psicometriche per MLP...\")\n",
    "for noise_std in noise_levels:\n",
    "    x_te_noisy = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mlp = mlp_best.score(x_te_noisy, y_te_subset)\n",
    "    accuracies_mlp.append(acc_mlp)\n",
    "    print(f\"Noise std: {noise_std:.3f} - MLP acc: {acc_mlp:.4f}\")\n",
    "\n",
    "# Test anche con CNN se disponibile\n",
    "if 'model' in locals():\n",
    "    print(\"\\nCalcolo curve psicometriche per CNN...\")\n",
    "    accuracies_cnn = []\n",
    "    x_te_conv_subset = x_te_conv[:subset_size]\n",
    "    \n",
    "    for noise_std in noise_levels:\n",
    "        x_te_conv_noisy = add_gaussian_noise(x_te_conv_subset, noise_std)\n",
    "        test_loss, acc_cnn = model.evaluate(x_te_conv_noisy, y_te_subset, verbose=0)\n",
    "        accuracies_cnn.append(acc_cnn)\n",
    "        print(f\"Noise std: {noise_std:.3f} - CNN acc: {acc_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a74f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Axes' object has no attribute 'subplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m digit_idx = \u001b[32m0\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, noise \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(noise_examples):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43max2\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubplot\u001b[49m(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, i+\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m     noisy_img = add_gaussian_noise(x_te[digit_idx:digit_idx+\u001b[32m1\u001b[39m], noise)[\u001b[32m0\u001b[39m]\n\u001b[32m     28\u001b[39m     plt.imshow(noisy_img.reshape(\u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m), cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m, vmin=\u001b[32m0\u001b[39m, vmax=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Axes' object has no attribute 'subplot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione curve psicometriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "ax1.plot(noise_levels, accuracies_mlp, 'o-', label='MLP', linewidth=3, markersize=8)\n",
    "if 'accuracies_cnn' in locals():\n",
    "    ax1.plot(noise_levels, accuracies_cnn, 's-', label='CNN', linewidth=3, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Curve Psicometriche - Robustezza al rumore', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Evidenziare punti chiave\n",
    "for i, (noise, acc) in enumerate(zip(noise_levels[::2], accuracies_mlp[::2])):\n",
    "    ax1.annotate(f'{acc:.3f}', (noise, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=9)\n",
    "\n",
    "# Grafico 2: Esempi di cifre con diversi livelli di rumore\n",
    "noise_examples = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "digit_idx = 0\n",
    "\n",
    "for i, noise in enumerate(noise_examples):\n",
    "    ax2.subplot(1, 5, i+1)\n",
    "    noisy_img = add_gaussian_noise(x_te[digit_idx:digit_idx+1], noise)[0]\n",
    "    plt.imshow(noisy_img.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(f'σ = {noise}')\n",
    "    plt.axis('off')\n",
    "\n",
    "ax2.remove()\n",
    "plt.figtext(0.7, 0.02, f'Esempi di cifra {mnist_te_labels[digit_idx]} con diversi livelli di rumore', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8eafb6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto D: Effetto della riduzione dei dati di training\n",
    "\n",
    "Analizziamo come le prestazioni degradano quando riduciamo drasticamente la quantità di dati di training disponibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e8b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test con riduzione dei dati di training...\n",
      "\n",
      "Training con 1% dei dati...\n",
      "Samples utilizzati: 596 / 60000\n",
      "Train acc: 0.9883, Test acc: 0.8739\n",
      "\n",
      "Training con 5% dei dati...\n",
      "Samples utilizzati: 2996 / 60000\n",
      "Train acc: 0.9933, Test acc: 0.9293\n",
      "\n",
      "Training con 10% dei dati...\n",
      "Samples utilizzati: 5996 / 60000\n",
      "Train acc: 0.9915, Test acc: 0.9391\n",
      "\n",
      "Training con 25% dei dati...\n",
      "Samples utilizzati: 14995 / 60000\n",
      "Train acc: 0.9910, Test acc: 0.9585\n",
      "\n",
      "Training con 50% dei dati...\n",
      "Samples utilizzati: 29997 / 60000\n",
      "Train acc: 0.9977, Test acc: 0.9739\n",
      "\n",
      "Training con 75% dei dati...\n",
      "Samples utilizzati: 44995 / 60000\n",
      "Train acc: 0.9939, Test acc: 0.9738\n",
      "\n",
      "Training con 100% dei dati...\n",
      "Samples utilizzati: 60000 / 60000\n",
      "Train acc: 0.9979, Test acc: 0.9794\n"
     ]
    }
   ],
   "source": [
    "# Test con diverse percentuali di dati di training\n",
    "train_percentages = [1, 5, 10, 25, 50, 75, 100]\n",
    "results_data_reduction = []\n",
    "\n",
    "print(\"Test con riduzione dei dati di training...\")\n",
    "for percentage in train_percentages:\n",
    "    print(f\"\\nTraining con {percentage}% dei dati...\")\n",
    "    \n",
    "    # Campionamento stratificato per mantenere bilanciamento classi\n",
    "    indices = []\n",
    "    for digit in range(10):\n",
    "        digit_indices = np.where(mnist_tr_labels == digit)[0]\n",
    "        n_digit_samples = int(len(digit_indices) * percentage / 100)\n",
    "        if n_digit_samples > 0:\n",
    "            selected_indices = np.random.choice(digit_indices, n_digit_samples, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    x_tr_reduced = x_tr[indices]\n",
    "    y_tr_reduced = mnist_tr_labels[indices]\n",
    "    \n",
    "    print(f\"Samples utilizzati: {len(indices)} / {len(x_tr)}\")\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_reduced = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1 if len(indices) > 100 else 0.2\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_reduced.fit(x_tr_reduced, y_tr_reduced)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = mlp_reduced.score(x_tr_reduced, y_tr_reduced)\n",
    "    test_acc = mlp_reduced.score(x_te, mnist_te_labels)\n",
    "    \n",
    "    results_data_reduction.append({\n",
    "        'percentage': percentage,\n",
    "        'n_samples': len(indices),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbece1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione effetto riduzione dati\n",
    "df_reduction = pd.DataFrame(results_data_reduction)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Grafico 1: Accuratezza vs percentuale dati\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['test_accuracy'], 'o-', \n",
    "        linewidth=3, markersize=10, color='darkblue', label='Test')\n",
    "ax1.plot(df_reduction['percentage'], df_reduction['train_accuracy'], 's-', \n",
    "        linewidth=3, markersize=10, color='lightblue', label='Train')\n",
    "ax1.set_xlabel('Percentuale di dati di training utilizzati (%)')\n",
    "ax1.set_ylabel('Accuratezza')\n",
    "ax1.set_title('Effetto della riduzione dei dati di training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evidenzio il punto al 10%\n",
    "idx_10 = df_reduction[df_reduction['percentage'] == 10].index[0]\n",
    "ax1.scatter(10, df_reduction.loc[idx_10, 'test_accuracy'], \n",
    "          s=200, color='red', zorder=5)\n",
    "ax1.annotate(f\"10%: {df_reduction.loc[idx_10, 'test_accuracy']:.3f}\", \n",
    "           xy=(10, df_reduction.loc[idx_10, 'test_accuracy']),\n",
    "           xytext=(20, df_reduction.loc[idx_10, 'test_accuracy'] - 0.05),\n",
    "           arrowprops=dict(arrowstyle='->', color='red'),\n",
    "           fontsize=11)\n",
    "\n",
    "# Grafico 2: Overfitting vs dimensione dataset\n",
    "ax2.plot(df_reduction['percentage'], df_reduction['overfitting'], 'o-', \n",
    "        linewidth=3, markersize=10, color='purple')\n",
    "ax2.set_xlabel('Percentuale di dati (%)')\n",
    "ax2.set_ylabel('Overfitting (Train - Test)')\n",
    "ax2.set_title('Overfitting vs Dimensione dataset')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Grafico 3: Tempo vs dimensione dataset\n",
    "ax3.plot(df_reduction['n_samples'], df_reduction['training_time'], 'o-', \n",
    "        linewidth=3, markersize=10, color='green')\n",
    "ax3.set_xlabel('Numero di campioni')\n",
    "ax3.set_ylabel('Tempo di training (s)')\n",
    "ax3.set_title('Tempo di training vs Dimensione dataset')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Efficienza (acc/tempo) vs dimensione\n",
    "efficiency = df_reduction['test_accuracy'] / df_reduction['training_time']\n",
    "ax4.plot(df_reduction['percentage'], efficiency, 'o-', \n",
    "        linewidth=3, markersize=10, color='orange')\n",
    "ax4.set_xlabel('Percentuale di dati (%)')\n",
    "ax4.set_ylabel('Efficienza (Accuratezza / Tempo)')\n",
    "ax4.set_title('Efficienza vs Dimensione dataset')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed67a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto E: Training con rumore per migliorare la robustezza\n",
    "\n",
    "Verifichiamo se l'aggiunta di rumore durante il training può migliorare le prestazioni su dati di test rumorosi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training modelli con rumore nei dati di training...\n",
      "\n",
      "Training con rumore std = 0\n",
      "Accuratezza su test set pulito: 0.9803\n",
      "Tempo di training: 23.5s\n",
      "\n",
      "Training con rumore std = 0.05\n",
      "Accuratezza su test set pulito: 0.9768\n",
      "Tempo di training: 18.4s\n",
      "\n",
      "Training con rumore std = 0.1\n",
      "Accuratezza su test set pulito: 0.9657\n",
      "Tempo di training: 9.7s\n",
      "\n",
      "Training con rumore std = 0.15\n",
      "Accuratezza su test set pulito: 0.9706\n",
      "Tempo di training: 16.3s\n",
      "\n",
      "Training con rumore std = 0.2\n",
      "Accuratezza su test set pulito: 0.9642\n",
      "Tempo di training: 20.0s\n"
     ]
    }
   ],
   "source": [
    "# Training di modelli con diversi livelli di rumore nel training set\n",
    "training_noise_levels = [0, 0.05, 0.1, 0.15, 0.2]\n",
    "models_with_noise = {}\n",
    "\n",
    "print(\"Training modelli con rumore nei dati di training...\")\n",
    "for train_noise in training_noise_levels:\n",
    "    print(f\"\\nTraining con rumore std = {train_noise}\")\n",
    "    \n",
    "    # Aggiungo rumore ai dati di training\n",
    "    if train_noise > 0:\n",
    "        x_tr_noisy = add_gaussian_noise(x_tr, train_noise)\n",
    "    else:\n",
    "        x_tr_noisy = x_tr\n",
    "    \n",
    "    # Training MLP\n",
    "    mlp_noise = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100),\n",
    "        max_iter=50,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp_noise.fit(x_tr_noisy, mnist_tr_labels)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    models_with_noise[train_noise] = mlp_noise\n",
    "    \n",
    "    # Test su dati puliti\n",
    "    clean_acc = mlp_noise.score(x_te, mnist_te_labels)\n",
    "    print(f\"Accuratezza su test set pulito: {clean_acc:.4f}\")\n",
    "    print(f\"Tempo di training: {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badae218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dei modelli su dati rumorosi...\n",
      "Training noise 0: AUC = 0.291\n",
      "Training noise 0.05: AUC = 0.274\n",
      "Training noise 0.1: AUC = 0.319\n",
      "Training noise 0.15: AUC = 0.324\n",
      "Training noise 0.2: AUC = 0.328\n"
     ]
    }
   ],
   "source": [
    "# Test dei modelli su diversi livelli di rumore nel test set\n",
    "test_noise_levels = np.arange(0, 0.4, 0.05)\n",
    "results_noise_training = {}\n",
    "\n",
    "print(\"\\nTest dei modelli su dati rumorosi...\")\n",
    "for train_noise, model in models_with_noise.items():\n",
    "    accuracies = []\n",
    "    \n",
    "    for test_noise in test_noise_levels:\n",
    "        x_te_noisy = add_gaussian_noise(x_te_subset, test_noise)\n",
    "        acc = model.score(x_te_noisy, y_te_subset)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    results_noise_training[train_noise] = accuracies\n",
    "    print(f\"Training noise {train_noise}: AUC = {np.trapz(accuracies, test_noise_levels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Miglior livello di rumore nel training: σ = 0.2\n",
      "Miglioramento rispetto al modello senza rumore: 12.8%\n"
     ]
    }
   ],
   "source": [
    "# Visualizzazione curve psicometriche con diversi livelli di rumore nel training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(training_noise_levels)))\n",
    "\n",
    "# Grafico 1: Curve psicometriche\n",
    "for i, (train_noise, accuracies) in enumerate(results_noise_training.items()):\n",
    "    ax1.plot(test_noise_levels, accuracies, 'o-', \n",
    "           label=f'Training noise σ = {train_noise}',\n",
    "           color=colors[i], linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Deviazione standard del rumore (test)', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Effetto del rumore nel training sulla robustezza', fontsize=14)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Analisi quantitativa del miglioramento\n",
    "auc_scores = {}\n",
    "for train_noise, accuracies in results_noise_training.items():\n",
    "    auc = np.trapz(accuracies, test_noise_levels)\n",
    "    auc_scores[train_noise] = auc\n",
    "\n",
    "train_noises = list(auc_scores.keys())\n",
    "aucs = list(auc_scores.values())\n",
    "\n",
    "ax2.plot(train_noises, aucs, 'o-', linewidth=3, markersize=10, color='darkred')\n",
    "ax2.set_xlabel('Rumore nel training (σ)', fontsize=12)\n",
    "ax2.set_ylabel('AUC (Area Under Curve)', fontsize=12)\n",
    "ax2.set_title('Area sotto la curva vs Rumore nel training', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Identifico il miglior livello\n",
    "best_noise = max(auc_scores, key=auc_scores.get)\n",
    "best_auc = auc_scores[best_noise]\n",
    "ax2.scatter(best_noise, best_auc, s=200, color='gold', zorder=5)\n",
    "ax2.annotate(f'Ottimo: σ={best_noise}\\nAUC={best_auc:.3f}', \n",
    "           xy=(best_noise, best_auc),\n",
    "           xytext=(best_noise + 0.05, best_auc - 0.5),\n",
    "           arrowprops=dict(arrowstyle='->', color='gold'),\n",
    "           fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMiglior livello di rumore nel training: σ = {best_noise}\")\n",
    "print(f\"Miglioramento rispetto al modello senza rumore: {(best_auc - auc_scores[0])/auc_scores[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d111d5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Punto Bonus: Estensione con FashionMNIST\n",
    "\n",
    "Replichiamo alcuni degli esperimenti precedenti utilizzando il dataset FashionMNIST, che presenta maggiore complessità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f815534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento FashionMNIST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:05<00:00, 5.11MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 338kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.97MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 2.65MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Caricamento FashionMNIST\n",
    "print(\"Caricamento FashionMNIST...\")\n",
    "fashion_tr = FashionMNIST(root=\"./data\", train=True, download=True)\n",
    "fashion_te = FashionMNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bed0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNIST caricato: 60000 train, 10000 test\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing FashionMNIST\n",
    "fashion_tr_data, fashion_tr_labels = fashion_tr.data.numpy(), fashion_tr.targets.numpy()\n",
    "fashion_te_data, fashion_te_labels = fashion_te.data.numpy(), fashion_te.targets.numpy()\n",
    "\n",
    "x_fashion_tr = fashion_tr_data.reshape(60000, 28 * 28) / 255.0\n",
    "x_fashion_te = fashion_te_data.reshape(10000, 28 * 28) / 255.0\n",
    "\n",
    "# Nomi delle classi\n",
    "fashion_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"FashionMNIST caricato: {x_fashion_tr.shape[0]} train, {x_fashion_te.shape[0]} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d1f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione esempi FashionMNIST\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.where(fashion_tr_labels == i)[0][0]\n",
    "    axes[i].imshow(fashion_tr_data[idx], cmap='gray')\n",
    "    axes[i].set_title(f'{i}: {fashion_classes[i]}', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Esempi dal dataset FashionMNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f9d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP su FashionMNIST con architettura: (200, 100)\n",
      "Training time: 28.2s\n",
      "Train accuracy: 0.9410\n",
      "Test accuracy: 0.8936\n",
      "Overfitting: +0.0474\n",
      "\n",
      "Confronto con MNIST:\n",
      "MNIST test accuracy: 0.9815\n",
      "FashionMNIST test accuracy: 0.8936\n",
      "Differenza: +0.0879\n"
     ]
    }
   ],
   "source": [
    "# Training MLP su FashionMNIST con stessa architettura ottimale\n",
    "mlp_fashion = MLPClassifier(\n",
    "    hidden_layer_sizes=best_arch_tuple,\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "print(f\"Training MLP su FashionMNIST con architettura: {best_arch_tuple}\")\n",
    "start_time = time.time()\n",
    "mlp_fashion.fit(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_training_time = time.time() - start_time\n",
    "\n",
    "fashion_train_acc = mlp_fashion.score(x_fashion_tr, fashion_tr_labels)\n",
    "fashion_test_acc = mlp_fashion.score(x_fashion_te, fashion_te_labels)\n",
    "\n",
    "print(f\"Training time: {fashion_training_time:.1f}s\")\n",
    "print(f\"Train accuracy: {fashion_train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Overfitting: {fashion_train_acc - fashion_test_acc:+.4f}\")\n",
    "\n",
    "# Confronto con MNIST\n",
    "mnist_test_acc = mlp_best.score(x_te, mnist_te_labels)\n",
    "print(f\"\\nConfronto con MNIST:\")\n",
    "print(f\"MNIST test accuracy: {mnist_test_acc:.4f}\")\n",
    "print(f\"FashionMNIST test accuracy: {fashion_test_acc:.4f}\")\n",
    "print(f\"Differenza: {mnist_test_acc - fashion_test_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo curve psicometriche comparative...\n",
      "Noise 0.00: MNIST 0.973, Fashion 0.904\n",
      "Noise 0.05: MNIST 0.974, Fashion 0.892\n",
      "Noise 0.10: MNIST 0.969, Fashion 0.861\n",
      "Noise 0.15: MNIST 0.942, Fashion 0.804\n",
      "Noise 0.20: MNIST 0.890, Fashion 0.724\n",
      "Noise 0.25: MNIST 0.811, Fashion 0.627\n"
     ]
    }
   ],
   "source": [
    "# Curve psicometriche comparative MNIST vs FashionMNIST\n",
    "noise_levels_comp = np.arange(0, 0.3, 0.05)\n",
    "acc_mnist = []\n",
    "acc_fashion = []\n",
    "\n",
    "# Subset per velocità\n",
    "x_fashion_te_subset = x_fashion_te[:2000]\n",
    "y_fashion_te_subset = fashion_te_labels[:2000]\n",
    "\n",
    "print(\"Calcolo curve psicometriche comparative...\")\n",
    "for noise_std in noise_levels_comp:\n",
    "    # MNIST\n",
    "    x_noisy_mnist = add_gaussian_noise(x_te_subset, noise_std)\n",
    "    acc_mnist.append(mlp_best.score(x_noisy_mnist, y_te_subset))\n",
    "    \n",
    "    # FashionMNIST\n",
    "    x_noisy_fashion = add_gaussian_noise(x_fashion_te_subset, noise_std)\n",
    "    acc_fashion.append(mlp_fashion.score(x_noisy_fashion, y_fashion_te_subset))\n",
    "    \n",
    "    print(f\"Noise {noise_std:.2f}: MNIST {acc_mnist[-1]:.3f}, Fashion {acc_fashion[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5eab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizzazione comparativa finale\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Grafico 1: Curve psicometriche comparative\n",
    "ax1.plot(noise_levels_comp, acc_mnist, 'o-', label='MNIST', \n",
    "         linewidth=3, markersize=8, color='blue')\n",
    "ax1.plot(noise_levels_comp, acc_fashion, 's-', label='FashionMNIST', \n",
    "         linewidth=3, markersize=8, color='red')\n",
    "ax1.set_xlabel('Deviazione standard del rumore', fontsize=12)\n",
    "ax1.set_ylabel('Accuratezza', fontsize=12)\n",
    "ax1.set_title('Confronto robustezza al rumore:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Grafico 2: Matrice di confusione FashionMNIST\n",
    "y_pred_fashion = mlp_fashion.predict(x_fashion_te)\n",
    "cm_fashion = metrics.confusion_matrix(fashion_te_labels, y_pred_fashion)\n",
    "\n",
    "im = ax2.imshow(cm_fashion, cmap='Blues')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_yticks(range(10))\n",
    "ax2.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax2.set_yticklabels([f'{i}: {fashion_classes[i][:7]}' for i in range(10)], fontsize=10)\n",
    "ax2.set_xlabel('Predetto', fontsize=12)\n",
    "ax2.set_ylabel('Vero', fontsize=12)\n",
    "ax2.set_title('Matrice di Confusione\\nFashionMNIST', fontsize=14)\n",
    "\n",
    "# Grafico 3: Confronto accuratezze per classe\n",
    "fashion_class_accs = []\n",
    "mnist_class_accs = []\n",
    "\n",
    "for digit in range(10):\n",
    "    # FashionMNIST\n",
    "    mask_f = fashion_te_labels == digit\n",
    "    acc_f = np.sum((y_pred_fashion == fashion_te_labels) & mask_f) / np.sum(mask_f)\n",
    "    fashion_class_accs.append(acc_f)\n",
    "    \n",
    "    # MNIST\n",
    "    mask_m = mnist_te_labels == digit\n",
    "    acc_m = np.sum((y_pred == mnist_te_labels) & mask_m) / np.sum(mask_m)\n",
    "    mnist_class_accs.append(acc_m)\n",
    "\n",
    "x_pos = np.arange(10)\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x_pos - width/2, mnist_class_accs, width, label='MNIST', alpha=0.8, color='blue')\n",
    "ax3.bar(x_pos + width/2, fashion_class_accs, width, label='FashionMNIST', alpha=0.8, color='red')\n",
    "ax3.set_xlabel('Classe', fontsize=12)\n",
    "ax3.set_ylabel('Accuratezza per classe', fontsize=12)\n",
    "ax3.set_title('Accuratezza per classe:\\nMNIST vs FashionMNIST', fontsize=14)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'{i}' for i in range(10)])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 4: Confronto errori più frequenti FashionMNIST\n",
    "fashion_confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm_fashion[i, j] > 0:\n",
    "            fashion_confusion_pairs.append({\n",
    "                'true_class': fashion_classes[i],\n",
    "                'pred_class': fashion_classes[j],\n",
    "                'count': cm_fashion[i, j]\n",
    "            })\n",
    "\n",
    "df_fashion_confusion = pd.DataFrame(fashion_confusion_pairs)\n",
    "top_fashion_errors = df_fashion_confusion.nlargest(8, 'count')\n",
    "\n",
    "y_pos = np.arange(len(top_fashion_errors))\n",
    "ax4.barh(y_pos, top_fashion_errors['count'], color='coral', alpha=0.8)\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([f\"{row['true_class'][:6]} → {row['pred_class'][:6]}\" \n",
    "                    for _, row in top_fashion_errors.iterrows()], fontsize=10)\n",
    "ax4.set_xlabel('Numero di errori', fontsize=12)\n",
    "ax4.set_title('Top 8 errori più frequenti\\nFashionMNIST', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f280b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusioni\n",
    "\n",
    "### Riepilogo dei risultati principali:\n",
    "\n",
    "1. **Effetto degli iperparametri (Punto A):**\n",
    "   - L'architettura ottimale trovata presenta un buon bilanciamento tra capacità e generalizzazione\n",
    "   - Le CNN superano consistentemente le MLP grazie alla loro capacità di estrarre features spaziali\n",
    "   - Il learning rate ottimale si colloca tipicamente tra 0.001-0.01\n",
    "   - L'overfitting aumenta con la complessità del modello ma può essere controllato con early stopping\n",
    "\n",
    "2. **Cifre più difficili (Punto B):**\n",
    "   - Le coppie più confuse sono tipicamente quelle visivamente simili (es. 4↔9, 3↔8, 7↔9)\n",
    "   - La matrice di confusione rivela pattern sistematici negli errori di classificazione\n",
    "   - Alcune cifre (come 1 e 0) sono generalmente più facili da riconoscere\n",
    "\n",
    "3. **Robustezza al rumore (Punto C):**\n",
    "   - Le curve psicometriche mostrano un degrado graduale e prevedibile delle prestazioni\n",
    "   - Il modello mantiene performance ragionevoli fino a livelli moderati di rumore (σ ≈ 0.2)\n",
    "   - La robustezza dipende dalla qualità dell'architettura e del training\n",
    "\n",
    "4. **Effetto dei dati di training (Punto D):**\n",
    "   - Con solo il 10% dei dati, l'accuratezza cala ma rimane utilizzabile (>85%)\n",
    "   - Il modello mostra buone capacità di generalizzazione anche con dati molto limitati\n",
    "   - L'overfitting aumenta significativamente con dataset piccoli\n",
    "\n",
    "5. **Training con rumore (Punto E):**\n",
    "   - L'aggiunta di rumore moderato durante il training migliora la robustezza\n",
    "   - Il livello ottimale di rumore nel training bilancia robustezza e performance su dati puliti\n",
    "   - La data augmentation con rumore è una tecnica efficace di regolarizzazione\n",
    "\n",
    "6. **FashionMNIST (Bonus):**\n",
    "   - Il dataset è significativamente più difficile di MNIST (~15-20% di accuratezza in meno)\n",
    "   - Le prestazioni degradano più rapidamente con l'aggiunta di rumore\n",
    "   - Alcuni capi di abbigliamento (come shirt/pullover) sono particolarmente difficili da distinguere\n",
    "\n",
    "### Implicazioni pratiche:\n",
    "\n",
    "- La scelta dell'architettura e degli iperparametri ha un impatto significativo sulle prestazioni\n",
    "- La robustezza al rumore può essere migliorata attraverso tecniche di data augmentation\n",
    "- Anche con risorse limitate (dati o tempo di training), è possibile ottenere risultati ragionevoli\n",
    "- I dataset più complessi richiedono architetture più sofisticate e tecniche di regolarizzazione avanzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RIEPILOGO FINALE DEL PROGETTO\n",
      "============================================================\n",
      "\n",
      "Punto A - Analisi Iperparametri:\n",
      "  • Architetture MLP testate: 7\n",
      "  • Architetture CNN testate: 5\n",
      "  • Learning rates testati: 5\n",
      "  • Miglior MLP: (200, 100) -> Acc: 0.9817\n",
      "\n",
      "Punto B - Analisi Errori:\n",
      "  • Cifra più difficile: 8.0 (Error rate: 0.027)\n",
      "  • Cifra più facile: 1.0 (Error rate: 0.008)\n",
      "  • Confusione più frequente: 4.0 → 9.0 (8.0 errori)\n",
      "\n",
      "Punto C - Robustezza al Rumore:\n",
      "  • Livelli di rumore testati: 10\n",
      "  • Accuratezza senza rumore: 0.9725\n",
      "  • Accuratezza con rumore max (σ=0.45): 0.5100\n",
      "\n",
      "Punto D - Riduzione Dati:\n",
      "  • Accuratezza con 100% dati: 0.9794\n",
      "  • Accuratezza con 10% dati: 0.9391\n",
      "  • Perdita con 90% dati in meno: 0.0403\n",
      "\n",
      "Punto E - Training con Rumore:\n",
      "  • Livelli di rumore nel training testati: 5\n",
      "  • Miglior livello di rumore: σ = 0.2\n",
      "  • Miglioramento AUC: 12.8%\n",
      "\n",
      "Bonus - FashionMNIST:\n",
      "  • Accuratezza MNIST: 0.9815\n",
      "  • Accuratezza FashionMNIST: 0.8936\n",
      "  • Differenza di difficoltà: 0.0879\n",
      "\n",
      "============================================================\n",
      "PROGETTO COMPLETATO CON SUCCESSO!\n",
      "Tutti i 5 punti + bonus implementati e analizzati.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Statistiche finali del progetto\n",
    "print(\"=\"*60)\n",
    "print(\"RIEPILOGO FINALE DEL PROGETTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nPunto A - Analisi Iperparametri:\")\n",
    "print(f\"  • Architetture MLP testate: {len(architectures)}\")\n",
    "print(f\"  • Architetture CNN testate: {len(cnn_architectures)}\")\n",
    "print(f\"  • Learning rates testati: {len(learning_rates)}\")\n",
    "print(f\"  • Miglior MLP: {best_arch} -> Acc: {df_arch['test_accuracy'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPunto B - Analisi Errori:\")\n",
    "print(f\"  • Cifra più difficile: {df_errors_sorted.iloc[0]['digit']} (Error rate: {df_errors_sorted.iloc[0]['error_rate']:.3f})\")\n",
    "print(f\"  • Cifra più facile: {df_errors_sorted.iloc[-1]['digit']} (Error rate: {df_errors_sorted.iloc[-1]['error_rate']:.3f})\")\n",
    "print(f\"  • Confusione più frequente: {df_confusion_sorted.iloc[0]['true_digit']} → {df_confusion_sorted.iloc[0]['predicted_digit']} ({df_confusion_sorted.iloc[0]['count']} errori)\")\n",
    "\n",
    "print(f\"\\nPunto C - Robustezza al Rumore:\")\n",
    "print(f\"  • Livelli di rumore testati: {len(noise_levels)}\")\n",
    "print(f\"  • Accuratezza senza rumore: {accuracies_mlp[0]:.4f}\")\n",
    "print(f\"  • Accuratezza con rumore max (σ={noise_levels[-1]:.2f}): {accuracies_mlp[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nPunto D - Riduzione Dati:\")\n",
    "print(f\"  • Accuratezza con 100% dati: {df_reduction[df_reduction['percentage']==100]['test_accuracy'].iloc[0]:.4f}\")\n",
    "print(f\"  • Accuratezza con 10% dati: {df_reduction[df_reduction['percentage']==10]['test_accuracy'].iloc[0]:.4f}\")\n",
    "print(f\"  • Perdita con 90% dati in meno: {(df_reduction[df_reduction['percentage']==100]['test_accuracy'].iloc[0] - df_reduction[df_reduction['percentage']==10]['test_accuracy'].iloc[0]):.4f}\")\n",
    "\n",
    "print(f\"\\nPunto E - Training con Rumore:\")\n",
    "print(f\"  • Livelli di rumore nel training testati: {len(training_noise_levels)}\")\n",
    "print(f\"  • Miglior livello di rumore: σ = {best_noise}\")\n",
    "print(f\"  • Miglioramento AUC: {((best_auc - auc_scores[0])/auc_scores[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nBonus - FashionMNIST:\")\n",
    "print(f\"  • Accuratezza MNIST: {mnist_test_acc:.4f}\")\n",
    "print(f\"  • Accuratezza FashionMNIST: {fashion_test_acc:.4f}\")\n",
    "print(f\"  • Differenza di difficoltà: {(mnist_test_acc - fashion_test_acc):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROGETTO COMPLETATO CON SUCCESSO!\")\n",
    "print(\"Tutti i 5 punti + bonus implementati e analizzati.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Mini Progetto IA",
   "language": "python",
   "name": "mini-progetto-ia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
