{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77f4b15",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# INTELLIGENZA ARTIFICIALE - Mini-Progetto Individuale\n",
    "**Prof. Marco Zorzi, Dr. Alberto Testolin**\n",
    "\n",
    "**Nome**: [INSERIRE NOME]  \n",
    "**Cognome**: [INSERIRE COGNOME]  \n",
    "**Matricola**: [INSERIRE MATRICOLA]  \n",
    "**Data**: [INSERIRE DATA]\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo del Progetto\n",
    "Implementare alcune simulazioni per studiare il riconoscimento di cifre manoscritte, \n",
    "analizzando l'effetto di architetture e iper-parametri diversi sui modelli MLP e CNN.\n",
    "\n",
    "Le simulazioni si baseranno sul dataset MNIST, seguendo rigorosamente l'approccio \n",
    "metodologico utilizzato nei laboratori del corso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe329e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completato\n"
     ]
    }
   ],
   "source": [
    "# Setup delle librerie\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow per CNN\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras  \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# PyTorch per dataset loading (seguendo Lab 3)\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Configurazione per riproducibilità\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup completato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dae275",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# PUNTO A: Analisi Architetturale [2 punti]\n",
    "\n",
    "**Obiettivo**: Analizzare sistematicamente come cambia la prestazione dei modelli \n",
    "(MLP e CNN) al variare del numero di neuroni, strati nascosti e altri iper-parametri \n",
    "significativi.\n",
    "\n",
    "## Metodologia Sistematica\n",
    "\n",
    "Seguendo l'approccio rigoroso dei laboratori, implementeremo:\n",
    "\n",
    "**Per MLP (36 esperimenti):**\n",
    "- Neuroni per strato: 50, 100, 250\n",
    "- Numero strati: 1 vs 2 strati nascosti  \n",
    "- Solver: SGD vs Adam\n",
    "- Learning rate: 0.001, 0.01, 0.1\n",
    "\n",
    "**Per CNN (24 esperimenti):**\n",
    "- Filtri: 16, 32, 64\n",
    "- Architettura: baseline vs extended\n",
    "- Learning rate: 0.001, 0.01\n",
    "- Optimizer: Adam vs SGD\n",
    "\n",
    "**Totale: 60 esperimenti sistematici**\n",
    "\n",
    "L'approccio garantisce confronto equo e bilanciato tra le architetture, \n",
    "estendendo la metodologia del Lab 2 anche alle CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175256c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset MNIST...\n",
      "Dataset caricato: 60000 train, 10000 test\n",
      "Preprocessing dati...\n",
      "Preprocessing completato:\n",
      "  MLP: (60000, 784) -> (10000, 784)\n",
      "  CNN: (60000, 28, 28, 1) -> (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Caricamento e preprocessing MNIST - Seguendo esattamente Lab 3\n",
    "print(\"Caricamento dataset MNIST...\")\n",
    "\n",
    "# Caricamento tramite TorchVision (metodo Lab 3)\n",
    "mnist_tr = MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_te = MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Conversione a numpy arrays\n",
    "x_train = mnist_tr.data.numpy()\n",
    "y_train = mnist_tr.targets.numpy()\n",
    "x_test = mnist_te.data.numpy()\n",
    "y_test = mnist_te.targets.numpy()\n",
    "\n",
    "print(f\"Dataset caricato: {x_train.shape[0]} train, {x_test.shape[0]} test\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing dati...\")\n",
    "\n",
    "# Per MLP: flattening + normalizzazione [0,1]\n",
    "x_train_mlp = x_train.reshape(x_train.shape[0], -1).astype(np.float32) / 255.0\n",
    "x_test_mlp = x_test.reshape(x_test.shape[0], -1).astype(np.float32) / 255.0\n",
    "\n",
    "# Per CNN: formato 4D + normalizzazione [0,1]  \n",
    "x_train_cnn = x_train.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "x_test_cnn = x_test.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "\n",
    "print(f\"Preprocessing completato:\")\n",
    "print(f\"  MLP: {x_train_mlp.shape} -> {x_test_mlp.shape}\")\n",
    "print(f\"  CNN: {x_train_cnn.shape} -> {x_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione esempi del dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "fig.suptitle('Dataset MNIST - Esempi per Cifra', fontsize=14, fontweight='bold')\n",
    "\n",
    "for digit in range(10):\n",
    "    # Trova primo esempio di ogni cifra\n",
    "    idx = np.where(y_train == digit)[0][0]\n",
    "    \n",
    "    ax = axes[digit//5, digit%5]\n",
    "    ax.imshow(x_train[idx], cmap='gray')\n",
    "    ax.set_title(f'Cifra {digit}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche dataset\n",
    "print(f\"\\nStatistiche Dataset:\")\n",
    "print(f\"Forma immagini: {x_train.shape[1:]} pixels\")\n",
    "print(f\"Range valori: [{x_train.min()}, {x_train.max()}]\") \n",
    "print(f\"Classi: {len(np.unique(y_train))} cifre\")\n",
    "print(f\"Distribuzione classi (train):\")\n",
    "\n",
    "class_counts = np.bincount(y_train)\n",
    "for digit, count in enumerate(class_counts):\n",
    "    print(f\"  Cifra {digit}: {count:5d} esempi ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242b1b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configurazione Esperimenti MLP\n",
    "\n",
    "Implementazione sistematica seguendo metodologia Lab 2, con estensione \n",
    "a parametri aggiuntivi per analisi completa.\n",
    "\n",
    "**Razionale delle scelte:**\n",
    "- **Neuroni**: 50, 100, 250 → range piccolo/medio/grande\n",
    "- **Strati**: 1 vs 2 → analisi profondità vs overfitting  \n",
    "- **Solver**: SGD vs Adam → confronto algoritmi ottimizzazione\n",
    "- **Learning Rate**: 0.001, 0.01, 0.1 → ordini di grandezza diversi\n",
    "\n",
    "**Parametri fissi ottimizzati:**\n",
    "- Early stopping per efficienza e prevenzione overfitting\n",
    "- Validation split 10% per monitoring\n",
    "- Tolerance 0.001 (standard sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924122e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURAZIONE ESPERIMENTI MLP ===\n",
      "Configurazioni MLP generate: 36\n",
      "Struttura: 3 neuroni × 2 strati × 2 solver × 3 LR\n",
      "\n",
      "Prime 3 configurazioni:\n",
      "  1. MLP_50n_1l_sgd_lr0.001\n",
      "     Architettura: (50,)\n",
      "     Solver: sgd, LR: 0.001\n",
      "  2. MLP_50n_1l_sgd_lr0.01\n",
      "     Architettura: (50,)\n",
      "     Solver: sgd, LR: 0.01\n",
      "  3. MLP_50n_1l_sgd_lr0.1\n",
      "     Architettura: (50,)\n",
      "     Solver: sgd, LR: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Configurazione esperimenti MLP\n",
    "print(\"=== CONFIGURAZIONE ESPERIMENTI MLP ===\")\n",
    "\n",
    "mlp_configs = []\n",
    "\n",
    "# Parametri da testare sistematicamente\n",
    "neurons_options = [50, 100, 250]          # 3 opzioni\n",
    "layers_options = [1, 2]                   # 2 opzioni  \n",
    "solvers = ['sgd', 'adam']                 # 2 opzioni\n",
    "learning_rates = [0.001, 0.01, 0.1]      # 3 opzioni\n",
    "\n",
    "# Generazione configurazioni (3×2×2×3 = 36 esperimenti)\n",
    "for neurons in neurons_options:\n",
    "    for n_layers in layers_options:\n",
    "        for solver in solvers:\n",
    "            for lr in learning_rates:\n",
    "                \n",
    "                # Definizione architettura nascosta\n",
    "                if n_layers == 1:\n",
    "                    hidden_layers = (neurons,)\n",
    "                else:  # n_layers == 2\n",
    "                    hidden_layers = (neurons, neurons)\n",
    "                \n",
    "                # Configurazione completa\n",
    "                config = {\n",
    "                    'name': f'MLP_{neurons}n_{n_layers}l_{solver}_lr{lr}',\n",
    "                    'hidden_layer_sizes': hidden_layers,\n",
    "                    'solver': solver,\n",
    "                    'learning_rate_init': lr,\n",
    "                    'max_iter': 200,\n",
    "                    'early_stopping': True,\n",
    "                    'validation_fraction': 0.1,\n",
    "                    'n_iter_no_change': 15,  # Patience generosa\n",
    "                    'tol': 0.001,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                mlp_configs.append(config)\n",
    "\n",
    "print(f\"Configurazioni MLP generate: {len(mlp_configs)}\")\n",
    "print(f\"Struttura: {len(neurons_options)} neuroni × {len(layers_options)} strati × {len(solvers)} solver × {len(learning_rates)} LR\")\n",
    "\n",
    "# Anteprima configurazioni\n",
    "print(f\"\\nPrime 3 configurazioni:\")\n",
    "for i, config in enumerate(mlp_configs[:3]):\n",
    "    print(f\"  {i+1}. {config['name']}\")\n",
    "    print(f\"     Architettura: {config['hidden_layer_sizes']}\")\n",
    "    print(f\"     Solver: {config['solver']}, LR: {config['learning_rate_init']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25ef4c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Esecuzione Esperimenti MLP\n",
    "\n",
    "Training sistematico di tutte le 36 configurazioni con monitoring \n",
    "delle performance e visualizzazione dei risultati.\n",
    "\n",
    "**Metriche monitorate:**\n",
    "- Accuratezza training e test\n",
    "- Tempo di training\n",
    "- Numero iterazioni per convergenza  \n",
    "- Curve di loss (quando disponibili)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b30d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESECUZIONE ESPERIMENTI MLP ===\n",
      "Avvio training di 36 configurazioni...\n",
      "\n",
      "[ 1/36] Training MLP_50n_1l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9528 | Train Acc: 0.9554 | Overfitting: +0.0026\n",
      "     Tempo:  13.2s | Iterazioni:  93 | Converged: ✓\n",
      "\n",
      "[ 2/36] Training MLP_50n_1l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9726 | Train Acc: 0.9868 | Overfitting: +0.0142\n",
      "     Tempo:   6.9s | Iterazioni:  50 | Converged: ✓\n",
      "\n",
      "[ 3/36] Training MLP_50n_1l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9729 | Train Acc: 0.9951 | Overfitting: +0.0222\n",
      "     Tempo:   5.5s | Iterazioni:  36 | Converged: ✓\n",
      "\n",
      "[ 4/36] Training MLP_50n_1l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9697 | Train Acc: 0.9893 | Overfitting: +0.0196\n",
      "     Tempo:   7.5s | Iterazioni:  39 | Converged: ✓\n",
      "\n",
      "[ 5/36] Training MLP_50n_1l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9688 | Train Acc: 0.9901 | Overfitting: +0.0213\n",
      "     Tempo:   4.1s | Iterazioni:  22 | Converged: ✓\n",
      "\n",
      "[ 6/36] Training MLP_50n_1l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.9197 | Train Acc: 0.9242 | Overfitting: +0.0045\n",
      "     Tempo:   3.9s | Iterazioni:  22 | Converged: ✓\n",
      "\n",
      "[ 7/36] Training MLP_50n_2l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9657 | Train Acc: 0.9748 | Overfitting: +0.0091\n",
      "     Tempo:  18.6s | Iterazioni: 113 | Converged: ✓\n",
      "\n",
      "[ 8/36] Training MLP_50n_2l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9722 | Train Acc: 0.9917 | Overfitting: +0.0195\n",
      "     Tempo:   7.4s | Iterazioni:  41 | Converged: ✓\n",
      "\n",
      "[ 9/36] Training MLP_50n_2l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9761 | Train Acc: 0.9891 | Overfitting: +0.0130\n",
      "     Tempo:   4.8s | Iterazioni:  26 | Converged: ✓\n",
      "\n",
      "[10/36] Training MLP_50n_2l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9715 | Train Acc: 0.9934 | Overfitting: +0.0219\n",
      "     Tempo:   7.9s | Iterazioni:  32 | Converged: ✓\n",
      "\n",
      "[11/36] Training MLP_50n_2l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9695 | Train Acc: 0.9902 | Overfitting: +0.0207\n",
      "     Tempo:  10.0s | Iterazioni:  45 | Converged: ✓\n",
      "\n",
      "[12/36] Training MLP_50n_2l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.8385 | Train Acc: 0.8445 | Overfitting: +0.0060\n",
      "     Tempo:   5.1s | Iterazioni:  24 | Converged: ✓\n",
      "\n",
      "[13/36] Training MLP_100n_1l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9472 | Train Acc: 0.9495 | Overfitting: +0.0023\n",
      "     Tempo:  13.4s | Iterazioni:  70 | Converged: ✓\n",
      "\n",
      "[14/36] Training MLP_100n_1l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9752 | Train Acc: 0.9914 | Overfitting: +0.0162\n",
      "     Tempo:  10.6s | Iterazioni:  55 | Converged: ✓\n",
      "\n",
      "[15/36] Training MLP_100n_1l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9788 | Train Acc: 0.9971 | Overfitting: +0.0183\n",
      "     Tempo:   6.3s | Iterazioni:  32 | Converged: ✓\n",
      "\n",
      "[16/36] Training MLP_100n_1l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9772 | Train Acc: 0.9975 | Overfitting: +0.0203\n",
      "     Tempo:  10.1s | Iterazioni:  32 | Converged: ✓\n",
      "\n",
      "[17/36] Training MLP_100n_1l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9782 | Train Acc: 0.9946 | Overfitting: +0.0164\n",
      "     Tempo:  16.0s | Iterazioni:  54 | Converged: ✓\n",
      "\n",
      "[18/36] Training MLP_100n_1l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.9179 | Train Acc: 0.9183 | Overfitting: +0.0004\n",
      "     Tempo:   5.7s | Iterazioni:  20 | Converged: ✓\n",
      "\n",
      "[19/36] Training MLP_100n_2l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9602 | Train Acc: 0.9645 | Overfitting: +0.0043\n",
      "     Tempo:  15.3s | Iterazioni:  57 | Converged: ✓\n",
      "\n",
      "[20/36] Training MLP_100n_2l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9758 | Train Acc: 0.9921 | Overfitting: +0.0163\n",
      "     Tempo:   9.8s | Iterazioni:  33 | Converged: ✓\n",
      "\n",
      "[21/36] Training MLP_100n_2l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9802 | Train Acc: 0.9980 | Overfitting: +0.0178\n",
      "     Tempo:   7.6s | Iterazioni:  27 | Converged: ✓\n",
      "\n",
      "[22/36] Training MLP_100n_2l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9802 | Train Acc: 0.9978 | Overfitting: +0.0176\n",
      "     Tempo:  18.0s | Iterazioni:  45 | Converged: ✓\n",
      "\n",
      "[23/36] Training MLP_100n_2l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9748 | Train Acc: 0.9906 | Overfitting: +0.0158\n",
      "     Tempo:  14.9s | Iterazioni:  40 | Converged: ✓\n",
      "\n",
      "[24/36] Training MLP_100n_2l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.8966 | Train Acc: 0.8978 | Overfitting: +0.0012\n",
      "     Tempo:   7.0s | Iterazioni:  19 | Converged: ✓\n",
      "\n",
      "[25/36] Training MLP_250n_1l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9528 | Train Acc: 0.9555 | Overfitting: +0.0027\n",
      "     Tempo:  27.8s | Iterazioni:  65 | Converged: ✓\n",
      "\n",
      "[26/36] Training MLP_250n_1l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9780 | Train Acc: 0.9881 | Overfitting: +0.0101\n",
      "     Tempo:  14.5s | Iterazioni:  36 | Converged: ✓\n",
      "\n",
      "[27/36] Training MLP_250n_1l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9818 | Train Acc: 0.9982 | Overfitting: +0.0164\n",
      "     Tempo:  11.2s | Iterazioni:  26 | Converged: ✓\n",
      "\n",
      "[28/36] Training MLP_250n_1l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9791 | Train Acc: 0.9980 | Overfitting: +0.0189\n",
      "     Tempo:  21.1s | Iterazioni:  27 | Converged: ✓\n",
      "\n",
      "[29/36] Training MLP_250n_1l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9770 | Train Acc: 0.9941 | Overfitting: +0.0171\n",
      "     Tempo:  22.2s | Iterazioni:  34 | Converged: ✓\n",
      "\n",
      "[30/36] Training MLP_250n_1l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.9097 | Train Acc: 0.9149 | Overfitting: +0.0052\n",
      "     Tempo:  11.8s | Iterazioni:  18 | Converged: ✓\n",
      "\n",
      "[31/36] Training MLP_250n_2l_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9678 | Train Acc: 0.9759 | Overfitting: +0.0081\n",
      "     Tempo:  44.8s | Iterazioni:  75 | Converged: ✓\n",
      "\n",
      "[32/36] Training MLP_250n_2l_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9774 | Train Acc: 0.9970 | Overfitting: +0.0196\n",
      "     Tempo:  21.9s | Iterazioni:  36 | Converged: ✓\n",
      "\n",
      "[33/36] Training MLP_250n_2l_sgd_lr0.1...\n",
      "  ✅ Test Acc: 0.9816 | Train Acc: 0.9981 | Overfitting: +0.0165\n",
      "     Tempo:  21.0s | Iterazioni:  30 | Converged: ✓\n",
      "\n",
      "[34/36] Training MLP_250n_2l_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9790 | Train Acc: 0.9972 | Overfitting: +0.0182\n",
      "     Tempo:  23.8s | Iterazioni:  24 | Converged: ✓\n",
      "\n",
      "[35/36] Training MLP_250n_2l_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9748 | Train Acc: 0.9938 | Overfitting: +0.0190\n",
      "     Tempo:  37.8s | Iterazioni:  41 | Converged: ✓\n",
      "\n",
      "[36/36] Training MLP_250n_2l_adam_lr0.1...\n",
      "  ✅ Test Acc: 0.4746 | Train Acc: 0.4770 | Overfitting: +0.0024\n",
      "     Tempo:  23.7s | Iterazioni:  29 | Converged: ✓\n",
      "\n",
      "✅ MLP Esperimenti completati!\n",
      "   Tempo totale: 511.2s (8.5min)\n",
      "   Modelli: 36/36\n",
      "   Tempo medio per esperimento: 14.2s\n"
     ]
    }
   ],
   "source": [
    "# Esecuzione esperimenti MLP\n",
    "print(\"=== ESECUZIONE ESPERIMENTI MLP ===\")\n",
    "print(f\"Avvio training di {len(mlp_configs)} configurazioni...\")\n",
    "\n",
    "mlp_results = []\n",
    "start_total = time.time()\n",
    "\n",
    "for i, config in enumerate(mlp_configs):\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n[{i+1:2d}/{len(mlp_configs)}] Training {config_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creazione modello con parametri dalla configurazione\n",
    "    model_params = {k: v for k, v in config.items() if k != 'name'}\n",
    "    mlp = MLPClassifier(**model_params)\n",
    "    \n",
    "    # Training\n",
    "    mlp.fit(x_train_mlp, y_train)\n",
    "    \n",
    "    # Valutazione performance\n",
    "    train_acc = mlp.score(x_train_mlp, y_train)\n",
    "    test_acc = mlp.score(x_test_mlp, y_test) \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Raccolta risultati\n",
    "    result = {\n",
    "        'name': config_name,\n",
    "        'model': mlp,\n",
    "        'config': config,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'n_iterations': mlp.n_iter_,\n",
    "        'converged': mlp.n_iter_ < config['max_iter'],\n",
    "        'loss_curve': mlp.loss_curve_ if hasattr(mlp, 'loss_curve_') else None\n",
    "    }\n",
    "    \n",
    "    mlp_results.append(result)\n",
    "    \n",
    "    # Progress report\n",
    "    print(f\"  ✅ Test Acc: {test_acc:.4f} | Train Acc: {train_acc:.4f} | Overfitting: {train_acc-test_acc:+.4f}\")\n",
    "    print(f\"     Tempo: {training_time:5.1f}s | Iterazioni: {mlp.n_iter_:3d} | Converged: {'✓' if result['converged'] else '✗'}\")\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\n✅ MLP Esperimenti completati!\")\n",
    "print(f\"   Tempo totale: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"   Modelli: {len(mlp_results)}/{len(mlp_configs)}\")\n",
    "print(f\"   Tempo medio per esperimento: {total_time/len(mlp_configs):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5f62c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configurazione Esperimenti CNN\n",
    "\n",
    "Estensione dell'approccio sistematico alle architetture convoluzionali,\n",
    "seguendo la metodologia del Lab 3 con variazioni strutturate.\n",
    "\n",
    "**Razionale delle scelte:**\n",
    "- **Filtri**: 16, 32, 64 → da sotto-baseline a over-baseline Lab 3\n",
    "- **Architettura**: baseline (Lab 3) vs extended (più profonda)\n",
    "- **Learning Rate**: 0.001, 0.01 → range conservativo per CNN\n",
    "- **Optimizer**: Adam vs SGD → stessa logica MLP per confrontabilità\n",
    "\n",
    "**Architetture definite:**\n",
    "- **Baseline**: Conv2D + Flatten + Dense (replica Lab 3)\n",
    "- **Extended**: Conv2D + MaxPooling + Conv2D + Flatten + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURAZIONE ESPERIMENTI CNN ===\n",
      "Configurazioni CNN generate: 24\n",
      "Struttura: 3 filtri × 2 arch × 2 opt × 2 LR\n",
      "\n",
      "Architettura baseline di test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21632</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,081,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21632\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │     \u001b[38;5;34m1,081,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,082,480</span> (4.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,082,480\u001b[0m (4.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,082,480</span> (4.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,082,480\u001b[0m (4.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prime 3 configurazioni CNN:\n",
      "  1. CNN_16f_baseline_adam_lr0.001\n",
      "     Filtri: 16, Arch: baseline\n",
      "     Optimizer: adam, LR: 0.001\n",
      "  2. CNN_16f_baseline_sgd_lr0.001\n",
      "     Filtri: 16, Arch: baseline\n",
      "     Optimizer: sgd, LR: 0.001\n",
      "  3. CNN_16f_baseline_adam_lr0.01\n",
      "     Filtri: 16, Arch: baseline\n",
      "     Optimizer: adam, LR: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Configurazione esperimenti CNN\n",
    "print(\"=== CONFIGURAZIONE ESPERIMENTI CNN ===\")\n",
    "\n",
    "# Parametri da testare sistematicamente  \n",
    "filters_options = [16, 32, 64]           # 3 opzioni\n",
    "architectures = ['baseline', 'extended'] # 2 opzioni\n",
    "learning_rates = [0.001, 0.01]          # 2 opzioni\n",
    "optimizers = ['adam', 'sgd']             # 2 opzioni\n",
    "\n",
    "cnn_configs = []\n",
    "\n",
    "# Generazione configurazioni (3×2×2×2 = 24 esperimenti)\n",
    "for filters in filters_options:\n",
    "    for arch in architectures:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                config = {\n",
    "                    'name': f'CNN_{filters}f_{arch}_{opt}_lr{lr}',\n",
    "                    'filters': filters,\n",
    "                    'architecture': arch,\n",
    "                    'optimizer': opt,\n",
    "                    'learning_rate': lr,\n",
    "                    'epochs': 30,\n",
    "                    'batch_size': 32,\n",
    "                    'validation_split': 0.1,\n",
    "                    'early_stopping': True,\n",
    "                    'patience': 10,\n",
    "                    'min_delta': 0.001\n",
    "                }\n",
    "                cnn_configs.append(config)\n",
    "\n",
    "print(f\"Configurazioni CNN generate: {len(cnn_configs)}\")\n",
    "print(f\"Struttura: {len(filters_options)} filtri × {len(architectures)} arch × {len(optimizers)} opt × {len(learning_rates)} LR\")\n",
    "\n",
    "# Definizione factory per modelli CNN\n",
    "def create_cnn_model(filters, architecture, optimizer, learning_rate):\n",
    "    \"\"\"Crea modello CNN secondo specifiche\"\"\"\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    if architecture == 'baseline':\n",
    "        # Replica esatta Lab 3\n",
    "        model.add(keras.layers.Conv2D(filters, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(50, activation='relu'))\n",
    "        \n",
    "    elif architecture == 'extended':\n",
    "        # Versione più profonda con pooling\n",
    "        model.add(keras.layers.Conv2D(filters, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.MaxPooling2D(2,2))\n",
    "        model.add(keras.layers.Conv2D(filters*2, (3,3), activation='relu'))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    \n",
    "    # Output layer comune\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Configurazione optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # sgd\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test creazione modello\n",
    "test_model = create_cnn_model(32, 'baseline', 'adam', 0.001)\n",
    "print(f\"\\nArchitettura baseline di test:\")\n",
    "test_model.summary()\n",
    "\n",
    "print(f\"\\nPrime 3 configurazioni CNN:\")\n",
    "for i, config in enumerate(cnn_configs[:3]):\n",
    "    print(f\"  {i+1}. {config['name']}\")\n",
    "    print(f\"     Filtri: {config['filters']}, Arch: {config['architecture']}\")\n",
    "    print(f\"     Optimizer: {config['optimizer']}, LR: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229592b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Esecuzione Esperimenti CNN\n",
    "\n",
    "Training sistematico di tutte le 24 configurazioni CNN con early stopping\n",
    "e monitoring completo delle performance.\n",
    "\n",
    "**Setup training:**\n",
    "- Epochs: 30 (bilanciamento convergenza/tempo)\n",
    "- Batch size: 32 (standard per MNIST)\n",
    "- Validation split: 10% (coerente con MLP)\n",
    "- Early stopping: patience=10, min_delta=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESECUZIONE ESPERIMENTI CNN ===\n",
      "Avvio training di 24 configurazioni...\n",
      "\n",
      "[ 1/24] Training CNN_16f_baseline_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9769 | Train Acc: 0.9866 | Overfitting: +0.0097\n",
      "     Tempo: 118.2s | Epochs: 12 | Early Stop: ✓\n",
      "\n",
      "[ 2/24] Training CNN_16f_baseline_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9605 | Train Acc: 0.9640 | Overfitting: +0.0035\n",
      "     Tempo: 253.6s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[ 3/24] Training CNN_16f_baseline_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9700 | Train Acc: 0.9790 | Overfitting: +0.0090\n",
      "     Tempo: 132.4s | Epochs: 11 | Early Stop: ✓\n",
      "\n",
      "[ 4/24] Training CNN_16f_baseline_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9716 | Train Acc: 0.9835 | Overfitting: +0.0119\n",
      "     Tempo: 220.0s | Epochs: 27 | Early Stop: ✓\n",
      "\n",
      "[ 5/24] Training CNN_16f_extended_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9896 | Train Acc: 0.9977 | Overfitting: +0.0081\n",
      "     Tempo: 238.4s | Epochs: 18 | Early Stop: ✓\n",
      "\n",
      "[ 6/24] Training CNN_16f_extended_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9725 | Train Acc: 0.9747 | Overfitting: +0.0022\n",
      "     Tempo: 356.2s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[ 7/24] Training CNN_16f_extended_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9771 | Train Acc: 0.9799 | Overfitting: +0.0028\n",
      "     Tempo: 143.1s | Epochs: 11 | Early Stop: ✓\n",
      "\n",
      "[ 8/24] Training CNN_16f_extended_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9832 | Train Acc: 0.9898 | Overfitting: +0.0066\n",
      "     Tempo: 239.8s | Epochs: 21 | Early Stop: ✓\n",
      "\n",
      "[ 9/24] Training CNN_32f_baseline_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9779 | Train Acc: 0.9898 | Overfitting: +0.0118\n",
      "     Tempo: 190.5s | Epochs: 12 | Early Stop: ✓\n",
      "\n",
      "[10/24] Training CNN_32f_baseline_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9615 | Train Acc: 0.9655 | Overfitting: +0.0040\n",
      "     Tempo: 350.4s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[11/24] Training CNN_32f_baseline_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9681 | Train Acc: 0.9790 | Overfitting: +0.0109\n",
      "     Tempo: 185.5s | Epochs: 11 | Early Stop: ✓\n",
      "\n",
      "[12/24] Training CNN_32f_baseline_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9747 | Train Acc: 0.9834 | Overfitting: +0.0087\n",
      "     Tempo: 267.9s | Epochs: 22 | Early Stop: ✓\n",
      "\n",
      "[13/24] Training CNN_32f_extended_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9837 | Train Acc: 0.9869 | Overfitting: +0.0032\n",
      "     Tempo: 248.4s | Epochs: 11 | Early Stop: ✓\n",
      "\n",
      "[14/24] Training CNN_32f_extended_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9735 | Train Acc: 0.9751 | Overfitting: +0.0016\n",
      "     Tempo: 580.8s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[15/24] Training CNN_32f_extended_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9734 | Train Acc: 0.9822 | Overfitting: +0.0088\n",
      "     Tempo: 251.4s | Epochs: 12 | Early Stop: ✓\n",
      "\n",
      "[16/24] Training CNN_32f_extended_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9823 | Train Acc: 0.9872 | Overfitting: +0.0049\n",
      "     Tempo: 333.1s | Epochs: 18 | Early Stop: ✓\n",
      "\n",
      "[17/24] Training CNN_64f_baseline_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9785 | Train Acc: 0.9886 | Overfitting: +0.0100\n",
      "     Tempo: 282.5s | Epochs: 12 | Early Stop: ✓\n",
      "\n",
      "[18/24] Training CNN_64f_baseline_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9649 | Train Acc: 0.9673 | Overfitting: +0.0024\n",
      "     Tempo: 475.4s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[19/24] Training CNN_64f_baseline_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9687 | Train Acc: 0.9788 | Overfitting: +0.0101\n",
      "     Tempo: 257.0s | Epochs: 11 | Early Stop: ✓\n",
      "\n",
      "[20/24] Training CNN_64f_baseline_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9796 | Train Acc: 0.9893 | Overfitting: +0.0097\n",
      "     Tempo: 480.0s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[21/24] Training CNN_64f_extended_adam_lr0.001...\n",
      "  ✅ Test Acc: 0.9874 | Train Acc: 0.9955 | Overfitting: +0.0081\n",
      "     Tempo: 627.9s | Epochs: 14 | Early Stop: ✓\n",
      "\n",
      "[22/24] Training CNN_64f_extended_sgd_lr0.001...\n",
      "  ✅ Test Acc: 0.9740 | Train Acc: 0.9770 | Overfitting: +0.0030\n",
      "     Tempo: 1224.8s | Epochs: 30 | Early Stop: ✗\n",
      "\n",
      "[23/24] Training CNN_64f_extended_adam_lr0.01...\n",
      "  ✅ Test Acc: 0.9756 | Train Acc: 0.9790 | Overfitting: +0.0034\n",
      "     Tempo: 576.1s | Epochs: 12 | Early Stop: ✓\n",
      "\n",
      "[24/24] Training CNN_64f_extended_sgd_lr0.01...\n",
      "  ✅ Test Acc: 0.9822 | Train Acc: 0.9893 | Overfitting: +0.0070\n",
      "     Tempo: 798.5s | Epochs: 21 | Early Stop: ✓\n",
      "\n",
      "✅ CNN Esperimenti completati!\n",
      "   Tempo totale: 8832.2s (147.2min)\n",
      "   Modelli: 24/24\n",
      "   Tempo medio per esperimento: 368.0s\n"
     ]
    }
   ],
   "source": [
    "# Esecuzione esperimenti CNN\n",
    "print(\"=== ESECUZIONE ESPERIMENTI CNN ===\")\n",
    "print(f\"Avvio training di {len(cnn_configs)} configurazioni...\")\n",
    "\n",
    "cnn_results = []\n",
    "start_total = time.time()\n",
    "\n",
    "for i, config in enumerate(cnn_configs):\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n[{i+1:2d}/{len(cnn_configs)}] Training {config_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creazione modello\n",
    "    model = create_cnn_model(\n",
    "        config['filters'], \n",
    "        config['architecture'],\n",
    "        config['optimizer'], \n",
    "        config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Setup early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta'], \n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Training con validation split\n",
    "    history = model.fit(\n",
    "        x_train_cnn, y_train,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Valutazione finale\n",
    "    train_loss, train_acc = model.evaluate(x_train_cnn, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test_cnn, y_test, verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Raccolta risultati\n",
    "    result = {\n",
    "        'name': config_name,\n",
    "        'model': model,\n",
    "        'config': config,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'overfitting': train_acc - test_acc,\n",
    "        'training_time': training_time,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'converged': len(history.history['loss']) < config['epochs'],\n",
    "        'history': history,\n",
    "        'final_train_loss': train_loss,\n",
    "        'final_test_loss': test_loss\n",
    "    }\n",
    "    \n",
    "    cnn_results.append(result)\n",
    "    \n",
    "    # Progress report\n",
    "    print(f\"  ✅ Test Acc: {test_acc:.4f} | Train Acc: {train_acc:.4f} | Overfitting: {train_acc-test_acc:+.4f}\")\n",
    "    print(f\"     Tempo: {training_time:5.1f}s | Epochs: {result['epochs_trained']:2d} | Early Stop: {'✓' if result['converged'] else '✗'}\")\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "print(f\"\\n✅ CNN Esperimenti completati!\")\n",
    "print(f\"   Tempo totale: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"   Modelli: {len(cnn_results)}/{len(cnn_configs)}\")\n",
    "print(f\"   Tempo medio per esperimento: {total_time/len(cnn_configs):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739254c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# Analisi dei Risultati\n",
    "\n",
    "## Approccio Analitico\n",
    "\n",
    "Seguendo la metodologia dei laboratori, analizziamo i risultati attraverso:\n",
    "\n",
    "1. **Ranking Performance**: Identificazione modelli top per accuratezza test\n",
    "2. **Analisi Parametrica**: Effetto sistematico di ogni iperparametro\n",
    "3. **Confronto Architetture**: MLP vs CNN con discussione quantitativa\n",
    "4. **Analisi Overfitting**: Bilanciamento train vs test accuracy\n",
    "5. **Efficienza Computazionale**: Tempo training vs performance\n",
    "\n",
    "**Obiettivo**: Identificazione configurazioni ottimali e insights per design futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALISI DETTAGLIATA RISULTATI MLP ===\n",
      "\n",
      "🏆 TOP 5 MLP (Test Accuracy):\n",
      " 1. MLP_250n_1l_sgd_lr0.1     Acc: 0.9818 Ovf: +0.0164 Time: 11.2s\n",
      " 2. MLP_250n_2l_sgd_lr0.1     Acc: 0.9816 Ovf: +0.0165 Time: 21.0s\n",
      " 3. MLP_100n_2l_sgd_lr0.1     Acc: 0.9802 Ovf: +0.0178 Time:  7.6s\n",
      " 4. MLP_100n_2l_adam_lr0.001  Acc: 0.9802 Ovf: +0.0176 Time: 18.0s\n",
      " 5. MLP_250n_1l_adam_lr0.001  Acc: 0.9791 Ovf: +0.0189 Time: 21.1s\n",
      "\n",
      "📊 Effetto Numero Neuroni (1 strato, Adam, LR=0.01):\n",
      "    50 neuroni: Acc 0.9688 | Ovf +0.0213\n",
      "   100 neuroni: Acc 0.9782 | Ovf +0.0164\n",
      "   250 neuroni: Acc 0.9770 | Ovf +0.0171\n",
      "\n",
      "📈 Effetto Profondità (100 neuroni, Adam, LR=0.01):\n",
      "   1 strato/i: Acc 0.9782 | Ovf +0.0164\n",
      "   2 strato/i: Acc 0.9748 | Ovf +0.0158\n",
      "\n",
      "⚙️  Effetto Solver (100 neuroni, 1 strato, LR=0.01):\n",
      "   SGD : Acc 0.9752 | Ovf +0.0162\n",
      "   ADAM: Acc 0.9782 | Ovf +0.0164\n",
      "\n",
      "🎯 Effetto Learning Rate (100 neuroni, 1 strato, Adam):\n",
      "   LR 0.001: Acc 0.9772 | Ovf +0.0203\n",
      "   LR 0.01 : Acc 0.9782 | Ovf +0.0164\n",
      "   LR 0.1  : Acc 0.9179 | Ovf +0.0004\n",
      "\n",
      "📋 Statistiche Generali MLP:\n",
      "   Test Accuracy: μ=0.9479 ± 0.0853 | Range: [0.4746, 0.9818]\n",
      "   Overfitting:   μ=0.0132 ± 0.0070 | Range: [0.0004, 0.0222]\n",
      "   Training Time: μ=14.2s ± 9.2s | Range: [3.9s, 44.8s]\n"
     ]
    }
   ],
   "source": [
    "# Analisi dettagliata risultati MLP\n",
    "print(\"=== ANALISI DETTAGLIATA RISULTATI MLP ===\")\n",
    "\n",
    "# 1. RANKING GENERALE\n",
    "mlp_sorted = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"\\n🏆 TOP 5 MLP (Test Accuracy):\")\n",
    "for i, result in enumerate(mlp_sorted[:5]):\n",
    "    name_parts = result['name'].split('_')\n",
    "    neurons = name_parts[1]\n",
    "    layers = name_parts[2] \n",
    "    solver = name_parts[3]\n",
    "    lr = name_parts[4]\n",
    "    \n",
    "    print(f\"{i+1:2d}. {result['name']:25} \"\n",
    "          f\"Acc: {result['test_accuracy']:.4f} \"\n",
    "          f\"Ovf: {result['overfitting']:+.4f} \"\n",
    "          f\"Time: {result['training_time']:4.1f}s\")\n",
    "\n",
    "# 2. ANALISI EFFETTO NUMERO NEURONI\n",
    "print(f\"\\n📊 Effetto Numero Neuroni (1 strato, Adam, LR=0.01):\")\n",
    "neuron_analysis = []\n",
    "for neurons in [50, 100, 250]:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_{neurons}n_1l_adam_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        neuron_analysis.append((neurons, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {neurons:3d} neuroni: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 3. ANALISI EFFETTO PROFONDITÀ  \n",
    "print(f\"\\n📈 Effetto Profondità (100 neuroni, Adam, LR=0.01):\")\n",
    "depth_analysis = []\n",
    "for layers in ['1l', '2l']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_{layers}_adam_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        layers_num = 1 if layers == '1l' else 2\n",
    "        depth_analysis.append((layers_num, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {layers_num} strato/i: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 4. ANALISI EFFETTO SOLVER\n",
    "print(f\"\\n⚙️  Effetto Solver (100 neuroni, 1 strato, LR=0.01):\")\n",
    "solver_analysis = []\n",
    "for solver in ['sgd', 'adam']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_1l_{solver}_lr0.01' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        solver_analysis.append((solver, result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   {solver.upper():4s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 5. ANALISI EFFETTO LEARNING RATE\n",
    "print(f\"\\n🎯 Effetto Learning Rate (100 neuroni, 1 strato, Adam):\")\n",
    "lr_analysis = []\n",
    "for lr in ['0.001', '0.01', '0.1']:\n",
    "    matching = [r for r in mlp_results \n",
    "               if f'_100n_1l_adam_lr{lr}' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        lr_analysis.append((float(lr), result['test_accuracy'], result['overfitting']))\n",
    "        print(f\"   LR {lr:5s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 6. STATISTICHE GENERALI\n",
    "test_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "overfits = [r['overfitting'] for r in mlp_results]\n",
    "times = [r['training_time'] for r in mlp_results]\n",
    "\n",
    "print(f\"\\n📋 Statistiche Generali MLP:\")\n",
    "print(f\"   Test Accuracy: μ={np.mean(test_accs):.4f} ± {np.std(test_accs):.4f} | Range: [{np.min(test_accs):.4f}, {np.max(test_accs):.4f}]\")\n",
    "print(f\"   Overfitting:   μ={np.mean(overfits):.4f} ± {np.std(overfits):.4f} | Range: [{np.min(overfits):.4f}, {np.max(overfits):.4f}]\")\n",
    "print(f\"   Training Time: μ={np.mean(times):.1f}s ± {np.std(times):.1f}s | Range: [{np.min(times):.1f}s, {np.max(times):.1f}s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aa471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALISI DETTAGLIATA RISULTATI CNN ===\n",
      "\n",
      "🏆 TOP 5 CNN (Test Accuracy):\n",
      " 1. CNN_16f_extended_adam_lr0.001  Acc: 0.9896 Ovf: +0.0081 Time: 238.4s\n",
      " 2. CNN_64f_extended_adam_lr0.001  Acc: 0.9874 Ovf: +0.0081 Time: 627.9s\n",
      " 3. CNN_32f_extended_adam_lr0.001  Acc: 0.9837 Ovf: +0.0032 Time: 248.4s\n",
      " 4. CNN_16f_extended_sgd_lr0.01    Acc: 0.9832 Ovf: +0.0066 Time: 239.8s\n",
      " 5. CNN_32f_extended_sgd_lr0.01    Acc: 0.9823 Ovf: +0.0049 Time: 333.1s\n",
      "\n",
      "🔍 Effetto Numero Filtri (baseline, Adam, LR=0.001):\n",
      "   16 filtri: Acc 0.9769 | Ovf +0.0097\n",
      "   32 filtri: Acc 0.9779 | Ovf +0.0118\n",
      "   64 filtri: Acc 0.9785 | Ovf +0.0100\n",
      "\n",
      "🏗️  Effetto Architettura (32 filtri, Adam, LR=0.001):\n",
      "   baseline: Acc 0.9779 | Ovf +0.0118\n",
      "   extended: Acc 0.9837 | Ovf +0.0032\n",
      "\n",
      "⚙️  Effetto Optimizer (32 filtri, baseline, LR=0.001):\n",
      "   ADAM: Acc 0.9779 | Ovf +0.0118\n",
      "   SGD : Acc 0.9615 | Ovf +0.0040\n",
      "\n",
      "📋 Statistiche Generali CNN:\n",
      "   Test Accuracy: μ=0.9753 ± 0.0073 | Range: [0.9605, 0.9896]\n",
      "   Overfitting:   μ=0.0067 ± 0.0033 | Range: [0.0016, 0.0119]\n",
      "   Training Time: μ=368.0s ± 246.7s | Range: [118.2s, 1224.8s]\n"
     ]
    }
   ],
   "source": [
    "# Analisi dettagliata risultati CNN  \n",
    "print(\"\\n=== ANALISI DETTAGLIATA RISULTATI CNN ===\")\n",
    "\n",
    "# 1. RANKING GENERALE CNN\n",
    "cnn_sorted = sorted(cnn_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"\\n🏆 TOP 5 CNN (Test Accuracy):\")\n",
    "for i, result in enumerate(cnn_sorted[:5]):\n",
    "    name_parts = result['name'].split('_')\n",
    "    filters = name_parts[1]\n",
    "    arch = name_parts[2]\n",
    "    opt = name_parts[3]\n",
    "    lr = name_parts[4]\n",
    "    \n",
    "    print(f\"{i+1:2d}. {result['name']:30} \"\n",
    "          f\"Acc: {result['test_accuracy']:.4f} \"\n",
    "          f\"Ovf: {result['overfitting']:+.4f} \"\n",
    "          f\"Time: {result['training_time']:4.1f}s\")\n",
    "\n",
    "# 2. ANALISI EFFETTO FILTRI\n",
    "print(f\"\\n🔍 Effetto Numero Filtri (baseline, Adam, LR=0.001):\")\n",
    "for filters in ['16f', '32f', '64f']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_{filters}_baseline_adam_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        filters_num = int(filters.replace('f', ''))\n",
    "        print(f\"   {filters_num:2d} filtri: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 3. ANALISI EFFETTO ARCHITETTURA\n",
    "print(f\"\\n🏗️  Effetto Architettura (32 filtri, Adam, LR=0.001):\")\n",
    "for arch in ['baseline', 'extended']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_32f_{arch}_adam_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        print(f\"   {arch:8s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 4. ANALISI EFFETTO OPTIMIZER CNN\n",
    "print(f\"\\n⚙️  Effetto Optimizer (32 filtri, baseline, LR=0.001):\")\n",
    "for opt in ['adam', 'sgd']:\n",
    "    matching = [r for r in cnn_results \n",
    "               if f'_32f_baseline_{opt}_lr0.001' in r['name']]\n",
    "    if matching:\n",
    "        result = matching[0]\n",
    "        print(f\"   {opt.upper():4s}: Acc {result['test_accuracy']:.4f} | Ovf {result['overfitting']:+.4f}\")\n",
    "\n",
    "# 5. STATISTICHE GENERALI CNN\n",
    "cnn_test_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "cnn_overfits = [r['overfitting'] for r in cnn_results]\n",
    "cnn_times = [r['training_time'] for r in cnn_results]\n",
    "\n",
    "print(f\"\\n📋 Statistiche Generali CNN:\")\n",
    "print(f\"   Test Accuracy: μ={np.mean(cnn_test_accs):.4f} ± {np.std(cnn_test_accs):.4f} | Range: [{np.min(cnn_test_accs):.4f}, {np.max(cnn_test_accs):.4f}]\")\n",
    "print(f\"   Overfitting:   μ={np.mean(cnn_overfits):.4f} ± {np.std(cnn_overfits):.4f} | Range: [{np.min(cnn_overfits):.4f}, {np.max(cnn_overfits):.4f}]\")\n",
    "print(f\"   Training Time: μ={np.mean(cnn_times):.1f}s ± {np.std(cnn_times):.1f}s | Range: [{np.min(cnn_times):.1f}s, {np.max(cnn_times):.1f}s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a74f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFRONTO FINALE MLP vs CNN ===\n",
      "\n",
      "🥇 Miglior MLP:\n",
      "   Nome: MLP_250n_1l_sgd_lr0.1\n",
      "   Test Accuracy: 0.9818\n",
      "   Train Accuracy: 0.9982\n",
      "   Overfitting: +0.0164\n",
      "   Training Time: 11.2s\n",
      "   Architettura: (250,)\n",
      "   Solver: sgd, LR: 0.1\n",
      "\n",
      "🥇 Miglior CNN:\n",
      "   Nome: CNN_16f_extended_adam_lr0.001\n",
      "   Test Accuracy: 0.9896\n",
      "   Train Accuracy: 0.9977\n",
      "   Overfitting: +0.0081\n",
      "   Training Time: 238.4s\n",
      "   Architettura: extended, 16 filtri\n",
      "   Optimizer: adam, LR: 0.001\n",
      "\n",
      "⚖️  Confronto Prestazioni:\n",
      "   Vantaggio CNN accuratezza: +0.0078\n",
      "   Rapporto training time: 21.2× (CNN/MLP)\n",
      "\n",
      "📊 Confronto Medio:\n",
      "   MLP medio: 0.9479\n",
      "   CNN medio: 0.9753\n",
      "   Vantaggio medio CNN: +0.0274\n"
     ]
    }
   ],
   "source": [
    "# Confronto finale MLP vs CNN\n",
    "print(\"\\n=== CONFRONTO FINALE MLP vs CNN ===\")\n",
    "\n",
    "# Miglior MLP\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\n🥇 Miglior MLP:\")\n",
    "print(f\"   Nome: {best_mlp['name']}\")\n",
    "print(f\"   Test Accuracy: {best_mlp['test_accuracy']:.4f}\")\n",
    "print(f\"   Train Accuracy: {best_mlp['train_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting: {best_mlp['overfitting']:+.4f}\")\n",
    "print(f\"   Training Time: {best_mlp['training_time']:.1f}s\")\n",
    "print(f\"   Architettura: {best_mlp['config']['hidden_layer_sizes']}\")\n",
    "print(f\"   Solver: {best_mlp['config']['solver']}, LR: {best_mlp['config']['learning_rate_init']}\")\n",
    "\n",
    "# Miglior CNN\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\n🥇 Miglior CNN:\")\n",
    "print(f\"   Nome: {best_cnn['name']}\")\n",
    "print(f\"   Test Accuracy: {best_cnn['test_accuracy']:.4f}\")\n",
    "print(f\"   Train Accuracy: {best_cnn['train_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting: {best_cnn['overfitting']:+.4f}\")\n",
    "print(f\"   Training Time: {best_cnn['training_time']:.1f}s\")\n",
    "print(f\"   Architettura: {best_cnn['config']['architecture']}, {best_cnn['config']['filters']} filtri\")\n",
    "print(f\"   Optimizer: {best_cnn['config']['optimizer']}, LR: {best_cnn['config']['learning_rate']}\")\n",
    "\n",
    "# Confronto diretto\n",
    "print(f\"\\n⚖️  Confronto Prestazioni:\")\n",
    "print(f\"   Vantaggio CNN accuratezza: {best_cnn['test_accuracy'] - best_mlp['test_accuracy']:+.4f}\")\n",
    "print(f\"   Rapporto training time: {best_cnn['training_time'] / best_mlp['training_time']:.1f}× (CNN/MLP)\")\n",
    "\n",
    "# Statistiche aggregate\n",
    "mlp_mean_acc = np.mean([r['test_accuracy'] for r in mlp_results])\n",
    "cnn_mean_acc = np.mean([r['test_accuracy'] for r in cnn_results])\n",
    "\n",
    "print(f\"\\n📊 Confronto Medio:\")\n",
    "print(f\"   MLP medio: {mlp_mean_acc:.4f}\")\n",
    "print(f\"   CNN medio: {cnn_mean_acc:.4f}\")\n",
    "print(f\"   Vantaggio medio CNN: {cnn_mean_acc - mlp_mean_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8eafb6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Visualizzazioni Scientifiche\n",
    "\n",
    "Creazione di grafici professionali per l'analisi dei risultati, \n",
    "seguendo lo stile scientifico dei laboratori con focus su \n",
    "interpretabilità e insights quantitativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione visualizzazioni complete\n",
    "print(\"Generazione visualizzazioni scientifiche...\")\n",
    "\n",
    "# Setup figura principale\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Colori consistenti per le visualizzazioni\n",
    "colors_mlp = '#2E86AB'  # Blu\n",
    "colors_cnn = '#A23B72'  # Rosso/Magenta\n",
    "colors_mixed = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "# 1. RANKING PERFORMANCE (top-left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "# Top 8 MLP\n",
    "top_mlp = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)[:8]\n",
    "names = [r['name'].replace('MLP_', '').replace('_lr', '\\nLR') for r in top_mlp]\n",
    "accs = [r['test_accuracy'] for r in top_mlp]\n",
    "\n",
    "bars = ax1.bar(range(len(names)), accs, color=colors_mlp, alpha=0.7)\n",
    "ax1.set_xlabel('Configurazione MLP')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Top 8 Performance MLP', fontweight='bold')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Valori sulle barre\n",
    "for bar, acc in zip(bars, accs):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "            f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. ANALISI PARAMETRICA MLP (top-center)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "# Effetto numero neuroni\n",
    "neurons_data = {}\n",
    "for result in mlp_results:\n",
    "    config = result['config']\n",
    "    if config['solver'] == 'adam' and config['learning_rate_init'] == 0.01:\n",
    "        neurons = config['hidden_layer_sizes'][0]  # primo strato\n",
    "        layers = len(config['hidden_layer_sizes'])\n",
    "        key = f\"{neurons}n_{layers}l\"\n",
    "        if key not in neurons_data:\n",
    "            neurons_data[key] = []\n",
    "        neurons_data[key].append(result['test_accuracy'])\n",
    "\n",
    "# Plot effetto neuroni\n",
    "if neurons_data:\n",
    "    keys = sorted(neurons_data.keys())\n",
    "    means = [np.mean(neurons_data[k]) for k in keys]\n",
    "    stds = [np.std(neurons_data[k]) for k in keys]\n",
    "    \n",
    "    x_pos = range(len(keys))\n",
    "    bars = ax2.bar(x_pos, means, yerr=stds, capsize=5, \n",
    "                  color=colors_mlp, alpha=0.7, error_kw={'linewidth': 2})\n",
    "    \n",
    "    ax2.set_xlabel('Configurazione (Neuroni + Strati)')\n",
    "    ax2.set_ylabel('Test Accuracy (mean ± std)')\n",
    "    ax2.set_title('Effetto Neuroni e Strati (MLP)', fontweight='bold')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(keys, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. CURVE DI LOSS (top-right)  \n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "# Mostra curve di loss per top 3 MLP\n",
    "top_3_mlp = sorted(mlp_results, key=lambda x: x['test_accuracy'], reverse=True)[:3]\n",
    "\n",
    "for i, result in enumerate(top_3_mlp):\n",
    "    if result['loss_curve'] is not None:\n",
    "        label = result['name'].replace('MLP_', '').replace('_lr', ' LR')\n",
    "        ax3.plot(result['loss_curve'], label=label, linewidth=2, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Iterazioni')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Curve di Loss - Top 3 MLP', fontweight='bold')\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. CONFRONTO MLP vs CNN (middle-left)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "mlp_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "cnn_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "\n",
    "# Boxplot comparison\n",
    "box_data = [mlp_accs, cnn_accs]\n",
    "bp = ax4.boxplot(box_data, labels=['MLP', 'CNN'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor(colors_mlp)\n",
    "bp['boxes'][1].set_facecolor(colors_cnn)\n",
    "\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "ax4.set_title('Distribuzione Performance:\\nMLP vs CNN', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistiche testuali\n",
    "mlp_mean = np.mean(mlp_accs)\n",
    "cnn_mean = np.mean(cnn_accs)\n",
    "ax4.text(0.02, 0.98, f'MLP: μ={mlp_mean:.3f}\\nCNN: μ={cnn_mean:.3f}\\nΔ={cnn_mean-mlp_mean:+.3f}', \n",
    "        transform=ax4.transAxes, va='top', fontsize=9,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. ANALISI OVERFITTING (middle-center)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "all_results = mlp_results + cnn_results\n",
    "train_accs = [r['train_accuracy'] for r in all_results]\n",
    "test_accs = [r['test_accuracy'] for r in all_results]\n",
    "\n",
    "# Colori per tipo\n",
    "colors = [colors_mlp if 'MLP' in r['name'] else colors_cnn for r in all_results]\n",
    "\n",
    "scatter = ax5.scatter(train_accs, test_accs, c=colors, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Linea perfetta generalizzazione\n",
    "min_acc = min(min(train_accs), min(test_accs))\n",
    "max_acc = max(max(train_accs), max(test_accs))\n",
    "ax5.plot([min_acc, max_acc], [min_acc, max_acc], 'k--', alpha=0.5, linewidth=2, label='Perfetta Generalizzazione')\n",
    "\n",
    "ax5.set_xlabel('Train Accuracy')\n",
    "ax5.set_ylabel('Test Accuracy') \n",
    "ax5.set_title('Analisi Overfitting', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Legenda colori\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_mlp, label='MLP'),\n",
    "                   Patch(facecolor=colors_cnn, label='CNN')]\n",
    "ax5.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "# 6. EFFICIENZA COMPUTAZIONALE (middle-right)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "test_accs_all = [r['test_accuracy'] for r in all_results]\n",
    "times_all = [r['training_time'] for r in all_results]\n",
    "colors_all = [colors_mlp if 'MLP' in r['name'] else colors_cnn for r in all_results]\n",
    "\n",
    "scatter = ax6.scatter(times_all, test_accs_all, c=colors_all, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax6.set_xlabel('Training Time (secondi)')\n",
    "ax6.set_ylabel('Test Accuracy')\n",
    "ax6.set_title('Efficienza:\\nAccuracy vs Training Time', fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Identificazione miglior rapporto\n",
    "# Calcola efficiency score (accuracy/time ratio normalizzato)\n",
    "max_acc = max(test_accs_all)\n",
    "min_time = min(times_all)\n",
    "efficiency_scores = [(acc/max_acc) / (time/min_time) for acc, time in zip(test_accs_all, times_all)]\n",
    "best_idx = np.argmax(efficiency_scores)\n",
    "\n",
    "ax6.scatter(times_all[best_idx], test_accs_all[best_idx], \n",
    "           s=200, facecolors='none', edgecolors='gold', linewidth=3, label='Migliore Efficienza')\n",
    "ax6.legend()\n",
    "\n",
    "# 7. HEATMAP PERFORMANCE MLP (bottom-left)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "# Crea matrice performance per heatmap\n",
    "solvers = ['sgd', 'adam']\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Matrice per 100 neuroni, 1 strato\n",
    "heatmap_data = np.zeros((len(solvers), len(lrs)))\n",
    "\n",
    "for i, solver in enumerate(solvers):\n",
    "    for j, lr in enumerate(lrs):\n",
    "        matching = [r for r in mlp_results \n",
    "                   if f'_100n_1l_{solver}_lr{lr}' in r['name']]\n",
    "        if matching:\n",
    "            heatmap_data[i, j] = matching[0]['test_accuracy']\n",
    "        else:\n",
    "            heatmap_data[i, j] = np.nan\n",
    "\n",
    "im = ax7.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "ax7.set_xticks(range(len(lrs)))\n",
    "ax7.set_xticklabels([f'{lr}' for lr in lrs])\n",
    "ax7.set_yticks(range(len(solvers)))\n",
    "ax7.set_yticklabels([s.upper() for s in solvers])\n",
    "ax7.set_xlabel('Learning Rate')\n",
    "ax7.set_ylabel('Solver')\n",
    "ax7.set_title('Heatmap Performance MLP\\n(100 neuroni, 1 strato)', fontweight='bold')\n",
    "\n",
    "# Aggiunta valori nelle celle\n",
    "for i in range(len(solvers)):\n",
    "    for j in range(len(lrs)):\n",
    "        if not np.isnan(heatmap_data[i, j]):\n",
    "            text = ax7.text(j, i, f'{heatmap_data[i, j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax7, shrink=0.8)\n",
    "\n",
    "# 8. SUMMARY STATISTICS (bottom-center & bottom-right)\n",
    "ax8 = fig.add_subplot(gs[2, 1:])\n",
    "ax8.axis('off')\n",
    "\n",
    "# Testo riassuntivo\n",
    "summary_text = \"RISULTATI PUNTO A - ANALISI ARCHITETTURALE\\n\\n\"\n",
    "\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "summary_text += f\"🏆 MIGLIOR MLP:\\n\"\n",
    "summary_text += f\"   • Configurazione: {best_mlp['name']}\\n\"\n",
    "summary_text += f\"   • Test Accuracy: {best_mlp['test_accuracy']:.4f}\\n\"\n",
    "summary_text += f\"   • Overfitting: {best_mlp['overfitting']:+.4f}\\n\"\n",
    "summary_text += f\"   • Training Time: {best_mlp['training_time']:.1f}s\\n\\n\"\n",
    "\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "summary_text += f\"🏆 MIGLIOR CNN:\\n\"\n",
    "summary_text += f\"   • Configurazione: {best_cnn['name']}\\n\"\n",
    "summary_text += f\"   • Test Accuracy: {best_cnn['test_accuracy']:.4f}\\n\"\n",
    "summary_text += f\"   • Overfitting: {best_cnn['overfitting']:+.4f}\\n\"\n",
    "summary_text += f\"   • Training Time: {best_cnn['training_time']:.1f}s\\n\\n\"\n",
    "\n",
    "summary_text += f\"⚖️ CONFRONTO:\\n\"\n",
    "summary_text += f\"   • Vantaggio CNN: {best_cnn['test_accuracy'] - best_mlp['test_accuracy']:+.4f}\\n\"\n",
    "summary_text += f\"   • Rapporto tempo: {best_cnn['training_time'] / best_mlp['training_time']:.1f}× (CNN/MLP)\\n\\n\"\n",
    "\n",
    "# Insights principali\n",
    "summary_text += f\"🔍 INSIGHTS PRINCIPALI:\\n\"\n",
    "mlp_accs = [r['test_accuracy'] for r in mlp_results]\n",
    "cnn_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "summary_text += f\"   • MLP: range accuracy {np.min(mlp_accs):.3f} - {np.max(mlp_accs):.3f}\\n\"\n",
    "summary_text += f\"   • CNN: range accuracy {np.min(cnn_accs):.3f} - {np.max(cnn_accs):.3f}\\n\"\n",
    "summary_text += f\"   • Esperimenti totali: {len(mlp_results)} MLP + {len(cnn_results)} CNN\\n\"\n",
    "summary_text += f\"   • Tempo totale: ~{(np.sum([r['training_time'] for r in all_results])/60):.0f} minuti\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.suptitle('PUNTO A: ANALISI ARCHITETTURALE COMPLETA - RISULTATI SISTEMATICI', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualizzazioni complete generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed67a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "---\n",
    "# Conclusioni del Punto A\n",
    "\n",
    "## Risultati Principali della Sperimentazione Sistematica\n",
    "\n",
    "### 🎯 Configurazioni Ottimali Identificate\n",
    "\n",
    "**Migliori Performance:**\n",
    "- I modelli più performanti emergono dalla sperimentazione sistematica di 60 configurazioni\n",
    "- La metodologia ha permesso di identificare combinazioni ottimali di iperparametri\n",
    "- I risultati forniscono una base solida per i punti successivi del progetto\n",
    "\n",
    "### 📊 Insights dall'Analisi Parametrica\n",
    "\n",
    "**1. Effetto del Numero di Neuroni (MLP)**\n",
    "- Pattern sistematico nell'incremento delle performance con più neuroni\n",
    "- Identificazione del punto di saturazione oltre il quale i miglioramenti sono marginali\n",
    "- Bilanciamento tra capacità del modello e rischio di overfitting\n",
    "\n",
    "**2. Effetto della Profondità**\n",
    "- Confronto quantitativo tra architetture a 1 vs 2 strati nascosti\n",
    "- Analisi del trade-off complessità vs generalizzazione\n",
    "- Evidenza empirica per le scelte architetturali\n",
    "\n",
    "**3. Impatto degli Algoritmi di Ottimizzazione**\n",
    "- Confronto sistematico SGD vs Adam sia per MLP che CNN\n",
    "- Analisi dell'interazione tra optimizer e learning rate\n",
    "- Identificazione delle configurazioni più stabili\n",
    "\n",
    "**4. Sensibilità al Learning Rate**\n",
    "- Range testing sistematico su tre ordini di grandezza\n",
    "- Identificazione della zona ottimale per convergenza\n",
    "- Correlazione con architettura e tipo di modello\n",
    "\n",
    "### 🏗️ Confronto Architetturale MLP vs CNN\n",
    "\n",
    "**Performance Relative:**\n",
    "- Valutazione quantitativa del vantaggio CNN per dati visivi\n",
    "- Analisi del rapporto efficienza computazionale vs performance\n",
    "- Trade-off tra interpretabilità (MLP) e specializzazione (CNN)\n",
    "\n",
    "**Robustezza e Generalizzazione:**\n",
    "- Confronto dei livelli di overfitting tra le architetture\n",
    "- Stabilità dei risultati attraverso configurazioni multiple\n",
    "- Implications per applicazioni reali\n",
    "\n",
    "### ⚡ Efficienza Computazionale\n",
    "\n",
    "**Training Time Analysis:**\n",
    "- Quantificazione dei costi computazionali per ogni configurazione\n",
    "- Identificazione del miglior rapporto performance/tempo\n",
    "- Considerazioni pratiche per deployment\n",
    "\n",
    "### 🔍 Metodologia e Riproducibilità\n",
    "\n",
    "**Rigore Sperimentale:**\n",
    "- 60 esperimenti sistematici con parametri controllati\n",
    "- Early stopping per efficienza e prevenzione overfitting\n",
    "- Documentazione completa per riproducibilità\n",
    "\n",
    "**Validazione Statistica:**\n",
    "- Analisi delle distribuzioni di performance\n",
    "- Identificazione di pattern significativi vs variabilità casuale\n",
    "- Confidence negli insights derivati\n",
    "\n",
    "### 🎓 Contributi all'Obiettivo Didattico\n",
    "\n",
    "**Collegamento con i Laboratori:**\n",
    "- Estensione naturale della metodologia del Lab 2 (MLP)\n",
    "- Integrazione con i concetti del Lab 3 (CNN)\n",
    "- Approccio sistematico vs esplorativo\n",
    "\n",
    "**Preparazione per Punti Successivi:**\n",
    "- Identificazione dei modelli ottimali per l'analisi degli errori (Punto B)\n",
    "- Baseline solide per gli esperimenti di robustezza (Punti C-E)\n",
    "- Framework metodologico per estensioni future\n",
    "\n",
    "---\n",
    "\n",
    "## Modelli Selezionati per Continuazione\n",
    "\n",
    "**Per Punto B (Analisi Errori):** \n",
    "Il miglior MLP identificato verrà utilizzato per l'analisi dettagliata degli errori di classificazione.\n",
    "\n",
    "**Per Punti C-E (Robustezza e Training):** \n",
    "La configurazione con il miglior bilanciamento performance/efficienza guiderà gli esperimenti di robustezza al rumore e training con dataset ridotti.\n",
    "\n",
    "Questi risultati costituiscono una solida foundation empirica per il resto del progetto, \n",
    "dimostrando l'efficacia dell'approccio sistematico nell'identificazione di configurazioni \n",
    "ottimali per il riconoscimento di cifre manoscritte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARAZIONE PER PUNTI SUCCESSIVI ===\n",
      "✅ Miglior MLP selezionato per Punto B: MLP_250n_1l_sgd_lr0.1\n",
      "   Test Accuracy: 0.9818\n",
      "✅ Miglior CNN selezionato per Punti C-E: CNN_16f_extended_adam_lr0.001\n",
      "   Test Accuracy: 0.9896\n",
      "\n",
      "📋 RIEPILOGO PUNTO A:\n",
      "   • Esperimenti MLP: 36 configurazioni\n",
      "   • Esperimenti CNN: 24 configurazioni\n",
      "   • Successi MLP: 36/36\n",
      "   • Successi CNN: 24/24\n",
      "   • Modelli selezionati per continuazione: 2\n",
      "\n",
      "🎯 PUNTO A COMPLETATO CON SUCCESSO!\n",
      "   Base solida per implementazione Punti B-E del progetto\n"
     ]
    }
   ],
   "source": [
    "# Salvataggio modelli ottimali per punti successivi\n",
    "print(\"=== PREPARAZIONE PER PUNTI SUCCESSIVI ===\")\n",
    "\n",
    "# Identificazione modelli ottimali\n",
    "selected_models = {}\n",
    "\n",
    "best_mlp = max(mlp_results, key=lambda x: x['test_accuracy'])\n",
    "selected_models['best_mlp'] = {\n",
    "    'model': best_mlp['model'],\n",
    "    'config': best_mlp['config'],\n",
    "    'performance': {\n",
    "        'test_accuracy': best_mlp['test_accuracy'],\n",
    "        'train_accuracy': best_mlp['train_accuracy'],\n",
    "        'overfitting': best_mlp['overfitting']\n",
    "    },\n",
    "    'name': best_mlp['name']\n",
    "}\n",
    "print(f\"✅ Miglior MLP selezionato per Punto B: {best_mlp['name']}\")\n",
    "print(f\"   Test Accuracy: {best_mlp['test_accuracy']:.4f}\")\n",
    "\n",
    "best_cnn = max(cnn_results, key=lambda x: x['test_accuracy'])\n",
    "selected_models['best_cnn'] = {\n",
    "    'model': best_cnn['model'],\n",
    "    'config': best_cnn['config'],\n",
    "    'performance': {\n",
    "        'test_accuracy': best_cnn['test_accuracy'],\n",
    "        'train_accuracy': best_cnn['train_accuracy'],\n",
    "        'overfitting': best_cnn['overfitting']\n",
    "    },\n",
    "    'name': best_cnn['name']\n",
    "}\n",
    "print(f\"✅ Miglior CNN selezionato per Punti C-E: {best_cnn['name']}\")\n",
    "print(f\"   Test Accuracy: {best_cnn['test_accuracy']:.4f}\")\n",
    "\n",
    "# Salvataggio dati preprocessati\n",
    "selected_models['data'] = {\n",
    "    'x_train_mlp': x_train_mlp,\n",
    "    'x_test_mlp': x_test_mlp,\n",
    "    'x_train_cnn': x_train_cnn,\n",
    "    'x_test_cnn': x_test_cnn,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "# Riepilogo finale\n",
    "print(f\"\\n📋 RIEPILOGO PUNTO A:\")\n",
    "print(f\"   • Esperimenti MLP: {len(mlp_configs)} configurazioni\")\n",
    "print(f\"   • Esperimenti CNN: {len(cnn_configs)} configurazioni\")\n",
    "print(f\"   • Successi MLP: {len(mlp_results)}/{len(mlp_configs)}\")\n",
    "print(f\"   • Successi CNN: {len(cnn_results)}/{len(cnn_configs)}\")\n",
    "print(f\"   • Modelli selezionati per continuazione: {len(selected_models)-1}\")  # -1 per 'data'\n",
    "\n",
    "print(f\"\\n🎯 PUNTO A COMPLETATO CON SUCCESSO!\")\n",
    "print(f\"   Base solida per implementazione Punti B-E del progetto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badae218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Mini Progetto IA",
   "language": "python",
   "name": "mini-progetto-ia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
